import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import requests
from bs4 import BeautifulSoup
import threading
from urllib.parse import urljoin, urlparse
import pandas as pd
from datetime import datetime, timedelta
import os
import time
import re
import unicodedata
import json
import pickle
import smtplib
import email.mime.text
import email.mime.multipart
MimeText = email.mime.text.MIMEText
MimeMultipart = email.mime.multipart.MIMEMultipart
import schedule
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

# ì„ íƒì  import (ì—†ì–´ë„ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ê°€ëŠ¥)
try:
    import matplotlib.pyplot as plt
    import matplotlib.font_manager as fm
    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
    import seaborn as sns
    CHART_AVAILABLE = True
except ImportError:
    CHART_AVAILABLE = False

try:
    from plyer import notification
    NOTIFICATION_AVAILABLE = True
except ImportError:
    NOTIFICATION_AVAILABLE = False

# Playwright ê´€ë ¨ import (ì„ íƒì )
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

class WebCrawlerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("ì›¹ í¬ë¡¤ëŸ¬")
        self.root.geometry("800x600")
        
        # ë©”ì¸ í”„ë ˆì„
        main_frame = ttk.Frame(root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # URL ì…ë ¥ ì„¹ì…˜
        url_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ì„¤ì •", padding="5")
        url_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        ttk.Label(url_frame, text="URL (ì—”í„°í‚¤ë¡œ ì‹œì‘):").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        self.url_var = tk.StringVar(value="https://example.com")
        self.url_entry = ttk.Entry(url_frame, textvariable=self.url_var, width=60)
        self.url_entry.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(0, 5))
        
        # URL ì…ë ¥ì°½ì— í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ ë°”ì¸ë”© ì¶”ê°€
        self.setup_url_entry_bindings()
        
        self.crawl_button = ttk.Button(url_frame, text="í¬ë¡¤ë§ ì‹œì‘", command=self.start_crawling)
        self.crawl_button.grid(row=0, column=2, padx=(5, 0))
        
        self.stop_button = ttk.Button(url_frame, text="ì¤‘ì§€", command=self.stop_crawling, state='disabled')
        self.stop_button.grid(row=0, column=3, padx=(5, 0))
        
        # í¬ë¡¤ë§ ì˜µì…˜
        options_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ì˜µì…˜", padding="5")
        options_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(5, 0))
        
        # ê¸°ë³¸ ì¶”ì¶œ ì˜µì…˜
        extract_frame = ttk.Frame(options_frame)
        extract_frame.grid(row=0, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(0, 5))
        
        self.extract_links = tk.BooleanVar(value=True)
        ttk.Checkbutton(extract_frame, text="ë§í¬ ì¶”ì¶œ", variable=self.extract_links).grid(row=0, column=0, sticky=tk.W)
        
        self.extract_images = tk.BooleanVar(value=True)
        ttk.Checkbutton(extract_frame, text="ì´ë¯¸ì§€ ì¶”ì¶œ", variable=self.extract_images).grid(row=0, column=1, sticky=tk.W, padx=(10, 0))
        
        self.extract_text = tk.BooleanVar(value=True)
        ttk.Checkbutton(extract_frame, text="í…ìŠ¤íŠ¸ ì¶”ì¶œ", variable=self.extract_text).grid(row=0, column=2, sticky=tk.W, padx=(10, 0))
        
        # ê³ ê¸‰ ì˜µì…˜
        advanced_frame = ttk.Frame(options_frame)
        advanced_frame.grid(row=1, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(5, 0))
        
        # ë¸Œë¼ìš°ì € ëª¨ë“œ
        ttk.Label(advanced_frame, text="ë¸Œë¼ìš°ì € ëª¨ë“œ:").grid(row=0, column=0, sticky=tk.W)
        self.browser_mode = tk.StringVar(value="requests")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë¸Œë¼ìš°ì € ëª¨ë“œ ë¦¬ìŠ¤íŠ¸
        browser_modes = ["requests", "selenium"]
        if PLAYWRIGHT_AVAILABLE:
            browser_modes.append("playwright")
        
        browser_combo = ttk.Combobox(advanced_frame, textvariable=self.browser_mode, values=browser_modes, width=12)
        browser_combo.grid(row=0, column=1, sticky=tk.W, padx=(5, 10))
        
        # í˜ì´ì§€ ìˆ˜
        ttk.Label(advanced_frame, text="í˜ì´ì§€ ìˆ˜:").grid(row=0, column=2, sticky=tk.W)
        self.max_pages = tk.StringVar(value="1")
        ttk.Entry(advanced_frame, textvariable=self.max_pages, width=5).grid(row=0, column=3, sticky=tk.W, padx=(5, 10))
        
        # í¬ë¡¤ë§ ê°„ê²©
        ttk.Label(advanced_frame, text="ê°„ê²©(ì´ˆ):").grid(row=0, column=4, sticky=tk.W)
        self.crawl_delay = tk.StringVar(value="1")
        ttk.Entry(advanced_frame, textvariable=self.crawl_delay, width=5).grid(row=0, column=5, sticky=tk.W, padx=(5, 10))
        
        # ì‚¬ì´íŠ¸ë³„ ì„¤ì •
        site_frame = ttk.Frame(options_frame)
        site_frame.grid(row=2, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(5, 0))
        
        ttk.Label(site_frame, text="ì‚¬ì´íŠ¸ ì¢…ë¥˜:").grid(row=0, column=0, sticky=tk.W)
        self.site_type = tk.StringVar(value="ì¼ë°˜")
        site_combo = ttk.Combobox(site_frame, textvariable=self.site_type, 
                                values=["ì¼ë°˜", "ë„¤ì´ë²„ ì‡¼í•‘", "ì¸ìŠ¤íƒ€ê·¸ë¨", "ë¶€ë™ì‚°"], width=12)
        site_combo.grid(row=0, column=1, sticky=tk.W, padx=(5, 10))
        
        # ì¶”ì¶œí•  ë°ì´í„° ì„ íƒ
        data_frame = ttk.Frame(options_frame)
        data_frame.grid(row=3, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(5, 0))
        
        ttk.Label(data_frame, text="ì¶”ì¶œ ë°ì´í„°:").grid(row=0, column=0, sticky=tk.W)
        self.extract_title = tk.BooleanVar(value=True)
        ttk.Checkbutton(data_frame, text="ì œëª©", variable=self.extract_title).grid(row=0, column=1, sticky=tk.W, padx=(5, 0))
        
        self.extract_price = tk.BooleanVar(value=True)
        ttk.Checkbutton(data_frame, text="ê°€ê²©", variable=self.extract_price).grid(row=0, column=2, sticky=tk.W, padx=(5, 0))
        
        self.extract_date = tk.BooleanVar(value=True)
        ttk.Checkbutton(data_frame, text="ë‚ ì§œ", variable=self.extract_date).grid(row=0, column=3, sticky=tk.W, padx=(5, 0))
        
        self.extract_description = tk.BooleanVar(value=False)
        ttk.Checkbutton(data_frame, text="ì„¤ëª…", variable=self.extract_description).grid(row=0, column=4, sticky=tk.W, padx=(5, 0))
        
        self.extract_images = tk.BooleanVar(value=False)
        ttk.Checkbutton(data_frame, text="ì´ë¯¸ì§€", variable=self.extract_images).grid(row=0, column=5, sticky=tk.W, padx=(5, 0))
        
        # ì§„í–‰ìƒí™© í‘œì‹œ
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # ì—‘ì…€ ì €ì¥ ë²„íŠ¼
        export_frame = ttk.Frame(main_frame)
        export_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        self.export_button = ttk.Button(export_frame, text="ì—‘ì…€ë¡œ ì €ì¥", command=self.export_to_excel, state='disabled')
        self.export_button.pack(side=tk.RIGHT)
        
        ttk.Label(export_frame, text="í¬ë¡¤ë§ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.").pack(side=tk.LEFT)
        
        # ê²°ê³¼ í‘œì‹œ ì˜ì—­
        result_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ê²°ê³¼", padding="5")
        result_frame.grid(row=4, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # íƒ­ ìœ„ì ¯
        self.notebook = ttk.Notebook(result_frame)
        self.notebook.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í˜ì´ì§€ ì •ë³´ íƒ­
        self.info_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.info_frame, text="í˜ì´ì§€ ì •ë³´")
        
        self.info_text = scrolledtext.ScrolledText(self.info_frame, height=8, wrap=tk.WORD)
        self.info_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ë§í¬ íƒ­
        self.links_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.links_frame, text="ë§í¬")
        
        self.links_text = scrolledtext.ScrolledText(self.links_frame, height=8, wrap=tk.WORD)
        self.links_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ì´ë¯¸ì§€ íƒ­
        self.images_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.images_frame, text="ì´ë¯¸ì§€")
        
        self.images_text = scrolledtext.ScrolledText(self.images_frame, height=8, wrap=tk.WORD)
        self.images_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í…ìŠ¤íŠ¸ íƒ­
        self.content_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.content_frame, text="í…ìŠ¤íŠ¸ ë‚´ìš©")
        
        self.content_text = scrolledtext.ScrolledText(self.content_frame, height=8, wrap=tk.WORD)
        self.content_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ê²°ê³¼ í…Œì´ë¸” íƒ­
        self.table_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.table_frame, text="ê²°ê³¼ í…Œì´ë¸”")
        
        # í…Œì´ë¸” ë·° ìƒì„±
        self.create_table_view()
        
        # ì‚¬ì´íŠ¸ ì¶”ì²œ íƒ­
        self.recommend_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.recommend_frame, text="ì‚¬ì´íŠ¸ ì¶”ì²œ")
        
        self.recommend_text = scrolledtext.ScrolledText(self.recommend_frame, height=8, wrap=tk.WORD)
        self.recommend_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ê¸°ìˆ ìŠ¤íƒ ì„¤ëª… íƒ­
        self.tech_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.tech_frame, text="ê¸°ìˆ ìŠ¤íƒ ê°€ì´ë“œ")
        
        self.tech_text = scrolledtext.ScrolledText(self.tech_frame, height=8, wrap=tk.WORD)
        self.tech_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ì•Œë¦¼ ì„¤ì • íƒ­
        self.alert_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.alert_frame, text="ì•Œë¦¼ ì„¤ì •")
        self.setup_alert_tab()
        
        # ë°ì´í„° ë¶„ì„ íƒ­
        self.analysis_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.analysis_frame, text="ë°ì´í„° ë¶„ì„")
        self.setup_analysis_tab()
        
        # ìŠ¤ì¼€ì¤„ë§ íƒ­
        self.schedule_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.schedule_frame, text="ìŠ¤ì¼€ì¤„ë§")
        self.setup_schedule_tab()
        
        # ìƒíƒœ í‘œì‹œì¤„
        self.status_var = tk.StringVar(value="ì¤€ë¹„")
        status_bar = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN)
        status_bar.grid(row=5, column=0, columnspan=2, sticky=(tk.W, tk.E))
        
        # ê·¸ë¦¬ë“œ ê°€ì¤‘ì¹˜ ì„¤ì •
        root.columnconfigure(0, weight=1)
        root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(4, weight=1)
        result_frame.columnconfigure(0, weight=1)
        result_frame.rowconfigure(0, weight=1)
        
        # ê° íƒ­ì˜ ê·¸ë¦¬ë“œ ì„¤ì •
        for frame in [self.info_frame, self.links_frame, self.images_frame, self.content_frame, 
                      self.table_frame, self.recommend_frame, self.tech_frame, self.alert_frame, 
                      self.analysis_frame, self.schedule_frame]:
            frame.columnconfigure(0, weight=1)
            frame.rowconfigure(0, weight=1)
        
        url_frame.columnconfigure(1, weight=1)
        
        # í¬ë¡¤ë§ ìƒíƒœ ë° ë°ì´í„° ì €ì¥ìš© ë³€ìˆ˜
        self.is_crawling = False
        self.crawled_data = []
        self.driver = None
        self.playwright = None
        self.browser = None
        self.page = None
        self.current_page = 1
        self.retry_count = 0
        self.max_retries = 3
        
        # ì•Œë¦¼ ë° ë¶„ì„ ê´€ë ¨ ë³€ìˆ˜
        self.alert_settings = {
            'email_enabled': False,
            'notification_enabled': True,
            'price_threshold': 0,
            'keywords': []
        }
        self.historical_data = []
        self.monitoring_active = False
        
        # ìŠ¤ì¼€ì¤„ë§ ê´€ë ¨ ë³€ìˆ˜
        self.scheduler = BackgroundScheduler()
        self.scheduler.start()
        self.scheduled_jobs = []
        
        # ì¤‘ë‹¨ì  ì¬ì‹œì‘ ê´€ë ¨ ë³€ìˆ˜
        self.checkpoint_file = "crawling_checkpoint.pkl"
        self.current_task = None
        self.task_progress = {
            'total_pages': 0,
            'completed_pages': 0,
            'failed_pages': [],
            'current_url': '',
            'settings': {}
        }
        
        # íƒ­ ë‚´ìš© ì´ˆê¸°í™”
        self.load_static_content()
    
    def setup_alert_tab(self):
        """ì•Œë¦¼ ì„¤ì • íƒ­ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
        # ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ í”„ë ˆì„
        canvas = tk.Canvas(self.alert_frame)
        scrollbar = ttk.Scrollbar(self.alert_frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # ì•Œë¦¼ ì„¤ì • ì„¹ì…˜
        alert_config = ttk.LabelFrame(scrollable_frame, text="ì•Œë¦¼ ì„¤ì •", padding="10")
        alert_config.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 10), padx=10)
        
        # ë°ìŠ¤í¬íƒ‘ ì•Œë¦¼ ì„¤ì •
        self.notification_enabled = tk.BooleanVar(value=True)
        ttk.Checkbutton(alert_config, text="ë°ìŠ¤í¬íƒ‘ ì•Œë¦¼ ì‚¬ìš©", 
                       variable=self.notification_enabled).grid(row=0, column=0, sticky=tk.W, pady=2)
        
        # ì´ë©”ì¼ ì•Œë¦¼ ì„¤ì •
        self.email_enabled = tk.BooleanVar(value=False)
        ttk.Checkbutton(alert_config, text="ì´ë©”ì¼ ì•Œë¦¼ ì‚¬ìš©", 
                       variable=self.email_enabled).grid(row=1, column=0, sticky=tk.W, pady=2)
        
        # ì´ë©”ì¼ ì„¤ì • í”„ë ˆì„
        email_frame = ttk.LabelFrame(alert_config, text="ì´ë©”ì¼ ì„¤ì •", padding="5")
        email_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=5)
        
        ttk.Label(email_frame, text="ë°œì‹  ì´ë©”ì¼:").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        self.sender_email = tk.StringVar()
        ttk.Entry(email_frame, textvariable=self.sender_email, width=30).grid(row=0, column=1, sticky=(tk.W, tk.E))
        
        ttk.Label(email_frame, text="ì•± ë¹„ë°€ë²ˆí˜¸:").grid(row=1, column=0, sticky=tk.W, padx=(0, 5))
        self.sender_password = tk.StringVar()
        ttk.Entry(email_frame, textvariable=self.sender_password, show="*", width=30).grid(row=1, column=1, sticky=(tk.W, tk.E))
        
        ttk.Label(email_frame, text="ìˆ˜ì‹  ì´ë©”ì¼:").grid(row=2, column=0, sticky=tk.W, padx=(0, 5))
        self.receiver_email = tk.StringVar()
        ttk.Entry(email_frame, textvariable=self.receiver_email, width=30).grid(row=2, column=1, sticky=(tk.W, tk.E))
        
        # ê°€ê²© ì•Œë¦¼ ì„¤ì •
        price_frame = ttk.LabelFrame(scrollable_frame, text="ê°€ê²© ì•Œë¦¼ ì„¤ì •", padding="10")
        price_frame.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=(0, 10), padx=10)
        
        ttk.Label(price_frame, text="ê°€ê²© ë³€ë™ ì„ê³„ê°’:").grid(row=0, column=0, sticky=tk.W)
        self.price_threshold = tk.StringVar(value="10")
        ttk.Entry(price_frame, textvariable=self.price_threshold, width=10).grid(row=0, column=1, sticky=tk.W, padx=(5, 5))
        ttk.Label(price_frame, text="%").grid(row=0, column=2, sticky=tk.W)
        
        # í‚¤ì›Œë“œ ì•Œë¦¼ ì„¤ì •
        keyword_frame = ttk.LabelFrame(scrollable_frame, text="í‚¤ì›Œë“œ ì•Œë¦¼ ì„¤ì •", padding="10")
        keyword_frame.grid(row=2, column=0, sticky=(tk.W, tk.E), pady=(0, 10), padx=10)
        
        ttk.Label(keyword_frame, text="ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ:").grid(row=0, column=0, sticky=tk.W)
        self.keyword_entry = tk.StringVar()
        keyword_entry_widget = ttk.Entry(keyword_frame, textvariable=self.keyword_entry, width=30)
        keyword_entry_widget.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(5, 5))
        
        ttk.Button(keyword_frame, text="ì¶”ê°€", command=self.add_keyword).grid(row=0, column=2, padx=(5, 0))
        
        # í‚¤ì›Œë“œ ëª©ë¡
        self.keyword_listbox = tk.Listbox(keyword_frame, height=4)
        self.keyword_listbox.grid(row=1, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(5, 0))
        
        ttk.Button(keyword_frame, text="í‚¤ì›Œë“œ ì‚­ì œ", command=self.remove_keyword).grid(row=2, column=0, pady=(5, 0))
        
        # ëª¨ë‹ˆí„°ë§ ì œì–´ ë²„íŠ¼
        control_frame = ttk.LabelFrame(scrollable_frame, text="ëª¨ë‹ˆí„°ë§ ì œì–´", padding="10")
        control_frame.grid(row=3, column=0, sticky=(tk.W, tk.E), pady=(0, 10), padx=10)
        
        self.monitor_button = ttk.Button(control_frame, text="ëª¨ë‹ˆí„°ë§ ì‹œì‘", command=self.toggle_monitoring)
        self.monitor_button.grid(row=0, column=0, padx=(0, 5))
        
        ttk.Button(control_frame, text="ì•Œë¦¼ í…ŒìŠ¤íŠ¸", command=self.test_notification).grid(row=0, column=1, padx=(5, 0))
        
        # ìŠ¤í¬ë¡¤ë°” ì„¤ì •
        canvas.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        
    def setup_analysis_tab(self):
        """ë°ì´í„° ë¶„ì„ íƒ­ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
        # ë©”ì¸ í”„ë ˆì„
        main_analysis_frame = ttk.Frame(self.analysis_frame)
        main_analysis_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), padx=10, pady=10)
        
        # ë¶„ì„ ì˜µì…˜ í”„ë ˆì„
        options_frame = ttk.LabelFrame(main_analysis_frame, text="ë¶„ì„ ì˜µì…˜", padding="10")
        options_frame.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # ì°¨íŠ¸ íƒ€ì… ì„ íƒ
        ttk.Label(options_frame, text="ì°¨íŠ¸ íƒ€ì…:").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        self.chart_type = tk.StringVar(value="ê°€ê²© ì¶”ì´")
        chart_combo = ttk.Combobox(options_frame, textvariable=self.chart_type, 
                                 values=["ê°€ê²© ì¶”ì´", "í‚¤ì›Œë“œ ë¹ˆë„", "ì‚¬ì´íŠ¸ë³„ í†µê³„", "ì‹œê°„ëŒ€ë³„ ë¶„í¬"])
        chart_combo.grid(row=0, column=1, sticky=tk.W, padx=(0, 10))
        
        # ë¶„ì„ ë²„íŠ¼
        ttk.Button(options_frame, text="ì°¨íŠ¸ ìƒì„±", command=self.generate_chart).grid(row=0, column=2, padx=(5, 0))
        ttk.Button(options_frame, text="ë¦¬í¬íŠ¸ ìƒì„±", command=self.generate_report).grid(row=0, column=3, padx=(5, 0))
        
        # ì°¨íŠ¸ í‘œì‹œ ì˜ì—­
        self.chart_frame = ttk.LabelFrame(main_analysis_frame, text="ì°¨íŠ¸", padding="5")
        self.chart_frame.grid(row=1, column=0, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # í†µê³„ ì •ë³´ í‘œì‹œ ì˜ì—­
        stats_frame = ttk.LabelFrame(main_analysis_frame, text="í†µê³„ ì •ë³´", padding="5")
        stats_frame.grid(row=2, column=0, sticky=(tk.W, tk.E), pady=(0, 10))
        
        self.stats_text = scrolledtext.ScrolledText(stats_frame, height=6, wrap=tk.WORD)
        self.stats_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ì°¨íŠ¸ ì‚¬ìš© ë¶ˆê°€ ì‹œ ì•ˆë‚´
        if not CHART_AVAILABLE:
            ttk.Label(self.chart_frame, text="ì°¨íŠ¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.\n'pip install matplotlib seaborn'").grid(row=0, column=0, pady=20)
    
    def setup_schedule_tab(self):
        """ìŠ¤ì¼€ì¤„ë§ íƒ­ì„ êµ¬ì„±í•©ë‹ˆë‹¤."""
        # ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ í”„ë ˆì„
        canvas = tk.Canvas(self.schedule_frame)
        scrollbar = ttk.Scrollbar(self.schedule_frame, orient="vertical", command=canvas.yview)
        scrollable_frame = ttk.Frame(canvas)
        
        scrollable_frame.bind(
            "<Configure>",
            lambda e: canvas.configure(scrollregion=canvas.bbox("all"))
        )
        
        canvas.create_window((0, 0), window=scrollable_frame, anchor="nw")
        canvas.configure(yscrollcommand=scrollbar.set)
        
        # ìŠ¤ì¼€ì¤„ ì„¤ì • ì„¹ì…˜
        schedule_config = ttk.LabelFrame(scrollable_frame, text="ìŠ¤ì¼€ì¤„ ì„¤ì •", padding="10")
        schedule_config.grid(row=0, column=0, sticky=(tk.W, tk.E), pady=(0, 10), padx=10)
        
        # ìŠ¤ì¼€ì¤„ íƒ€ì… ì„ íƒ
        ttk.Label(schedule_config, text="ì‹¤í–‰ ì£¼ê¸°:").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        self.schedule_type = tk.StringVar(value="ì¼íšŒì„±")
        schedule_combo = ttk.Combobox(schedule_config, textvariable=self.schedule_type, 
                                    values=["ì¼íšŒì„±", "ë§¤ì¼", "ë§¤ì£¼", "ë§¤ì›”"], width=15)
        schedule_combo.grid(row=0, column=1, sticky=tk.W, padx=(0, 10))
        
        # ì‹œê°„ ì„¤ì •
        ttk.Label(schedule_config, text="ì‹¤í–‰ ì‹œê°„:").grid(row=1, column=0, sticky=tk.W, padx=(0, 5))
        time_frame = ttk.Frame(schedule_config)
        time_frame.grid(row=1, column=1, sticky=tk.W, pady=5)
        
        self.schedule_hour = tk.StringVar(value="09")
        ttk.Spinbox(time_frame, from_=0, to=23, textvariable=self.schedule_hour, width=5, format="%02.0f").grid(row=0, column=0)
        ttk.Label(time_frame, text=":").grid(row=0, column=1)
        self.schedule_minute = tk.StringVar(value="00")
        ttk.Spinbox(time_frame, from_=0, to=59, textvariable=self.schedule_minute, width=5, format="%02.0f").grid(row=0, column=2)
        
        # ìš”ì¼ ì„ íƒ (ì£¼ê°„ ìŠ¤ì¼€ì¤„ìš©)
        ttk.Label(schedule_config, text="ìš”ì¼ (ë§¤ì£¼):").grid(row=2, column=0, sticky=tk.W, padx=(0, 5))
        self.schedule_weekday = tk.StringVar(value="ì›”ìš”ì¼")
        weekday_combo = ttk.Combobox(schedule_config, textvariable=self.schedule_weekday,
                                   values=["ì›”ìš”ì¼", "í™”ìš”ì¼", "ìˆ˜ìš”ì¼", "ëª©ìš”ì¼", "ê¸ˆìš”ì¼", "í† ìš”ì¼", "ì¼ìš”ì¼"], width=15)
        weekday_combo.grid(row=2, column=1, sticky=tk.W)
        
        # ì¼ì ì„¤ì • (ì›”ê°„ ìŠ¤ì¼€ì¤„ìš©)
        ttk.Label(schedule_config, text="ì¼ì (ë§¤ì›”):").grid(row=3, column=0, sticky=tk.W, padx=(0, 5))
        self.schedule_day = tk.StringVar(value="1")
        ttk.Spinbox(schedule_config, from_=1, to=28, textvariable=self.schedule_day, width=5).grid(row=3, column=1, sticky=tk.W)
        
        # ì´ë©”ì¼ ê²°ê³¼ ì „ì†¡ ì„¤ì •
        self.email_results = tk.BooleanVar(value=True)
        ttk.Checkbutton(schedule_config, text="ê²°ê³¼ë¥¼ ì´ë©”ì¼ë¡œ ì „ì†¡", 
                       variable=self.email_results).grid(row=4, column=0, columnspan=2, sticky=tk.W, pady=5)
        
        # ìŠ¤ì¼€ì¤„ ì œì–´ ë²„íŠ¼
        control_frame = ttk.Frame(schedule_config)
        control_frame.grid(row=5, column=0, columnspan=2, pady=10)
        
        ttk.Button(control_frame, text="ìŠ¤ì¼€ì¤„ ì¶”ê°€", command=self.add_schedule).grid(row=0, column=0, padx=(0, 5))
        ttk.Button(control_frame, text="ìŠ¤ì¼€ì¤„ ì‚­ì œ", command=self.remove_schedule).grid(row=0, column=1, padx=(5, 0))
        
        # í™œì„± ìŠ¤ì¼€ì¤„ ëª©ë¡
        schedule_list_frame = ttk.LabelFrame(scrollable_frame, text="í™œì„± ìŠ¤ì¼€ì¤„", padding="10")
        schedule_list_frame.grid(row=1, column=0, sticky=(tk.W, tk.E), pady=(0, 10), padx=10)
        
        # ìŠ¤ì¼€ì¤„ ëª©ë¡ íŠ¸ë¦¬ë·°
        columns = ('ID', 'Type', 'Time', 'Next Run', 'Status')
        self.schedule_tree = ttk.Treeview(schedule_list_frame, columns=columns, show='headings', height=6)
        
        self.schedule_tree.heading('ID', text='ID')
        self.schedule_tree.heading('Type', text='íƒ€ì…')
        self.schedule_tree.heading('Time', text='ì‹œê°„')
        self.schedule_tree.heading('Next Run', text='ë‹¤ìŒ ì‹¤í–‰')
        self.schedule_tree.heading('Status', text='ìƒíƒœ')
        
        self.schedule_tree.column('ID', width=50)
        self.schedule_tree.column('Type', width=80)
        self.schedule_tree.column('Time', width=100)
        self.schedule_tree.column('Next Run', width=150)
        self.schedule_tree.column('Status', width=80)
        
        self.schedule_tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ë³µêµ¬ ê¸°ëŠ¥ ì„¹ì…˜
        recovery_frame = ttk.LabelFrame(scrollable_frame, text="ë³µêµ¬ ê¸°ëŠ¥", padding="10")
        recovery_frame.grid(row=2, column=0, sticky=(tk.W, tk.E), pady=(0, 10), padx=10)
        
        # ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ
        self.checkpoint_status = tk.StringVar(value="ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
        ttk.Label(recovery_frame, textvariable=self.checkpoint_status).grid(row=0, column=0, columnspan=2, pady=5)
        
        # ë³µêµ¬ ë²„íŠ¼
        recovery_control = ttk.Frame(recovery_frame)
        recovery_control.grid(row=1, column=0, columnspan=2, pady=5)
        
        ttk.Button(recovery_control, text="ë§ˆì§€ë§‰ ì‘ì—… ë³µêµ¬", command=self.restore_last_task).grid(row=0, column=0, padx=(0, 5))
        ttk.Button(recovery_control, text="ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ", command=self.clear_checkpoint).grid(row=0, column=1, padx=(5, 0))
        
        # ìë™ ì €ì¥ ì„¤ì •
        self.auto_save = tk.BooleanVar(value=True)
        ttk.Checkbutton(recovery_frame, text="ìë™ ì²´í¬í¬ì¸íŠ¸ ì €ì¥", 
                       variable=self.auto_save).grid(row=2, column=0, columnspan=2, sticky=tk.W, pady=5)
        
        # ìŠ¤í¬ë¡¤ë°” ì„¤ì •
        canvas.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        
        # ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.update_checkpoint_status()
    
    def setup_url_entry_bindings(self):
        """URL ì…ë ¥ì°½ì— í‚¤ë³´ë“œ ë‹¨ì¶•í‚¤ ë°”ì¸ë”©ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        import platform
        
        # ìš´ì˜ì²´ì œ ê°ì§€
        system = platform.system()
        
        if system == "Darwin":  # macOS
            # Cmd+A: ì „ì²´ ì„ íƒ
            self.url_entry.bind('<Command-a>', self.select_all_url)
            # Cmd+C: ë³µì‚¬
            self.url_entry.bind('<Command-c>', self.copy_url)
            # Cmd+V: ë¶™ì—¬ë„£ê¸°
            self.url_entry.bind('<Command-v>', self.paste_url)
            # Cmd+X: ì˜ë¼ë‚´ê¸°
            self.url_entry.bind('<Command-x>', self.cut_url)
            # Cmd+Z: ì‹¤í–‰ì·¨ì†Œ
            self.url_entry.bind('<Command-z>', self.undo_url)
        else:  # Windows, Linux
            # Ctrl+A: ì „ì²´ ì„ íƒ
            self.url_entry.bind('<Control-a>', self.select_all_url)
            # Ctrl+C: ë³µì‚¬
            self.url_entry.bind('<Control-c>', self.copy_url)
            # Ctrl+V: ë¶™ì—¬ë„£ê¸°
            self.url_entry.bind('<Control-v>', self.paste_url)
            # Ctrl+X: ì˜ë¼ë‚´ê¸°
            self.url_entry.bind('<Control-x>', self.cut_url)
            # Ctrl+Z: ì‹¤í–‰ì·¨ì†Œ
            self.url_entry.bind('<Control-z>', self.undo_url)
        
        # ìš°í´ë¦­ ì»¨í…ìŠ¤íŠ¸ ë©”ë‰´ ì¶”ê°€
        self.url_entry.bind('<Button-2>', self.show_context_menu)  # macOS
        self.url_entry.bind('<Button-3>', self.show_context_menu)  # Windows/Linux
        
        # í¬ì»¤ìŠ¤ ì‹œ ì „ì²´ ì„ íƒ (ì„ íƒì‚¬í•­)
        self.url_entry.bind('<FocusIn>', self.on_url_focus)
        
        # ì—”í„°í‚¤ë¡œ í¬ë¡¤ë§ ì‹œì‘
        self.url_entry.bind('<Return>', self.on_url_enter)
        self.url_entry.bind('<KP_Enter>', self.on_url_enter)  # ìˆ«ìíŒ¨ë“œ ì—”í„°í‚¤ë„ ì§€ì›
    
    def select_all_url(self, event=None):
        """URL ì…ë ¥ì°½ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        self.url_entry.select_range(0, tk.END)
        return 'break'
    
    def copy_url(self, event=None):
        """ì„ íƒëœ í…ìŠ¤íŠ¸ë¥¼ í´ë¦½ë³´ë“œë¡œ ë³µì‚¬í•©ë‹ˆë‹¤."""
        try:
            if self.url_entry.selection_present():
                text = self.url_entry.selection_get()
                self.root.clipboard_clear()
                self.root.clipboard_append(text)
        except tk.TclError:
            pass
        return 'break'
    
    def paste_url(self, event=None):
        """í´ë¦½ë³´ë“œì˜ ë‚´ìš©ì„ í˜„ì¬ ì»¤ì„œ ìœ„ì¹˜ì— ë¶™ì—¬ë„£ìŠµë‹ˆë‹¤."""
        try:
            clipboard_text = self.root.clipboard_get()
            # í˜„ì¬ ì„ íƒëœ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ëŒ€ì²´, ì—†ìœ¼ë©´ ì»¤ì„œ ìœ„ì¹˜ì— ì‚½ì…
            if self.url_entry.selection_present():
                self.url_entry.delete(tk.SEL_FIRST, tk.SEL_LAST)
            self.url_entry.insert(tk.INSERT, clipboard_text)
        except tk.TclError:
            pass
        return 'break'
    
    def cut_url(self, event=None):
        """ì„ íƒëœ í…ìŠ¤íŠ¸ë¥¼ ì˜ë¼ë‚´ì–´ í´ë¦½ë³´ë“œë¡œ ë³µì‚¬í•©ë‹ˆë‹¤."""
        try:
            if self.url_entry.selection_present():
                text = self.url_entry.selection_get()
                self.root.clipboard_clear()
                self.root.clipboard_append(text)
                self.url_entry.delete(tk.SEL_FIRST, tk.SEL_LAST)
        except tk.TclError:
            pass
        return 'break'
    
    def undo_url(self, event=None):
        """ë§ˆì§€ë§‰ ì‘ì—…ì„ ì‹¤í–‰ì·¨ì†Œí•©ë‹ˆë‹¤."""
        try:
            self.url_entry.edit_undo()
        except tk.TclError:
            pass
        return 'break'
    
    def show_context_menu(self, event):
        """ìš°í´ë¦­ ì»¨í…ìŠ¤íŠ¸ ë©”ë‰´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        # ì»¨í…ìŠ¤íŠ¸ ë©”ë‰´ ìƒì„±
        context_menu = tk.Menu(self.root, tearoff=0)
        context_menu.add_command(label="ì˜ë¼ë‚´ê¸°", command=lambda: self.cut_url())
        context_menu.add_command(label="ë³µì‚¬", command=lambda: self.copy_url())
        context_menu.add_command(label="ë¶™ì—¬ë„£ê¸°", command=lambda: self.paste_url())
        context_menu.add_separator()
        context_menu.add_command(label="ì „ì²´ ì„ íƒ", command=lambda: self.select_all_url())
        context_menu.add_separator()
        context_menu.add_command(label="ì‹¤í–‰ì·¨ì†Œ", command=lambda: self.undo_url())
        
        try:
            context_menu.tk_popup(event.x_root, event.y_root)
        finally:
            context_menu.grab_release()
    
    def on_url_focus(self, event=None):
        """URL ì…ë ¥ì°½ì´ í¬ì»¤ìŠ¤ë¥¼ ë°›ì„ ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        # ê¸°ë³¸ URLì´ ìˆì„ ë•Œë§Œ ì „ì²´ ì„ íƒ (ì„ íƒì‚¬í•­)
        if self.url_var.get() == "https://example.com":
            self.root.after(50, self.select_all_url)  # ì•½ê°„ì˜ ì§€ì—° í›„ ì„ íƒ
    
    def on_url_enter(self, event=None):
        """URL ì…ë ¥ì°½ì—ì„œ ì—”í„°í‚¤ë¥¼ ëˆ„ë¥¼ ë•Œ í˜¸ì¶œë©ë‹ˆë‹¤."""
        # í¬ë¡¤ë§ì´ ì§„í–‰ ì¤‘ì´ ì•„ë‹ˆê³  í¬ë¡¤ë§ ë²„íŠ¼ì´ í™œì„±í™”ë˜ì–´ ìˆì„ ë•Œë§Œ ì‹¤í–‰
        if not self.is_crawling and self.crawl_button['state'] != 'disabled':
            # URLì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€ í™•ì¸
            url = self.url_var.get().strip()
            if url:
                # í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼ í´ë¦­ê³¼ ë™ì¼í•œ ë™ì‘
                self.start_crawling()
                return 'break'  # ì´ë²¤íŠ¸ ì „íŒŒ ì¤‘ë‹¨
        return None
    
    def create_table_view(self):
        """ê²°ê³¼ í…Œì´ë¸” ë·°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # í…Œì´ë¸” í”„ë ˆì„
        table_container = ttk.Frame(self.table_frame)
        table_container.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í…Œì´ë¸” (Treeview) ìƒì„±
        columns = ('Type', 'Title', 'URL', 'Description')
        self.tree = ttk.Treeview(table_container, columns=columns, show='headings', height=15)
        
        # ì»¬ëŸ¼ í—¤ë” ì„¤ì •
        self.tree.heading('Type', text='íƒ€ì…')
        self.tree.heading('Title', text='ì œëª©/í…ìŠ¤íŠ¸')
        self.tree.heading('URL', text='URL')
        self.tree.heading('Description', text='ì„¤ëª…')
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree.column('Type', width=80, minwidth=60)
        self.tree.column('Title', width=200, minwidth=150)
        self.tree.column('URL', width=300, minwidth=200)
        self.tree.column('Description', width=150, minwidth=100)
        
        # ìŠ¤í¬ë¡¤ë°” ì¶”ê°€
        v_scrollbar = ttk.Scrollbar(table_container, orient=tk.VERTICAL, command=self.tree.yview)
        h_scrollbar = ttk.Scrollbar(table_container, orient=tk.HORIZONTAL, command=self.tree.xview)
        self.tree.configure(yscrollcommand=v_scrollbar.set, xscrollcommand=h_scrollbar.set)
        
        # ìœ„ì ¯ ë°°ì¹˜
        self.tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        v_scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        h_scrollbar.grid(row=1, column=0, sticky=(tk.W, tk.E))
        
        # ê·¸ë¦¬ë“œ ê°€ì¤‘ì¹˜ ì„¤ì •
        table_container.columnconfigure(0, weight=1)
        table_container.rowconfigure(0, weight=1)
    
    def stop_crawling(self):
        """í¬ë¡¤ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.is_crawling = False
        self.status_var.set("í¬ë¡¤ë§ ì¤‘ì§€ë¨")
        
        # Selenium ë“œë¼ì´ë²„ ì •ë¦¬
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None
        
        # Playwright ì •ë¦¬
        if self.page:
            try:
                self.page.close()
            except:
                pass
            self.page = None
        
        if self.browser:
            try:
                self.browser.close()
            except:
                pass
            self.browser = None
        
        if self.playwright:
            try:
                self.playwright.stop()
            except:
                pass
            self.playwright = None
    
    def start_crawling(self, scheduled=False):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        url = self.url_var.get().strip()
        if not url:
            if not scheduled:
                messagebox.showerror("ì˜¤ë¥˜", "URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            self.url_var.set(url)
        
        # í˜„ì¬ ì‘ì—… ì •ë³´ ì €ì¥
        self.current_task = {
            'url': url,
            'browser_mode': self.browser_mode.get(),
            'max_pages': int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1,
            'crawl_delay': float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1,
            'site_type': self.site_type.get(),
            'started_at': datetime.now(),
            'scheduled': scheduled
        }
        
        # ì§„í–‰ ìƒí™© ì´ˆê¸°í™”
        self.task_progress = {
            'total_pages': self.current_task['max_pages'],
            'completed_pages': 0,
            'failed_pages': [],
            'current_url': url,
            'settings': self.current_task.copy()
        }
        
        # UI ìƒíƒœ ë³€ê²½ (ìŠ¤ì¼€ì¤„ëœ ì‘ì—…ì´ ì•„ë‹ ë•Œë§Œ)
        if not scheduled:
            self.crawl_button.config(state='disabled')
            self.stop_button.config(state='normal')
            self.export_button.config(state='disabled')
            self.progress.start()
        
        self.status_var.set("í¬ë¡¤ë§ ì¤‘...")
        self.is_crawling = True
        
        # ê²°ê³¼ ì˜ì—­ ì´ˆê¸°í™”
        if not scheduled:
            self.clear_results()
        self.crawled_data = []
        self.current_page = 1
        self.retry_count = 0
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if self.auto_save.get():
            self.save_checkpoint()
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        if self.browser_mode.get() == "selenium":
            thread = threading.Thread(target=self.crawl_with_selenium_checkpoint, args=(url, scheduled))
        elif self.browser_mode.get() == "playwright":
            thread = threading.Thread(target=self.crawl_with_playwright_checkpoint, args=(url, scheduled))
        else:
            thread = threading.Thread(target=self.crawl_website_checkpoint, args=(url, scheduled))
        thread.daemon = True
        thread.start()
    
    def clear_results(self):
        """ê²°ê³¼ ì˜ì—­ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.info_text.delete(1.0, tk.END)
        self.links_text.delete(1.0, tk.END)
        self.images_text.delete(1.0, tk.END)
        self.content_text.delete(1.0, tk.END)
        
        # í…Œì´ë¸” ì´ˆê¸°í™”
        for item in self.tree.get_children():
            self.tree.delete(item)
    
    def load_static_content(self):
        """ì •ì ì¸ íƒ­ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        # ì‚¬ì´íŠ¸ ì¶”ì²œ ë‚´ìš©
        recommend_content = """
ğŸŒ í¬ë¡¤ë§í•˜ê¸° ì‰¬ìš´ ì¶”ì²œ ì‚¬ì´íŠ¸ë“¤

ğŸ“° ë‰´ìŠ¤ ì‚¬ì´íŠ¸
â€¢ https://news.ycombinator.com - í•´ì»¤ë‰´ìŠ¤ (ê°„ë‹¨í•œ HTML êµ¬ì¡°)
â€¢ https://quotes.toscrape.com - í¬ë¡¤ë§ ì—°ìŠµìš© ì‚¬ì´íŠ¸
â€¢ https://books.toscrape.com - ë„ì„œ ì •ë³´ ì—°ìŠµ ì‚¬ì´íŠ¸

ğŸ›ï¸ ê³µê³µ ë°ì´í„°
â€¢ https://httpbin.org - HTTP í…ŒìŠ¤íŠ¸ìš© ì‚¬ì´íŠ¸
â€¢ https://jsonplaceholder.typicode.com - JSON API í…ŒìŠ¤íŠ¸
â€¢ https://reqres.in - API í…ŒìŠ¤íŠ¸ìš©

ğŸ“Š ì—°ìŠµìš© ì‚¬ì´íŠ¸
â€¢ https://scrape.center - í¬ë¡¤ë§ ì—°ìŠµ ì „ìš©
â€¢ https://webscraper.io/test-sites - ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
â€¢ https://example.com - ê°€ì¥ ê¸°ë³¸ì ì¸ í…ŒìŠ¤íŠ¸

ğŸ›’ ì „ììƒê±°ë˜ (ì£¼ì˜ í•„ìš”)
â€¢ ì¼ë¶€ ì „ììƒê±°ë˜ ì‚¬ì´íŠ¸ë“¤ (robots.txt ì¤€ìˆ˜ í•„ìˆ˜)
â€¢ ì˜¤í”ˆ ë§ˆì¼“ ìƒí’ˆ ì •ë³´ (ì´ìš©ì•½ê´€ í™•ì¸ í•„ìš”)

âš ï¸ ì£¼ì˜ì‚¬í•­
â€¢ robots.txt íŒŒì¼ì„ í•­ìƒ í™•ì¸í•˜ì„¸ìš”
â€¢ ê³¼ë„í•œ ìš”ì²­ì€ í”¼í•˜ì„¸ìš” (ì„œë²„ ë¶€í•˜ ë°©ì§€)
â€¢ ì €ì‘ê¶Œì´ ìˆëŠ” ì½˜í…ì¸ ëŠ” ì£¼ì˜í•˜ì„¸ìš”
â€¢ ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„°ëŠ” ìˆ˜ì§‘í•˜ì§€ ë§ˆì„¸ìš”
â€¢ ì›¹ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ì„ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”

ğŸ¯ ì´ˆë³´ì ì¶”ì²œ ìˆœì„œ
1. quotes.toscrape.com (ê¸°ë³¸ ì—°ìŠµ)
2. books.toscrape.com (í˜ì´ì§€ë„¤ì´ì…˜ ì—°ìŠµ)
3. news.ycombinator.com (ì‹¤ì œ ì‚¬ì´íŠ¸ ì—°ìŠµ)
4. ì›í•˜ëŠ” ì‚¬ì´íŠ¸ (ë‹¨ê³„ì  ì ‘ê·¼)
"""
        
        # ê¸°ìˆ ìŠ¤íƒ ê°€ì´ë“œ ë‚´ìš©
        tech_content = """
ğŸŒ ì‚¬ì´íŠ¸ ê¸°ìˆ ìŠ¤íƒë³„ í¬ë¡¤ë§ ë‚œì´ë„ ê°€ì´ë“œ

ğŸ“Š í¬ë¡¤ë§ ë‚œì´ë„ ë¶„ë¥˜

ğŸŸ¢ ì‰¬ì›€ (ì •ì  ì‚¬ì´íŠ¸)
â€¢ ì¼ë°˜ HTML/CSS ì‚¬ì´íŠ¸
â€¢ ì„œë²„ ì‚¬ì´ë“œ ë Œë”ë§ (SSR)
â€¢ WordPress, Jekyll ë“± ì •ì  ì‚¬ì´íŠ¸ ìƒì„±ê¸°

í¬ë¡¤ë§ ë°©ë²•:
â†’ requests + BeautifulSoupë¡œ ì¶©ë¶„
â†’ í˜ì´ì§€ ì†ŒìŠ¤ì— ëª¨ë“  ë°ì´í„°ê°€ í¬í•¨ë¨
â†’ ì˜ˆ: ë¸”ë¡œê·¸, ë‰´ìŠ¤ ì‚¬ì´íŠ¸, ê¸°ì—… í™ˆí˜ì´ì§€

ğŸŸ¡ ë³´í†µ (í•˜ì´ë¸Œë¦¬ë“œ)
â€¢ jQuery ê¸°ë°˜ ì‚¬ì´íŠ¸
â€¢ ì¼ë¶€ ë™ì  ë¡œë”©ì´ ìˆëŠ” ì‚¬ì´íŠ¸
â€¢ AJAXë¡œ ì¼ë¶€ ì½˜í…ì¸  ë¡œë“œ

í¬ë¡¤ë§ ë°©ë²•:
â†’ requests + BeautifulSoup (ê¸°ë³¸ ì½˜í…ì¸ )
â†’ API ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„ í›„ ì§ì ‘ í˜¸ì¶œ
â†’ í•„ìš”ì‹œ Seleniumìœ¼ë¡œ ë™ì  ë¶€ë¶„ë§Œ ì²˜ë¦¬

ğŸŸ  ì–´ë ¤ì›€ (SPA - Single Page Application)
â€¢ React, Vue.js, Angular ê¸°ë°˜
â€¢ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ë Œë”ë§ (CSR)
â€¢ ëŒ€ë¶€ë¶„ì˜ ì½˜í…ì¸ ê°€ JavaScriptë¡œ ìƒì„±

í¬ë¡¤ë§ ë°©ë²•:
â†’ Selenium ë˜ëŠ” Playwright í•„ìˆ˜
â†’ í˜ì´ì§€ ë¡œë”© ì™„ë£Œê¹Œì§€ ëŒ€ê¸° í•„ìš”
â†’ ì˜ˆ: ëª¨ë˜ ì „ììƒê±°ë˜, ì†Œì…œë¯¸ë””ì–´

ğŸ”´ ë§¤ìš° ì–´ë ¤ì›€ (ê³ ë„í™”ëœ SPA)
â€¢ Next.js, Nuxt.js (SSR + CSR í˜¼í•©)
â€¢ ë³µì¡í•œ ìƒíƒœ ê´€ë¦¬ (Redux, Vuex)
â€¢ ì§€ì—° ë¡œë”©, ë¬´í•œ ìŠ¤í¬ë¡¤
â€¢ ê°•ë ¥í•œ ë´‡ ì°¨ë‹¨ ì‹œìŠ¤í…œ

í¬ë¡¤ë§ ë°©ë²•:
â†’ Playwright + ê³ ê¸‰ ê¸°ë²• (ê¶Œì¥)
â†’ í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € + í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
â†’ API ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§
â†’ ì˜ˆ: Netflix, Instagram, LinkedIn

ğŸš€ ë¸Œë¼ìš°ì € ìë™í™” ë„êµ¬ ë¹„êµ

ğŸ“Š Requests vs Selenium vs Playwright
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ë„êµ¬      â”‚   ì†ë„     â”‚   ì•ˆì •ì„±   â”‚   í˜¸í™˜ì„±     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Requests    â”‚ ë§¤ìš° ë¹ ë¦„  â”‚ ë†’ìŒ       â”‚ ì •ì  ì‚¬ì´íŠ¸  â”‚
â”‚ Selenium    â”‚ ëŠë¦¼       â”‚ ë³´í†µ       â”‚ ëŒ€ë¶€ë¶„ ì‚¬ì´íŠ¸â”‚
â”‚ Playwright  â”‚ ë¹ ë¦„       â”‚ ë§¤ìš° ë†’ìŒ  â”‚ ëª¨ë“  ì‚¬ì´íŠ¸  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ Playwright ì¥ì :
â€¢ Seleniumë³´ë‹¤ 2-3ë°° ë¹ ë¦„
â€¢ ì•ˆì •ì ì¸ ìš”ì†Œ ëŒ€ê¸° ë° ì„ íƒ
â€¢ ë„¤íŠ¸ì›Œí¬ ì¸í„°ì…‰ì…˜ ì§€ì›
â€¢ ì—¬ëŸ¬ ë¸Œë¼ìš°ì € ì—”ì§„ ì§€ì› (Chromium, Firefox, WebKit)
â€¢ ìŠ¤í¬ë¦°ìƒ· ë° ë¹„ë””ì˜¤ ë…¹í™” ê¸°ëŠ¥
â€¢ í–¥ìƒëœ ë””ë²„ê¹… ë„êµ¬

ğŸ› ï¸ ê¸°ìˆ ìŠ¤íƒë³„ ìƒì„¸ ë¶„ì„

ğŸ“„ ì •ì  HTML ì‚¬ì´íŠ¸
íŠ¹ì§•: ì„œë²„ì—ì„œ ì™„ì „í•œ HTML ë°˜í™˜
ë„êµ¬: requests + BeautifulSoup
ì˜ˆì‹œ: ì •ë¶€ê¸°ê´€, í•™êµ, ì „í†µì  ê¸°ì—… ì‚¬ì´íŠ¸

ğŸ¯ WordPress/Drupal ì‚¬ì´íŠ¸
íŠ¹ì§•: PHP ê¸°ë°˜, ëŒ€ë¶€ë¶„ ì„œë²„ ë Œë”ë§
ë„êµ¬: requests + BeautifulSoup
ì£¼ì˜: í”ŒëŸ¬ê·¸ì¸ì— ë”°ë¼ ë™ì  ìš”ì†Œ ìˆì„ ìˆ˜ ìˆìŒ

âš¡ jQuery ê¸°ë°˜ ì‚¬ì´íŠ¸
íŠ¹ì§•: í˜ì´ì§€ ë¡œë“œ í›„ ì¼ë¶€ ì½˜í…ì¸  ì¶”ê°€ ë¡œë“œ
ë„êµ¬: requests (ê¸°ë³¸) + Selenium (ë™ì  ë¶€ë¶„)
ì „ëµ: Network íƒ­ì—ì„œ AJAX ìš”ì²­ ë¶„ì„

ğŸ”¥ React/Vue/Angular SPA
íŠ¹ì§•: ë¹ˆ HTML + JavaScriptë¡œ ëª¨ë“  ì½˜í…ì¸  ìƒì„±
ë„êµ¬: Selenium/Playwright í•„ìˆ˜
ì „ëµ: 
- componentDidMount ëŒ€ê¸°
- API í˜¸ì¶œ ì§ì ‘ ë¶„ì„ (ë” íš¨ìœ¨ì )
- SSR ë²„ì „ ì°¾ê¸° (SEOìš© í˜ì´ì§€)

ğŸª ì „ììƒê±°ë˜ í”Œë«í¼ë³„
â€¢ Shopify: Liquid í…œí”Œë¦¿, ëŒ€ë¶€ë¶„ ì„œë²„ ë Œë”ë§
â€¢ WooCommerce: WordPress ê¸°ë°˜, ë¹„êµì  ì‰¬ì›€
â€¢ Magento: ë³µì¡í•œ êµ¬ì¡°, ì¼ë¶€ AJAX
â€¢ ì»¤ìŠ¤í…€ React: ë§¤ìš° ì–´ë ¤ì›€

ğŸ“± ëª¨ë°”ì¼ ì•± ê¸°ë°˜ ì›¹
íŠ¹ì§•: ì•±ê³¼ ë™ì¼í•œ API ì‚¬ìš©, ê³ ë„í™”ëœ ë³´ì•ˆ
ë„êµ¬: API ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ + requests
ì „ëµ: ëª¨ë°”ì¼ ì•± íŒ¨í‚· ë¶„ì„

ğŸ›¡ï¸ ë´‡ ì°¨ë‹¨ ê¸°ìˆ ë³„ ëŒ€ì‘
â€¢ Cloudflare: User-Agent, í—¤ë” ì¡°ì‘
â€¢ reCAPTCHA: ìˆ˜ë™ í•´ê²° ë˜ëŠ” ìš°íšŒ
â€¢ Rate Limiting: ìš”ì²­ ê°„ê²© ì¡°ì ˆ
â€¢ JavaScript Challenge: í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € í•„ìˆ˜

ğŸ¯ ì‹¤ì „ íŒë³„ ë°©ë²•

1ï¸âƒ£ í˜ì´ì§€ ì†ŒìŠ¤ ë³´ê¸° (Ctrl+U)
â†’ ë‚´ìš©ì´ ë³´ì´ë©´: ì •ì  (ì‰¬ì›€)
â†’ ê±°ì˜ ë¹„ì–´ìˆìœ¼ë©´: SPA (ì–´ë ¤ì›€)

2ï¸âƒ£ ê°œë°œì ë„êµ¬ Network íƒ­
â†’ HTML í•˜ë‚˜ë§Œ: ì •ì 
â†’ ë§ì€ XHR/Fetch: ë™ì 

3ï¸âƒ£ JavaScript ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
â†’ ë‚´ìš©ì´ ê·¸ëŒ€ë¡œ: ì •ì 
â†’ ë‚´ìš©ì´ ì‚¬ë¼ì§: ë™ì 

4ï¸âƒ£ URL íŒ¨í„´ í™•ì¸
â†’ /page/1, /category/tech: ì „í†µì 
â†’ /#/page/1, /app: SPA ê°€ëŠ¥ì„±

ğŸ’¡ íš¨ìœ¨ì ì¸ ì ‘ê·¼ ì „ëµ

1. ì •ì  ë¶„ì„ ìš°ì„  (requests + BeautifulSoup)
2. ì•ˆ ë˜ë©´ API ë¶„ì„ (Network íƒ­)
3. ë§ˆì§€ë§‰ì— Selenium/Playwright ì‚¬ìš©
4. ê°€ëŠ¥í•˜ë©´ ëª¨ë°”ì¼ ë²„ì „ì´ë‚˜ RSS í™œìš©

âš ï¸ ì£¼ì˜ì‚¬í•­
â€¢ robots.txtì™€ ì´ìš©ì•½ê´€ í™•ì¸ í•„ìˆ˜
â€¢ ê³¼ë„í•œ ìš”ì²­ ê¸ˆì§€ (ì„œë²„ ë¶€í•˜)
â€¢ ê°œì¸ì •ë³´ ì²˜ë¦¬ ì‹œ ë²•ì  ê²€í†  í•„ìš”
â€¢ ì €ì‘ê¶Œ ì½˜í…ì¸  ì£¼ì˜
"""
        
        self.recommend_text.insert(tk.END, recommend_content)
        self.tech_text.insert(tk.END, tech_content)
        
        # í¸ì§‘ ë¶ˆê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        self.recommend_text.config(state='disabled')
        self.tech_text.config(state='disabled')
    
    def crawl_website(self, url):
        """requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            delay = float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            for page in range(1, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p}/{max_pages} í¬ë¡¤ë§ ì¤‘..."))
                
                # í˜ì´ì§€ë³„ URL ìƒì„±
                page_url = self.generate_page_url(url, page)
                
                # ì¬ì‹œë„ ë¡œì§
                success = False
                for retry in range(self.max_retries):
                    try:
                        response = requests.get(page_url, headers=headers, timeout=10)
                        response.raise_for_status()
                        
                        # í•œê¸€ ì¸ì½”ë”© ì²˜ë¦¬ ê°œì„ 
                        if response.encoding == 'ISO-8859-1':
                            # ì˜ëª»ëœ ì¸ì½”ë”©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                            response.encoding = response.apparent_encoding
                        elif not response.encoding:
                            # ì¸ì½”ë”©ì´ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš°
                            response.encoding = 'utf-8'
                        
                        # í•œê¸€ ì‚¬ì´íŠ¸ì˜ ê²½ìš° ì¶”ê°€ ì²´í¬
                        if 'charset=' in response.headers.get('content-type', '').lower():
                            charset = response.headers.get('content-type', '').lower()
                            if 'euc-kr' in charset:
                                response.encoding = 'euc-kr'
                            elif 'utf-8' in charset:
                                response.encoding = 'utf-8'
                        else:
                            # apparent_encodingìœ¼ë¡œ ìë™ ê°ì§€
                            response.encoding = response.apparent_encoding or 'utf-8'
                        
                        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # ê²°ê³¼ ì—…ë°ì´íŠ¸ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
                        self.root.after(0, self.update_results, page_url, response, soup)
                        
                        success = True
                        break
                        
                    except requests.exceptions.RequestException as e:
                        self.root.after(0, lambda r=retry+1: self.status_var.set(f"ìš”ì²­ ì‹¤íŒ¨, ì¬ì‹œë„ {r}/{self.max_retries}"))
                        time.sleep(2)
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"ì˜¤ë¥˜ ë°œìƒ, ì¬ì‹œë„ {r}/{self.max_retries}: {err[:50]}"))
                        time.sleep(2)
                
                if not success:
                    self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p} í¬ë¡¤ë§ ì‹¤íŒ¨"))
                
                # í¬ë¡¤ë§ ê°„ê²©
                if page < max_pages and self.is_crawling:
                    time.sleep(delay)
            
            self.root.after(0, self.finalize_crawling)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
    
    def update_results(self, url, response, soup):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ UIì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            # í˜ì´ì§€ ì •ë³´ - í•œê¸€ ì²˜ë¦¬ ê°œì„ 
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"
            
            # ì œëª© í…ìŠ¤íŠ¸ ì •ë¦¬
            if title_text:
                import unicodedata
                title_text = unicodedata.normalize('NFC', title_text)
                title_text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', title_text)
                title_text = re.sub(r'\s+', ' ', title_text).strip()
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', 'ì„¤ëª… ì—†ìŒ') if meta_desc else "ì„¤ëª… ì—†ìŒ"
            
            # ì„¤ëª… í…ìŠ¤íŠ¸ ì •ë¦¬
            if description and description != 'ì„¤ëª… ì—†ìŒ':
                description = unicodedata.normalize('NFC', description)
                description = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', description)
                description = re.sub(r'\s+', ' ', description).strip()
            
            info = f"ì œëª©: {title_text}\n"
            info += f"URL: {url}\n"
            info += f"ìƒíƒœ ì½”ë“œ: {response.status_code}\n"
            info += f"ì½˜í…ì¸  íƒ€ì…: {response.headers.get('content-type', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
            info += f"ì½˜í…ì¸  í¬ê¸°: {len(response.content)} bytes\n"
            info += f"ì„¤ëª…: {description}\n"
            
            self.info_text.insert(tk.END, info)
            
            # ë§í¬ ì¶”ì¶œ
            if self.extract_links.get():
                links = soup.find_all('a', href=True)
                self.links_text.insert(tk.END, f"ì´ {len(links)}ê°œì˜ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n\n")
                
                for i, link in enumerate(links[:50], 1):  # ìµœëŒ€ 50ê°œë§Œ í‘œì‹œ
                    href = link['href']
                    text = link.get_text(strip=True)
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ ì •ë¦¬
                    if text:
                        text = unicodedata.normalize('NFC', text)
                        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                        text = re.sub(r'\s+', ' ', text).strip()
                        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œí•œ
                        if len(text) > 100:
                            text = text[:100] + "..."
                    else:
                        text = "(í…ìŠ¤íŠ¸ ì—†ìŒ)"
                    
                    full_url = urljoin(url, href)
                    self.links_text.insert(tk.END, f"{i}. {text}\n   URL: {full_url}\n\n")
                
                if len(links) > 50:
                    self.links_text.insert(tk.END, f"... ê·¸ ì™¸ {len(links) - 50}ê°œ ë§í¬ê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            if self.extract_images.get():
                images = soup.find_all('img', src=True)
                self.images_text.insert(tk.END, f"ì´ {len(images)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n\n")
                
                for i, img in enumerate(images[:30], 1):  # ìµœëŒ€ 30ê°œë§Œ í‘œì‹œ
                    src = img['src']
                    alt = img.get('alt', 'ëŒ€ì²´ í…ìŠ¤íŠ¸ ì—†ìŒ')
                    full_url = urljoin(url, src)
                    self.images_text.insert(tk.END, f"{i}. {alt}\n   URL: {full_url}\n\n")
                
                if len(images) > 30:
                    self.images_text.insert(tk.END, f"... ê·¸ ì™¸ {len(images) - 30}ê°œ ì´ë¯¸ì§€ê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            if self.extract_text.get():
                # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                for script in soup(["script", "style"]):
                    script.decompose()
                
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                # í•œê¸€ í…ìŠ¤íŠ¸ ì •ë¦¬
                import unicodedata
                # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (í•œê¸€ í˜¸í™˜ì„± ë¬¸ì ì²˜ë¦¬)
                text = unicodedata.normalize('NFC', text)
                # ì œì–´ ë¬¸ì ì œê±°
                text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                text = re.sub(r'\s+', ' ', text).strip()
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                if len(text) > 5000:
                    text = text[:5000] + "\n\n... (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
                
                self.content_text.insert(tk.END, text)
            
            # í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€
            self.populate_table(url, soup)
            
            self.status_var.set(f"í¬ë¡¤ë§ ì™„ë£Œ - {title_text}")
            
        except Exception as e:
            self.show_error(f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    def show_error(self, error_message):
        """ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        self.progress.stop()
        self.crawl_button.config(state='normal')
        self.stop_button.config(state='disabled')
        self.is_crawling = False
        self.status_var.set("ì˜¤ë¥˜ ë°œìƒ")
        messagebox.showerror("í¬ë¡¤ë§ ì˜¤ë¥˜", error_message)
    
    def populate_table(self, url, soup):
        """í…Œì´ë¸”ì— í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            # í˜ì´ì§€ ê¸°ë³¸ ì •ë³´
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"
            
            # í˜ì´ì§€ ì •ë³´ ì¶”ê°€
            page_data = {
                'type': 'í˜ì´ì§€',
                'title': title_text,
                'url': url,
                'description': 'ì›¹í˜ì´ì§€ ê¸°ë³¸ ì •ë³´'
            }
            self.tree.insert('', 'end', values=(page_data['type'], page_data['title'], 
                                              page_data['url'], page_data['description']))
            self.crawled_data.append(page_data)
            
            # ë§í¬ ì •ë³´ ì¶”ê°€
            if self.extract_links.get():
                links = soup.find_all('a', href=True)
                for i, link in enumerate(links[:50]):  # ìµœëŒ€ 50ê°œ
                    href = link['href']
                    text = link.get_text(strip=True)
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ ì •ë¦¬
                    if text:
                        import unicodedata
                        text = unicodedata.normalize('NFC', text)
                        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                        text = re.sub(r'\s+', ' ', text).strip()
                        # ê¸¸ì´ ì œí•œ
                        if len(text) > 100:
                            text = text[:100] + "..."
                    else:
                        text = '(í…ìŠ¤íŠ¸ ì—†ìŒ)'
                    
                    full_url = urljoin(url, href)
                    
                    link_data = {
                        'type': 'ë§í¬',
                        'title': text,
                        'url': full_url,
                        'description': f'ë§í¬ #{i+1}'
                    }
                    self.tree.insert('', 'end', values=(link_data['type'], link_data['title'], 
                                                      link_data['url'], link_data['description']))
                    self.crawled_data.append(link_data)
            
            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            if self.extract_images.get():
                images = soup.find_all('img', src=True)
                for i, img in enumerate(images[:30]):  # ìµœëŒ€ 30ê°œ
                    src = img['src']
                    alt = img.get('alt', 'ëŒ€ì²´ í…ìŠ¤íŠ¸ ì—†ìŒ')[:100]
                    full_url = urljoin(url, src)
                    
                    img_data = {
                        'type': 'ì´ë¯¸ì§€',
                        'title': alt,
                        'url': full_url,
                        'description': f'ì´ë¯¸ì§€ #{i+1}'
                    }
                    self.tree.insert('', 'end', values=(img_data['type'], img_data['title'], 
                                                      img_data['url'], img_data['description']))
                    self.crawled_data.append(img_data)
                    
        except Exception as e:
            print(f"í…Œì´ë¸” ì±„ìš°ê¸° ì˜¤ë¥˜: {str(e)}")
    
    def export_to_excel(self):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.crawled_data:
            messagebox.showwarning("ê²½ê³ ", "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # íŒŒì¼ ì €ì¥ ëŒ€í™”ìƒì - í•œê¸€ íŒŒì¼ëª… ì§€ì›
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            default_filename = f"í¬ë¡¤ë§ê²°ê³¼_{timestamp}.xlsx"  # í•œê¸€ íŒŒì¼ëª… ì‚¬ìš©
            
            file_path = filedialog.asksaveasfilename(
                defaultextension=".xlsx",
                filetypes=[("Excel íŒŒì¼", "*.xlsx"), ("ëª¨ë“  íŒŒì¼", "*.*")],
                initialvalue=default_filename,
                title="í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥"
            )
            
            if not file_path:
                return
            
            # í•œê¸€ ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì •ê·œí™”
            file_path = os.path.normpath(file_path)
            
            # DataFrame ìƒì„± - í•œê¸€ ë°ì´í„° ì²˜ë¦¬ ê°œì„ 
            processed_data = []
            for item in self.crawled_data:
                processed_item = {}
                for key, value in item.items():
                    # í…ìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ë° ì¸ì½”ë”© ì²˜ë¦¬
                    if isinstance(value, str):
                        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
                        value = unicodedata.normalize('NFC', value)
                        # íŠ¹ìˆ˜ë¬¸ì ë° ì œì–´ë¬¸ì ì œê±°
                        value = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', value)
                        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                        value = re.sub(r'\s+', ' ', value).strip()
                        # Excel ì…€ ê¸¸ì´ ì œí•œ (32767ì)
                        if len(value) > 32000:
                            value = value[:32000] + "..."
                    processed_item[key] = value
                processed_data.append(processed_item)
            
            df = pd.DataFrame(processed_data)
            df.columns = ['íƒ€ì…', 'ì œëª©/í…ìŠ¤íŠ¸', 'URL', 'ì„¤ëª…']
            
            # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ - í•œê¸€ ì§€ì› ê°•í™”
            with pd.ExcelWriter(
                file_path, 
                engine='openpyxl'
            ) as writer:
                # ì‹œíŠ¸ëª…ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€ê²½ (ì¼ë¶€ Excel ë²„ì „ í˜¸í™˜ì„±)
                sheet_name = 'CrawlingResults'
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ì„œì‹ ì„¤ì •
                worksheet = writer.sheets[sheet_name]
                
                # ìŠ¤íƒ€ì¼ importë¥¼ try-exceptë¡œ ê°ì‹¸ì„œ ì˜¤ë¥˜ ë°©ì§€
                try:
                    from openpyxl.styles import Font, PatternFill, Alignment
                    header_font = Font(bold=True, color="FFFFFF")
                    header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                    
                    # í—¤ë” í–‰ ìŠ¤íƒ€ì¼ ì ìš©
                    for cell in worksheet[1]:
                        cell.font = header_font
                        cell.fill = header_fill
                        cell.alignment = Alignment(horizontal="center", vertical="center")
                except ImportError:
                    # ìŠ¤íƒ€ì¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ
                    pass
                
                # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì • - í•œê¸€ í…ìŠ¤íŠ¸ ê³ ë ¤
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if cell.value:
                                # í•œê¸€ í…ìŠ¤íŠ¸ì˜ ì‹¤ì œ í‘œì‹œ ê¸¸ì´ ê³„ì‚°
                                text = str(cell.value)
                                # í•œê¸€ì€ 2ë°°, ì˜ë¬¸ì€ 1ë°°ë¡œ ê³„ì‚°
                                display_length = sum(2 if ord(char) > 127 else 1 for char in text)
                                if display_length > max_length:
                                    max_length = display_length
                        except:
                            pass
                    
                    # ìµœì†Œ 8, ìµœëŒ€ 60ìœ¼ë¡œ ì œí•œ
                    adjusted_width = min(max(max_length * 0.8, 8), 60)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
                
                # í–‰ ë†’ì´ ìë™ ì¡°ì •
                for row in worksheet.iter_rows():
                    max_lines = 1
                    for cell in row:
                        if cell.value and isinstance(cell.value, str):
                            lines = cell.value.count('\n') + 1
                            max_lines = max(max_lines, lines)
                    if max_lines > 1:
                        worksheet.row_dimensions[row[0].row].height = max_lines * 15
            
            # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ - íŒŒì¼ í¬ê¸° ì •ë³´ ì¶”ê°€
            file_size = os.path.getsize(file_path)
            size_str = f"{file_size:,} bytes"
            if file_size > 1024:
                size_str = f"{file_size/1024:.1f} KB"
            if file_size > 1024*1024:
                size_str = f"{file_size/(1024*1024):.1f} MB"
            
            messagebox.showinfo(
                "ì €ì¥ ì™„ë£Œ", 
                f"í¬ë¡¤ë§ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                f"íŒŒì¼: {os.path.basename(file_path)}\n"
                f"ìœ„ì¹˜: {os.path.dirname(file_path)}\n"
                f"í¬ê¸°: {size_str}\n"
                f"ë°ì´í„° ìˆ˜: {len(self.crawled_data)}ê°œ"
            )
            
        except UnicodeEncodeError as e:
            messagebox.showerror("ì¸ì½”ë”© ì˜¤ë¥˜", 
                f"íŒŒì¼ ì €ì¥ ì¤‘ ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
                f"ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
        except PermissionError:
            messagebox.showerror("ê¶Œí•œ ì˜¤ë¥˜", 
                f"íŒŒì¼ì„ ì €ì¥í•  ê¶Œí•œì´ ì—†ê±°ë‚˜ íŒŒì¼ì´ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.\n"
                f"ë‹¤ë¥¸ ìœ„ì¹˜ì— ì €ì¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ë‹«ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            messagebox.showerror("ì €ì¥ ì˜¤ë¥˜", 
                f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
                f"ì˜¤ë¥˜ ë‚´ìš©: {str(e)}\n\n"
                f"í•´ê²° ë°©ë²•:\n"
                f"1. ë‹¤ë¥¸ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥í•´ë³´ì„¸ìš”\n"
                f"2. ë‹¤ë¥¸ í´ë”ì— ì €ì¥í•´ë³´ì„¸ìš”\n"
                f"3. í”„ë¡œê·¸ë¨ì„ ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”")
    
    def setup_selenium_driver(self):
        """Selenium WebDriverë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-logging"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            
            # User-Agent ì„¤ì •
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            return True
        except Exception as e:
            self.root.after(0, self.show_error, f"Selenium ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False
    
    def setup_playwright_browser(self):
        """Playwright ë¸Œë¼ìš°ì €ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            if not PLAYWRIGHT_AVAILABLE:
                self.root.after(0, self.show_error, "Playwrightê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install playwright í›„ playwright installì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                return False
            
            self.playwright = sync_playwright().start()
            
            # ë¸Œë¼ìš°ì € ì„ íƒ (Chromium ê¸°ë³¸)
            self.browser = self.playwright.chromium.launch(
                headless=True,  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled'
                ]
            )
            
            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (ì‹œí¬ë¦¿ ëª¨ë“œì™€ ìœ ì‚¬)
            context = self.browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            
            # í˜ì´ì§€ ìƒì„±
            self.page = context.new_page()
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            self.page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined,
                });
            """)
            
            return True
        except Exception as e:
            self.root.after(0, self.show_error, f"Playwright ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False
    
    def crawl_with_selenium(self, url):
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.setup_selenium_driver():
            return
        
        try:
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            delay = float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1
            
            for page in range(1, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p}/{max_pages} í¬ë¡¤ë§ ì¤‘..."))
                
                # í˜ì´ì§€ë³„ URL ìƒì„±
                page_url = self.generate_page_url(url, page)
                
                # ì¬ì‹œë„ ë¡œì§
                success = False
                for retry in range(self.max_retries):
                    try:
                        self.driver.get(page_url)
                        WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        
                        # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ë§
                        if self.site_type.get() == "ë„¤ì´ë²„ ì‡¼í•‘":
                            self.crawl_naver_shopping()
                        elif self.site_type.get() == "ì¸ìŠ¤íƒ€ê·¸ë¨":
                            self.crawl_instagram()
                        elif self.site_type.get() == "ë¶€ë™ì‚°":
                            self.crawl_real_estate()
                        else:
                            self.crawl_general_selenium()
                        
                        success = True
                        break
                        
                    except TimeoutException:
                        self.root.after(0, lambda r=retry+1: self.status_var.set(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„ {r}/{self.max_retries}"))
                        time.sleep(2)
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"ì˜¤ë¥˜ ë°œìƒ, ì¬ì‹œë„ {r}/{self.max_retries}: {err[:50]}"))
                        time.sleep(2)
                
                if not success:
                    self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p} í¬ë¡¤ë§ ì‹¤íŒ¨"))
                
                # í¬ë¡¤ë§ ê°„ê²©
                if page < max_pages and self.is_crawling:
                    time.sleep(delay)
            
            self.root.after(0, self.finalize_crawling)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"Selenium í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        finally:
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None
    
    def generate_page_url(self, base_url, page):
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ URLì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if page == 1:
            return base_url
        
        # ì¼ë°˜ì ì¸ í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ë“¤
        if "?" in base_url:
            return f"{base_url}&page={page}"
        else:
            return f"{base_url}?page={page}"
    
    def crawl_naver_shopping(self):
        """ë„¤ì´ë²„ ì‡¼í•‘ í¬ë¡¤ë§"""
        try:
            # ìƒí’ˆ ëª©ë¡ ìš”ì†Œ ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".basicList_list__2YY7H, .product_list, .list_basis"))
            )
            
            # ìƒí’ˆ ìš”ì†Œë“¤ ì°¾ê¸°
            products = self.driver.find_elements(By.CSS_SELECTOR, ".basicList_item__1MBN3, .product_item, .item")
            
            for product in products:
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ë„¤ì´ë²„ì‡¼í•‘'}
                    
                    # ì œëª© ì¶”ì¶œ
                    if self.extract_title.get():
                        title_elem = product.find_element(By.CSS_SELECTOR, ".basicList_title__3P9Q7, .product_title, .item_title")
                        data['title'] = title_elem.text.strip()
                    
                    # ê°€ê²© ì¶”ì¶œ
                    if self.extract_price.get():
                        price_elem = product.find_element(By.CSS_SELECTOR, ".price_num__2WUXn, .product_price, .item_price")
                        data['price'] = price_elem.text.strip()
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = product.find_element(By.CSS_SELECTOR, "a")
                    data['url'] = link_elem.get_attribute('href')
                    
                    # ì´ë¯¸ì§€ ì¶”ì¶œ
                    if self.extract_images.get():
                        img_elem = product.find_element(By.CSS_SELECTOR, "img")
                        data['image_url'] = img_elem.get_attribute('src')
                    
                    data['description'] = f"ë„¤ì´ë²„ì‡¼í•‘ ìƒí’ˆ (í˜ì´ì§€ {self.current_page})"
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except NoSuchElementException:
                    continue
                    
        except TimeoutException:
            self.root.after(0, lambda: self.status_var.set("ë„¤ì´ë²„ ì‡¼í•‘ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"))
    
    def crawl_instagram(self):
        """ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ë¡¤ë§ (ì œí•œì )"""
        try:
            # ì¸ìŠ¤íƒ€ê·¸ë¨ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ë¯€ë¡œ ê¸°ë³¸ì ì¸ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ
            posts = self.driver.find_elements(By.CSS_SELECTOR, "article, ._aagu, .v1Nh3")
            
            for i, post in enumerate(posts[:10]):  # ìµœëŒ€ 10ê°œë§Œ
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ì¸ìŠ¤íƒ€ê·¸ë¨'}
                    
                    # ê¸°ë³¸ ì •ë³´
                    data['title'] = f"Instagram Post {i+1}"
                    data['url'] = self.driver.current_url
                    data['description'] = f"ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ (í˜ì´ì§€ {self.current_page})"
                    
                    # ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
                    if self.extract_images.get():
                        try:
                            img_elem = post.find_element(By.CSS_SELECTOR, "img")
                            data['image_url'] = img_elem.get_attribute('src')
                        except:
                            pass
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except:
                    continue
                    
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ë¡¤ë§ ì œí•œ: {err[:50]}"))
    
    def crawl_real_estate(self):
        """ë¶€ë™ì‚° ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        try:
            # ë¶€ë™ì‚° ë§¤ë¬¼ ìš”ì†Œë“¤ ì°¾ê¸°
            properties = self.driver.find_elements(By.CSS_SELECTOR, ".item, .list-item, .property-item, .estate-item")
            
            for prop in properties:
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ë¶€ë™ì‚°'}
                    
                    # ì œëª© (ì£¼ì†Œ/ë§¤ë¬¼ëª…)
                    if self.extract_title.get():
                        title_selectors = [".item-title", ".property-title", ".title", "h3", "h4"]
                        for selector in title_selectors:
                            try:
                                title_elem = prop.find_element(By.CSS_SELECTOR, selector)
                                data['title'] = title_elem.text.strip()
                                break
                            except:
                                continue
                    
                    # ê°€ê²© ì¶”ì¶œ
                    if self.extract_price.get():
                        price_selectors = [".price", ".cost", ".amount", ".fee"]
                        for selector in price_selectors:
                            try:
                                price_elem = prop.find_element(By.CSS_SELECTOR, selector)
                                data['price'] = price_elem.text.strip()
                                break
                            except:
                                continue
                    
                    # ë§í¬ ì¶”ì¶œ
                    try:
                        link_elem = prop.find_element(By.CSS_SELECTOR, "a")
                        data['url'] = link_elem.get_attribute('href')
                    except:
                        data['url'] = self.driver.current_url
                    
                    data['description'] = f"ë¶€ë™ì‚° ë§¤ë¬¼ (í˜ì´ì§€ {self.current_page})"
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except:
                    continue
                    
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ë¶€ë™ì‚° í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))
    
    def crawl_general_selenium(self):
        """ì¼ë°˜ì ì¸ Selenium í¬ë¡¤ë§"""
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ê¸°ì¡´ populate_table ë©”ì„œë“œ í™œìš©
            self.root.after(0, self.populate_table, self.driver.current_url, soup)
            
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ì¼ë°˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))
    
    def add_to_table(self, data):
        """í…Œì´ë¸”ì— ê°œë³„ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            self.tree.insert('', 'end', values=(
                data.get('type', ''),
                data.get('title', '')[:100],
                data.get('url', ''),
                data.get('description', '')
            ))
        except Exception as e:
            print(f"í…Œì´ë¸” ì¶”ê°€ ì˜¤ë¥˜: {str(e)}")
    
    def finalize_crawling(self):
        """í¬ë¡¤ë§ ì™„ë£Œ ì²˜ë¦¬"""
        self.progress.stop()
        self.crawl_button.config(state='normal')
        self.stop_button.config(state='disabled')
        self.export_button.config(state='normal')
        self.is_crawling = False
        self.status_var.set(f"í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(self.crawled_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘")

    def crawl_with_playwright(self, url):
        """Playwrightë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            if not self.setup_playwright_browser():
                return
            
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            delay = float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1
            
            for page in range(1, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p}/{max_pages} Playwright í¬ë¡¤ë§ ì¤‘..."))
                
                page_url = self.generate_page_url(url, page)
                
                success = False
                for retry in range(self.max_retries):
                    try:
                        # í˜ì´ì§€ ë¡œë“œ
                        self.page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                        
                        # ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ì™„ë£Œê¹Œì§€ ëŒ€ê¸°
                        self.page.wait_for_load_state('networkidle', timeout=10000)
                        
                        # ì¶”ê°€ ëŒ€ê¸° (JavaScript ì‹¤í–‰ ì™„ë£Œ)
                        self.page.wait_for_timeout(2000)
                        
                        # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ë§
                        domain = urlparse(page_url).netloc.lower()
                        if 'shopping.naver.com' in domain:
                            self.crawl_naver_shopping_playwright()
                        elif 'instagram.com' in domain:
                            self.crawl_instagram_playwright()
                        elif any(keyword in domain for keyword in ['zigbang', 'dabang', 'ë¶€ë™ì‚°']):
                            self.crawl_real_estate_playwright()
                        else:
                            self.crawl_general_playwright()
                        
                        success = True
                        break
                        
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"Playwright ì˜¤ë¥˜, ì¬ì‹œë„ {r}/{self.max_retries}: {err[:30]}"))
                        time.sleep(2)
                
                if not success:
                    self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p} Playwright í¬ë¡¤ë§ ì‹¤íŒ¨"))
                
                if page < max_pages and self.is_crawling:
                    time.sleep(delay)
            
            self.root.after(0, self.finalize_crawling)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"Playwright í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        finally:
            # Playwright ì •ë¦¬
            if self.page:
                try:
                    self.page.close()
                except:
                    pass
                self.page = None
            
            if self.browser:
                try:
                    self.browser.close()
                except:
                    pass
                self.browser = None
            
            if self.playwright:
                try:
                    self.playwright.stop()
                except:
                    pass
                self.playwright = None
    
    def crawl_general_playwright(self):
        """ì¼ë°˜ì ì¸ Playwright í¬ë¡¤ë§"""
        try:
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            content = self.page.content()
            soup = BeautifulSoup(content, 'html.parser')
            
            # ê¸°ì¡´ populate_table ë©”ì„œë“œ í™œìš©
            self.root.after(0, self.populate_table, self.page.url, soup)
            
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ì¼ë°˜ Playwright í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))
    
    def crawl_naver_shopping_playwright(self):
        """ë„¤ì´ë²„ ì‡¼í•‘ Playwright í¬ë¡¤ë§"""
        try:
            # ìƒí’ˆ ëª©ë¡ ëŒ€ê¸°
            self.page.wait_for_selector('.basicList_list_basis__uNBZx, .product_list, .goods_list', timeout=10000)
            
            # ìƒí’ˆ ìš”ì†Œë“¤ ê°€ì ¸ì˜¤ê¸°
            products = self.page.query_selector_all('.basicList_item__FxDgW, .product_item, .goods_item')
            
            for i, product in enumerate(products[:20]):  # ìµœëŒ€ 20ê°œ
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ë„¤ì´ë²„ì‡¼í•‘'}
                    
                    # ìƒí’ˆëª…
                    if self.extract_title.get():
                        title_elem = product.query_selector('.basicList_title__3P9Q7, .product_title, .goods_name')
                        if title_elem:
                            data['title'] = title_elem.text_content().strip()
                        else:
                            data['title'] = f"ìƒí’ˆ {i+1}"
                    
                    # ê°€ê²©
                    if self.extract_price.get():
                        price_elem = product.query_selector('.price_num__2WUXn, .product_price, .price')
                        if price_elem:
                            data['price'] = price_elem.text_content().strip()
                    
                    # ìƒí’ˆ ë§í¬
                    link_elem = product.query_selector('a')
                    if link_elem:
                        data['url'] = link_elem.get_attribute('href')
                        if data['url'] and not data['url'].startswith('http'):
                            data['url'] = 'https://shopping.naver.com' + data['url']
                    else:
                        data['url'] = self.page.url
                    
                    # ì„¤ëª…
                    data['description'] = f"ë„¤ì´ë²„ ì‡¼í•‘ ìƒí’ˆ (í˜ì´ì§€ {self.current_page})"
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ë„¤ì´ë²„ ì‡¼í•‘ Playwright í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))
    
    def crawl_instagram_playwright(self):
        """ì¸ìŠ¤íƒ€ê·¸ë¨ Playwright í¬ë¡¤ë§"""
        try:
            # ê²Œì‹œë¬¼ ìš”ì†Œë“¤ ëŒ€ê¸°
            self.page.wait_for_selector('article, ._aagu', timeout=10000)
            
            posts = self.page.query_selector_all('article, ._aagu')
            
            for i, post in enumerate(posts[:10]):  # ìµœëŒ€ 10ê°œ
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ì¸ìŠ¤íƒ€ê·¸ë¨'}
                    data['title'] = f"Instagram Post {i+1}"
                    data['url'] = self.page.url
                    data['description'] = f"ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ (í˜ì´ì§€ {self.current_page})"
                    
                    # ì´ë¯¸ì§€ URL ì¶”ì¶œ
                    if self.extract_images.get():
                        img_elem = post.query_selector('img')
                        if img_elem:
                            data['image_url'] = img_elem.get_attribute('src')
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ì¸ìŠ¤íƒ€ê·¸ë¨ Playwright í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))
    
    def crawl_real_estate_playwright(self):
        """ë¶€ë™ì‚° ì‚¬ì´íŠ¸ Playwright í¬ë¡¤ë§"""
        try:
            # ë§¤ë¬¼ ëª©ë¡ ëŒ€ê¸°
            self.page.wait_for_selector('.item, .property, .list-item', timeout=10000)
            
            properties = self.page.query_selector_all('.item, .property, .list-item')
            
            for i, prop in enumerate(properties[:15]):  # ìµœëŒ€ 15ê°œ
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ë¶€ë™ì‚°'}
                    
                    # ì œëª© (ì£¼ì†Œ/ë§¤ë¬¼ëª…)
                    if self.extract_title.get():
                        title_elem = prop.query_selector('.item-title, .property-title, .title, h3, h4')
                        if title_elem:
                            data['title'] = title_elem.text_content().strip()
                        else:
                            data['title'] = f"ë§¤ë¬¼ {i+1}"
                    
                    # ê°€ê²©
                    if self.extract_price.get():
                        price_elem = prop.query_selector('.price, .item-price, .property-price')
                        if price_elem:
                            data['price'] = price_elem.text_content().strip()
                    
                    # ë§¤ë¬¼ ë§í¬
                    link_elem = prop.query_selector('a')
                    if link_elem:
                        data['url'] = link_elem.get_attribute('href')
                        if data['url'] and not data['url'].startswith('http'):
                            base_url = f"{self.page.url.split('/')[0]}//{self.page.url.split('/')[2]}"
                            data['url'] = base_url + data['url']
                    else:
                        data['url'] = self.page.url
                    
                    # ì„¤ëª…
                    data['description'] = f"ë¶€ë™ì‚° ë§¤ë¬¼ (í˜ì´ì§€ {self.current_page})"
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except Exception:
                    continue
                    
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ë¶€ë™ì‚° Playwright í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))

    # ì•Œë¦¼ ê´€ë ¨ ë©”ì„œë“œë“¤
    def add_keyword(self):
        """í‚¤ì›Œë“œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        keyword = self.keyword_entry.get().strip()
        if keyword and keyword not in self.keyword_listbox.get(0, tk.END):
            self.keyword_listbox.insert(tk.END, keyword)
            self.keyword_entry.set("")
    
    def remove_keyword(self):
        """ì„ íƒëœ í‚¤ì›Œë“œë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
        try:
            selection = self.keyword_listbox.curselection()
            if selection:
                self.keyword_listbox.delete(selection[0])
        except:
            pass
    
    def toggle_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘/ì¤‘ì§€í•©ë‹ˆë‹¤."""
        if self.monitoring_active:
            self.monitoring_active = False
            self.monitor_button.config(text="ëª¨ë‹ˆí„°ë§ ì‹œì‘")
            self.status_var.set("ëª¨ë‹ˆí„°ë§ ì¤‘ì§€ë¨")
        else:
            self.monitoring_active = True
            self.monitor_button.config(text="ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
            self.status_var.set("ëª¨ë‹ˆí„°ë§ ì‹œì‘ë¨")
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
            threading.Thread(target=self.run_monitoring, daemon=True).start()
    
    def run_monitoring(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë‹ˆí„°ë§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        while self.monitoring_active:
            try:
                # í˜„ì¬ ì„¤ì •ëœ URLë¡œ í¬ë¡¤ë§ ì‹¤í–‰
                url = self.url_var.get().strip()
                if url:
                    self.check_for_alerts(url)
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤ ì²´í¬
            except Exception as e:
                print(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(60)  # ì˜¤ë¥˜ ì‹œ 1ë¶„ í›„ ì¬ì‹œë„
    
    def check_for_alerts(self, url):
        """ì•Œë¦¼ ì¡°ê±´ì„ ì²´í¬í•©ë‹ˆë‹¤."""
        try:
            # ê°„ë‹¨í•œ í¬ë¡¤ë§ìœ¼ë¡œ í˜„ì¬ ë°ì´í„° í™•ì¸
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê°€ê²© ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒ¨í„´)
            price_elements = soup.find_all(text=re.compile(r'[\d,]+ì›|[\$][\d,]+|\$[\d,.]+'))
            
            # í‚¤ì›Œë“œ ì²´í¬
            keywords = list(self.keyword_listbox.get(0, tk.END))
            page_text = soup.get_text().lower()
            
            alert_messages = []
            
            # í‚¤ì›Œë“œ ì•Œë¦¼ ì²´í¬
            for keyword in keywords:
                if keyword.lower() in page_text:
                    alert_messages.append(f"í‚¤ì›Œë“œ '{keyword}'ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
            
            # ê°€ê²© ë³€ë™ ì²´í¬ (ê¸°ë³¸ì ì¸ êµ¬í˜„)
            if price_elements and self.historical_data:
                current_prices = [self.extract_number(price.strip()) for price in price_elements[:3]]
                
                # ì´ì „ ë°ì´í„°ì™€ ë¹„êµ
                if len(self.historical_data) > 0:
                    prev_data = self.historical_data[-1]
                    threshold = float(self.price_threshold.get() or 10)
                    
                    for i, current_price in enumerate(current_prices):
                        if current_price and i < len(prev_data.get('prices', [])):
                            prev_price = prev_data['prices'][i]
                            if prev_price and current_price:
                                change_percent = abs((current_price - prev_price) / prev_price * 100)
                                if change_percent >= threshold:
                                    alert_messages.append(f"ê°€ê²© ë³€ë™ ê°ì§€: {change_percent:.1f}% ë³€í™”")
            
            # í˜„ì¬ ë°ì´í„° ì €ì¥
            current_data = {
                'timestamp': datetime.now(),
                'url': url,
                'prices': [self.extract_number(price.strip()) for price in price_elements[:3]],
                'keywords_found': [kw for kw in keywords if kw.lower() in page_text]
            }
            self.historical_data.append(current_data)
            
            # ë°ì´í„° ì œí•œ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            if len(self.historical_data) > 100:
                self.historical_data = self.historical_data[-100:]
            
            # ì•Œë¦¼ ë°œì†¡
            if alert_messages:
                self.send_alerts(alert_messages, url)
                
        except Exception as e:
            print(f"ì•Œë¦¼ ì²´í¬ ì˜¤ë¥˜: {e}")
    
    def extract_number(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ìë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            # ìˆ«ìì™€ ì½¤ë§ˆë§Œ ì¶”ì¶œ
            numbers = re.findall(r'[\d,]+', text.replace(',', ''))
            if numbers:
                return float(numbers[0])
        except:
            pass
        return None
    
    def send_alerts(self, messages, url):
        """ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤."""
        alert_text = "\n".join(messages)
        alert_title = "ì›¹ í¬ë¡¤ë§ ì•Œë¦¼"
        
        # ë°ìŠ¤í¬íƒ‘ ì•Œë¦¼
        if self.notification_enabled.get() and NOTIFICATION_AVAILABLE:
            try:
                notification.notify(
                    title=alert_title,
                    message=alert_text,
                    timeout=10
                )
            except Exception as e:
                print(f"ë°ìŠ¤í¬íƒ‘ ì•Œë¦¼ ì˜¤ë¥˜: {e}")
        
        # ì´ë©”ì¼ ì•Œë¦¼
        if self.email_enabled.get():
            self.send_email_alert(alert_title, alert_text, url)
    
    def send_email_alert(self, subject, message, url):
        """ì´ë©”ì¼ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤."""
        try:
            sender_email = self.sender_email.get()
            sender_password = self.sender_password.get()
            receiver_email = self.receiver_email.get()
            
            if not all([sender_email, sender_password, receiver_email]):
                print("ì´ë©”ì¼ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            # ì´ë©”ì¼ ë©”ì‹œì§€ êµ¬ì„±
            msg = MimeMultipart()
            msg['From'] = sender_email
            msg['To'] = receiver_email
            msg['Subject'] = subject
            
            body = f"""
ì›¹ í¬ë¡¤ë§ ì•Œë¦¼

URL: {url}
ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ì•Œë¦¼ ë‚´ìš©:
{message}

ìë™ ë°œì†¡ëœ ë©”ì‹œì§€ì…ë‹ˆë‹¤.
            """
            
            msg.attach(MimeText(body, 'plain', 'utf-8'))
            
            # Gmail SMTP ì„œë²„ ì„¤ì •
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, sender_password)
            
            server.send_message(msg)
            server.quit()
            
            print("ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ì´ë©”ì¼ ë°œì†¡ ì˜¤ë¥˜: {e}")
    
    def test_notification(self):
        """ì•Œë¦¼ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        test_messages = ["í…ŒìŠ¤íŠ¸ ì•Œë¦¼ì…ë‹ˆë‹¤.", "ì•Œë¦¼ ì„¤ì •ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤."]
        self.send_alerts(test_messages, "í…ŒìŠ¤íŠ¸ URL")
        messagebox.showinfo("í…ŒìŠ¤íŠ¸ ì™„ë£Œ", "ì•Œë¦¼ í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„° ë¶„ì„ ê´€ë ¨ ë©”ì„œë“œë“¤
    def generate_chart(self):
        """ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not CHART_AVAILABLE:
            messagebox.showerror("ì˜¤ë¥˜", "ì°¨íŠ¸ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ matplotlib ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
            return
        
        if not self.historical_data and not self.crawled_data:
            messagebox.showwarning("ê²½ê³ ", "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ë¡¤ë§ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        chart_type = self.chart_type.get()
        
        try:
            # ê¸°ì¡´ ì°¨íŠ¸ ì œê±°
            for widget in self.chart_frame.winfo_children():
                widget.destroy()
            
            # matplotlib í•œê¸€ í°íŠ¸ ì„¤ì •
            plt.rcParams['font.family'] = ['AppleGothic'] if os.name == 'posix' else ['Malgun Gothic']
            plt.rcParams['axes.unicode_minus'] = False
            
            fig, ax = plt.subplots(figsize=(8, 5))
            
            if chart_type == "ê°€ê²© ì¶”ì´":
                self.create_price_trend_chart(ax)
            elif chart_type == "í‚¤ì›Œë“œ ë¹ˆë„":
                self.create_keyword_frequency_chart(ax)
            elif chart_type == "ì‚¬ì´íŠ¸ë³„ í†µê³„":
                self.create_site_statistics_chart(ax)
            elif chart_type == "ì‹œê°„ëŒ€ë³„ ë¶„í¬":
                self.create_time_distribution_chart(ax)
            
            # ì°¨íŠ¸ë¥¼ tkinterì— í‘œì‹œ
            canvas = FigureCanvasTkAgg(fig, self.chart_frame)
            canvas.draw()
            canvas.get_tk_widget().grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
            
        except Exception as e:
            messagebox.showerror("ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜", f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def create_price_trend_chart(self, ax):
        """ê°€ê²© ì¶”ì´ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.historical_data:
            ax.text(0.5, 0.5, 'ê°€ê²© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤', ha='center', va='center', transform=ax.transAxes)
            return
        
        timestamps = [data['timestamp'] for data in self.historical_data]
        prices = []
        
        for data in self.historical_data:
            if data['prices'] and data['prices'][0]:
                prices.append(data['prices'][0])
            else:
                prices.append(0)
        
        ax.plot(timestamps, prices, marker='o', linewidth=2, markersize=4)
        ax.set_title('ê°€ê²© ì¶”ì´')
        ax.set_xlabel('ì‹œê°„')
        ax.set_ylabel('ê°€ê²©')
        ax.grid(True, alpha=0.3)
        
        # xì¶• ë‚ ì§œ í¬ë§· ì„¤ì •
        import matplotlib.dates as mdates
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
    
    def create_keyword_frequency_chart(self, ax):
        """í‚¤ì›Œë“œ ë¹ˆë„ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        keyword_counts = {}
        
        # í¬ë¡¤ë§ ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        for data in self.crawled_data:
            title = data.get('title', '').lower()
            description = data.get('description', '').lower()
            text = title + ' ' + description
            
            # ê°„ë‹¨í•œ ë‹¨ì–´ ë¶„ë¦¬ (í•œê¸€/ì˜ë¬¸)
            words = re.findall(r'[ê°€-í£]{2,}|[a-zA-Z]{3,}', text)
            for word in words:
                keyword_counts[word] = keyword_counts.get(word, 0) + 1
        
        if not keyword_counts:
            ax.text(0.5, 0.5, 'í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤', ha='center', va='center', transform=ax.transAxes)
            return
        
        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:10]
        keywords, counts = zip(*top_keywords)
        
        ax.bar(keywords, counts)
        ax.set_title('í‚¤ì›Œë“œ ë¹ˆë„')
        ax.set_xlabel('í‚¤ì›Œë“œ')
        ax.set_ylabel('ë¹ˆë„')
        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)
    
    def create_site_statistics_chart(self, ax):
        """ì‚¬ì´íŠ¸ë³„ í†µê³„ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        site_counts = {}
        
        for data in self.crawled_data:
            url = data.get('url', '')
            domain = urlparse(url).netloc
            site_counts[domain] = site_counts.get(domain, 0) + 1
        
        if not site_counts:
            ax.text(0.5, 0.5, 'ì‚¬ì´íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤', ha='center', va='center', transform=ax.transAxes)
            return
        
        sites, counts = zip(*site_counts.items())
        ax.pie(counts, labels=sites, autopct='%1.1f%%')
        ax.set_title('ì‚¬ì´íŠ¸ë³„ ë°ì´í„° ë¶„í¬')
    
    def create_time_distribution_chart(self, ax):
        """ì‹œê°„ëŒ€ë³„ ë¶„í¬ ì°¨íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.historical_data:
            ax.text(0.5, 0.5, 'ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤', ha='center', va='center', transform=ax.transAxes)
            return
        
        hours = [data['timestamp'].hour for data in self.historical_data]
        hour_counts = {}
        for hour in hours:
            hour_counts[hour] = hour_counts.get(hour, 0) + 1
        
        hours_list = list(range(24))
        counts_list = [hour_counts.get(hour, 0) for hour in hours_list]
        
        ax.bar(hours_list, counts_list)
        ax.set_title('ì‹œê°„ëŒ€ë³„ í™œë™ ë¶„í¬')
        ax.set_xlabel('ì‹œê°„')
        ax.set_ylabel('í™œë™ íšŸìˆ˜')
        ax.set_xticks(range(0, 24, 2))
    
    def generate_report(self):
        """ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.crawled_data and not self.historical_data:
            messagebox.showwarning("ê²½ê³ ", "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í†µê³„ ê³„ì‚°
        total_items = len(self.crawled_data)
        total_monitoring = len(self.historical_data)
        
        # ì‚¬ì´íŠ¸ë³„ í†µê³„
        site_stats = {}
        for data in self.crawled_data:
            site_type = data.get('type', 'ê¸°íƒ€')
            site_stats[site_type] = site_stats.get(site_type, 0) + 1
        
        # ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
        report = f"""í¬ë¡¤ë§ ë°ì´í„° ë¶„ì„ ë¦¬í¬íŠ¸
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š ì „ì²´ í†µê³„
â€¢ ì´ í¬ë¡¤ë§ í•­ëª©: {total_items}ê°œ
â€¢ ëª¨ë‹ˆí„°ë§ ê¸°ë¡: {total_monitoring}ê°œ

ğŸ“ˆ ì‚¬ì´íŠ¸ë³„ ë¶„í¬
"""
        
        for site_type, count in site_stats.items():
            percentage = (count / total_items * 100) if total_items > 0 else 0
            report += f"â€¢ {site_type}: {count}ê°œ ({percentage:.1f}%)\n"
        
        if self.historical_data:
            report += f"""
â° ëª¨ë‹ˆí„°ë§ ì •ë³´
â€¢ ì²« ê¸°ë¡: {self.historical_data[0]['timestamp'].strftime('%Y-%m-%d %H:%M')}
â€¢ ë§ˆì§€ë§‰ ê¸°ë¡: {self.historical_data[-1]['timestamp'].strftime('%Y-%m-%d %H:%M')}
â€¢ ì´ ëª¨ë‹ˆí„°ë§ ê¸°ê°„: {(self.historical_data[-1]['timestamp'] - self.historical_data[0]['timestamp']).days}ì¼
"""
        
        # í‚¤ì›Œë“œ ë¶„ì„
        if self.keyword_listbox.size() > 0:
            keywords = list(self.keyword_listbox.get(0, tk.END))
            report += f"""
ğŸ” ëª¨ë‹ˆí„°ë§ í‚¤ì›Œë“œ
â€¢ ì„¤ì •ëœ í‚¤ì›Œë“œ: {', '.join(keywords)}
"""
        
        report += f"""
ğŸ“‹ ë¶„ì„ ê¶Œì¥ì‚¬í•­
â€¢ ë°ì´í„°ê°€ ì¶©ë¶„íˆ ìŒ“ì´ë©´ ë” ì •í™•í•œ íŠ¸ë Œë“œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤
â€¢ ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë³€í™” íŒ¨í„´ì„ íŒŒì•…í•˜ì„¸ìš”
â€¢ ì•Œë¦¼ ì„¤ì •ì„ í†µí•´ ì¤‘ìš”í•œ ë³€í™”ë¥¼ ë†“ì¹˜ì§€ ë§ˆì„¸ìš”

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        
        # í†µê³„ í…ìŠ¤íŠ¸ ì˜ì—­ì— í‘œì‹œ
        self.stats_text.delete(1.0, tk.END)
        self.stats_text.insert(1.0, report)
        
        messagebox.showinfo("ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ", "ë¶„ì„ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ìŠ¤ì¼€ì¤„ë§ ê´€ë ¨ ë©”ì„œë“œë“¤
    def add_schedule(self):
        """ìƒˆë¡œìš´ ìŠ¤ì¼€ì¤„ì„ ì¶”ê°€í•©ë‹ˆë‹¤."""
        schedule_type = self.schedule_type.get()
        hour = int(self.schedule_hour.get())
        minute = int(self.schedule_minute.get())
        
        try:
            if schedule_type == "ì¼íšŒì„±":
                # ì˜¤ëŠ˜ ë˜ëŠ” ë‚´ì¼ ì§€ì •ëœ ì‹œê°„ì— ì‹¤í–‰
                now = datetime.now()
                scheduled_time = now.replace(hour=hour, minute=minute, second=0, microsecond=0)
                if scheduled_time <= now:
                    scheduled_time += timedelta(days=1)
                
                job = self.scheduler.add_job(
                    func=self.scheduled_crawl,
                    trigger='date',
                    run_date=scheduled_time,
                    id=f"onetime_{scheduled_time.strftime('%Y%m%d_%H%M%S')}"
                )
                
            elif schedule_type == "ë§¤ì¼":
                job = self.scheduler.add_job(
                    func=self.scheduled_crawl,
                    trigger=CronTrigger(hour=hour, minute=minute),
                    id=f"daily_{hour:02d}{minute:02d}"
                )
                
            elif schedule_type == "ë§¤ì£¼":
                weekday_map = {"ì›”ìš”ì¼": 0, "í™”ìš”ì¼": 1, "ìˆ˜ìš”ì¼": 2, "ëª©ìš”ì¼": 3, 
                             "ê¸ˆìš”ì¼": 4, "í† ìš”ì¼": 5, "ì¼ìš”ì¼": 6}
                weekday = weekday_map[self.schedule_weekday.get()]
                
                job = self.scheduler.add_job(
                    func=self.scheduled_crawl,
                    trigger=CronTrigger(day_of_week=weekday, hour=hour, minute=minute),
                    id=f"weekly_{weekday}_{hour:02d}{minute:02d}"
                )
                
            elif schedule_type == "ë§¤ì›”":
                day = int(self.schedule_day.get())
                job = self.scheduler.add_job(
                    func=self.scheduled_crawl,
                    trigger=CronTrigger(day=day, hour=hour, minute=minute),
                    id=f"monthly_{day}_{hour:02d}{minute:02d}"
                )
            
            self.scheduled_jobs.append({
                'job': job,
                'type': schedule_type,
                'time': f"{hour:02d}:{minute:02d}",
                'email_results': self.email_results.get()
            })
            
            self.update_schedule_list()
            messagebox.showinfo("ìŠ¤ì¼€ì¤„ ì¶”ê°€", f"{schedule_type} ìŠ¤ì¼€ì¤„ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            messagebox.showerror("ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì˜¤ë¥˜", f"ìŠ¤ì¼€ì¤„ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def remove_schedule(self):
        """ì„ íƒëœ ìŠ¤ì¼€ì¤„ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
        try:
            selection = self.schedule_tree.selection()
            if not selection:
                messagebox.showwarning("ì„ íƒ í•„ìš”", "ì‚­ì œí•  ìŠ¤ì¼€ì¤„ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                return
            
            item = self.schedule_tree.item(selection[0])
            job_id = item['values'][0]
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì‘ì—… ì œê±°
            self.scheduler.remove_job(job_id)
            
            # ëª©ë¡ì—ì„œ ì œê±°
            self.scheduled_jobs = [job for job in self.scheduled_jobs if job['job'].id != job_id]
            
            self.update_schedule_list()
            messagebox.showinfo("ìŠ¤ì¼€ì¤„ ì‚­ì œ", "ì„ íƒëœ ìŠ¤ì¼€ì¤„ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            messagebox.showerror("ìŠ¤ì¼€ì¤„ ì‚­ì œ ì˜¤ë¥˜", f"ìŠ¤ì¼€ì¤„ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def update_schedule_list(self):
        """ìŠ¤ì¼€ì¤„ ëª©ë¡ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        # ê¸°ì¡´ í•­ëª© ì‚­ì œ
        for item in self.schedule_tree.get_children():
            self.schedule_tree.delete(item)
        
        # í˜„ì¬ ìŠ¤ì¼€ì¤„ ëª©ë¡ ì¶”ê°€
        for job_info in self.scheduled_jobs:
            job = job_info['job']
            next_run = job.next_run_time.strftime('%Y-%m-%d %H:%M:%S') if job.next_run_time else "N/A"
            
            self.schedule_tree.insert('', 'end', values=(
                job.id,
                job_info['type'],
                job_info['time'],
                next_run,
                "í™œì„±"
            ))
    
    def scheduled_crawl(self):
        """ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        try:
            # ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì‹¤í–‰
            self.start_crawling(scheduled=True)
            
            # í¬ë¡¤ë§ ì™„ë£Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 30ë¶„)
            wait_time = 0
            while self.is_crawling and wait_time < 1800:  # 30ë¶„
                time.sleep(10)
                wait_time += 10
            
            # ê²°ê³¼ ì´ë©”ì¼ ë°œì†¡
            if self.email_results.get() and self.email_enabled.get():
                self.send_crawling_results_email()
                
        except Exception as e:
            print(f"ìŠ¤ì¼€ì¤„ëœ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
    
    def send_crawling_results_email(self):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì´ë©”ì¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤."""
        try:
            if not self.crawled_data:
                return
            
            # ê²°ê³¼ ìš”ì•½ ìƒì„±
            total_items = len(self.crawled_data)
            sites = set(urlparse(item.get('url', '')).netloc for item in self.crawled_data)
            
            # ì—‘ì…€ íŒŒì¼ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_filename = f"scheduled_crawling_{timestamp}.xlsx"
            
            # ì„ì‹œë¡œ ì—‘ì…€ íŒŒì¼ ì €ì¥
            processed_data = []
            for item in self.crawled_data:
                processed_item = {}
                for key, value in item.items():
                    if isinstance(value, str):
                        value = unicodedata.normalize('NFC', value)
                        value = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', value)
                        value = re.sub(r'\s+', ' ', value).strip()
                        if len(value) > 32000:
                            value = value[:32000] + "..."
                    processed_item[key] = value
                processed_data.append(processed_item)
            
            df = pd.DataFrame(processed_data)
            df.columns = ['íƒ€ì…', 'ì œëª©/í…ìŠ¤íŠ¸', 'URL', 'ì„¤ëª…']
            
            with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='CrawlingResults', index=False)
            
            # ì´ë©”ì¼ ë‚´ìš© êµ¬ì„±
            subject = f"ì›¹ í¬ë¡¤ë§ ê²°ê³¼ - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
            
            body = f"""
ìë™ ì›¹ í¬ë¡¤ë§ ê²°ê³¼

ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
í¬ë¡¤ë§ URL: {self.current_task['url'] if self.current_task else 'N/A'}

ğŸ“Š ê²°ê³¼ ìš”ì•½:
â€¢ ì´ ìˆ˜ì§‘ í•­ëª©: {total_items}ê°œ
â€¢ í¬ë¡¤ë§ ì‚¬ì´íŠ¸: {len(sites)}ê°œ
â€¢ íŒŒì¼ëª…: {excel_filename}

ìƒì„¸ ê²°ê³¼ëŠ” ì²¨ë¶€ëœ ì—‘ì…€ íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.

ìë™ ë°œì†¡ëœ ë©”ì‹œì§€ì…ë‹ˆë‹¤.
            """
            
            # íŒŒì¼ ì²¨ë¶€ ì´ë©”ì¼ ë°œì†¡
            self.send_email_with_attachment(subject, body, excel_filename)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            if os.path.exists(excel_filename):
                os.remove(excel_filename)
                
        except Exception as e:
            print(f"ê²°ê³¼ ì´ë©”ì¼ ë°œì†¡ ì˜¤ë¥˜: {e}")
    
    def send_email_with_attachment(self, subject, body, filename):
        """ì²¨ë¶€íŒŒì¼ì´ ìˆëŠ” ì´ë©”ì¼ì„ ë°œì†¡í•©ë‹ˆë‹¤."""
        try:
            from email.mime.base import MimeBase
            from email import encoders
            
            sender_email = self.sender_email.get()
            sender_password = self.sender_password.get()
            receiver_email = self.receiver_email.get()
            
            if not all([sender_email, sender_password, receiver_email]):
                print("ì´ë©”ì¼ ì„¤ì •ì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return
            
            msg = MimeMultipart()
            msg['From'] = sender_email
            msg['To'] = receiver_email
            msg['Subject'] = subject
            
            # ë³¸ë¬¸ ì¶”ê°€
            msg.attach(MimeText(body, 'plain', 'utf-8'))
            
            # íŒŒì¼ ì²¨ë¶€
            if os.path.exists(filename):
                with open(filename, "rb") as attachment:
                    part = MimeBase('application', 'octet-stream')
                    part.set_payload(attachment.read())
                
                encoders.encode_base64(part)
                part.add_header(
                    'Content-Disposition',
                    f'attachment; filename= {os.path.basename(filename)}'
                )
                msg.attach(part)
            
            # ì´ë©”ì¼ ë°œì†¡
            server = smtplib.SMTP('smtp.gmail.com', 587)
            server.starttls()
            server.login(sender_email, sender_password)
            server.send_message(msg)
            server.quit()
            
            print("ì²¨ë¶€íŒŒì¼ ì´ë©”ì¼ ë°œì†¡ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ì²¨ë¶€íŒŒì¼ ì´ë©”ì¼ ë°œì†¡ ì˜¤ë¥˜: {e}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë° ë³µêµ¬ ê´€ë ¨ ë©”ì„œë“œë“¤
    def save_checkpoint(self):
        """í˜„ì¬ ì§„í–‰ ìƒí™©ì„ ì²´í¬í¬ì¸íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            checkpoint_data = {
                'task_progress': self.task_progress,
                'current_task': self.current_task,
                'crawled_data': self.crawled_data,
                'timestamp': datetime.now(),
                'current_page': self.current_page,
                'retry_count': self.retry_count
            }
            
            with open(self.checkpoint_file, 'wb') as f:
                pickle.dump(checkpoint_data, f)
            
            self.update_checkpoint_status()
            
        except Exception as e:
            print(f"ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_checkpoint(self):
        """ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            if not os.path.exists(self.checkpoint_file):
                return None
            
            with open(self.checkpoint_file, 'rb') as f:
                checkpoint_data = pickle.load(f)
            
            return checkpoint_data
            
        except Exception as e:
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None
    
    def restore_last_task(self):
        """ë§ˆì§€ë§‰ ì‘ì—…ì„ ë³µêµ¬í•©ë‹ˆë‹¤."""
        checkpoint_data = self.load_checkpoint()
        
        if not checkpoint_data:
            messagebox.showwarning("ë³µêµ¬ ë¶ˆê°€", "ë³µêµ¬í•  ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° ë³µì›
            self.task_progress = checkpoint_data['task_progress']
            self.current_task = checkpoint_data['current_task']
            self.crawled_data = checkpoint_data['crawled_data']
            self.current_page = checkpoint_data.get('current_page', 1)
            self.retry_count = checkpoint_data.get('retry_count', 0)
            
            # UI ì„¤ì • ë³µì›
            if self.current_task:
                self.url_var.set(self.current_task['url'])
                self.browser_mode.set(self.current_task['browser_mode'])
                self.max_pages.set(str(self.current_task['max_pages']))
                self.crawl_delay.set(str(self.current_task['crawl_delay']))
                self.site_type.set(self.current_task['site_type'])
            
            # ì§„í–‰ ìƒí™© í‘œì‹œ
            completed = self.task_progress['completed_pages']
            total = self.task_progress['total_pages']
            
            result = messagebox.askyesno(
                "ì‘ì—… ë³µêµ¬", 
                f"ë§ˆì§€ë§‰ ì‘ì—…ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.\n\n"
                f"URL: {self.current_task['url']}\n"
                f"ì§„í–‰ë¥ : {completed}/{total} í˜ì´ì§€\n"
                f"ì‹œì‘ ì‹œê°„: {self.current_task['started_at'].strftime('%Y-%m-%d %H:%M:%S')}\n\n"
                f"ì´ ì‘ì—…ì„ ì´ì–´ì„œ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ?"
            )
            
            if result:
                # í¬ë¡¤ë§ ì¬ì‹œì‘
                self.resume_crawling()
            
        except Exception as e:
            messagebox.showerror("ë³µêµ¬ ì˜¤ë¥˜", f"ì‘ì—… ë³µêµ¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def resume_crawling(self):
        """ì¤‘ë‹¨ëœ í¬ë¡¤ë§ì„ ì¬ì‹œì‘í•©ë‹ˆë‹¤."""
        try:
            self.is_crawling = True
            self.status_var.set(f"ë³µêµ¬ëœ ì‘ì—… ì¬ì‹œì‘ ì¤‘... ({self.task_progress['completed_pages']}/{self.task_progress['total_pages']})")
            
            # UI ìƒíƒœ ë³€ê²½
            self.crawl_button.config(state='disabled')
            self.stop_button.config(state='normal')
            self.progress.start()
            
            # ì‹¤íŒ¨í•œ í˜ì´ì§€ë¶€í„° ì¬ì‹œì‘
            url = self.current_task['url']
            
            if self.current_task['browser_mode'] == "selenium":
                thread = threading.Thread(target=self.crawl_with_selenium_checkpoint, args=(url, False, True))
            elif self.current_task['browser_mode'] == "playwright":
                thread = threading.Thread(target=self.crawl_with_playwright_checkpoint, args=(url, False, True))
            else:
                thread = threading.Thread(target=self.crawl_website_checkpoint, args=(url, False, True))
            
            thread.daemon = True
            thread.start()
            
        except Exception as e:
            messagebox.showerror("ì¬ì‹œì‘ ì˜¤ë¥˜", f"í¬ë¡¤ë§ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
    
    def clear_checkpoint(self, show_message=True):
        """ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤."""
        try:
            if os.path.exists(self.checkpoint_file):
                os.remove(self.checkpoint_file)
            
            self.task_progress = {
                'total_pages': 0,
                'completed_pages': 0,
                'failed_pages': [],
                'current_url': '',
                'settings': {}
            }
            self.current_task = None
            
            self.update_checkpoint_status()
            
            # ë©”ì‹œì§€ í‘œì‹œ ì—¬ë¶€ë¥¼ ì„ íƒì ìœ¼ë¡œ ì²˜ë¦¬
            if show_message:
                messagebox.showinfo("ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ", "ì²´í¬í¬ì¸íŠ¸ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            if show_message:
                messagebox.showerror("ì‚­ì œ ì˜¤ë¥˜", f"ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")
            else:
                print(f"ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì˜¤ë¥˜: {str(e)}")
    
    def update_checkpoint_status(self):
        """ì²´í¬í¬ì¸íŠ¸ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            if os.path.exists(self.checkpoint_file):
                checkpoint_data = self.load_checkpoint()
                if checkpoint_data:
                    timestamp = checkpoint_data['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
                    completed = checkpoint_data['task_progress']['completed_pages']
                    total = checkpoint_data['task_progress']['total_pages']
                    self.checkpoint_status.set(f"ì²´í¬í¬ì¸íŠ¸ ìˆìŒ: {timestamp} ({completed}/{total})")
                else:
                    self.checkpoint_status.set("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì†ìƒë¨")
            else:
                self.checkpoint_status.set("ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ")
        except:
            self.checkpoint_status.set("ì²´í¬í¬ì¸íŠ¸ ìƒíƒœ í™•ì¸ ë¶ˆê°€")
    
    # ì²´í¬í¬ì¸íŠ¸ë¥¼ í¬í•¨í•œ í¬ë¡¤ë§ ë©”ì„œë“œë“¤
    def crawl_website_checkpoint(self, url, scheduled=False, resume=False):
        """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì´ í¬í•¨ëœ ê¸°ë³¸ í¬ë¡¤ë§"""
        try:
            start_page = self.current_page if resume else 1
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            
            for page in range(start_page, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.task_progress['completed_pages'] = page - 1
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if self.auto_save.get() and page % 5 == 0:  # 5í˜ì´ì§€ë§ˆë‹¤ ì €ì¥
                    self.save_checkpoint()
                
                # í˜ì´ì§€ í¬ë¡¤ë§ ì‹œë„
                success = False
                for retry in range(self.max_retries):
                    try:
                        page_url = self.generate_page_url(url, page)
                        response = self.crawl_with_retry(page_url)
                        
                        if response:
                            soup = BeautifulSoup(response.content, 'html.parser')
                            self.root.after(0, self.update_results, page_url, response, soup)
                            success = True
                            break
                            
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"í˜ì´ì§€ {page} ì˜¤ë¥˜, ì¬ì‹œë„ {r}/{self.max_retries}: {err[:30]}"))
                        time.sleep(2)
                
                if not success:
                    self.task_progress['failed_pages'].append(page)
                
                # í¬ë¡¤ë§ ê°„ê²©
                if page < max_pages and self.is_crawling:
                    time.sleep(float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1)
            
            # ì‘ì—… ì™„ë£Œ ì²˜ë¦¬
            self.root.after(0, self.finalize_crawling_checkpoint, scheduled)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"ì²´í¬í¬ì¸íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
    
    def crawl_with_selenium_checkpoint(self, url, scheduled=False, resume=False):
        """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì´ í¬í•¨ëœ Selenium í¬ë¡¤ë§"""
        if not self.setup_selenium_driver():
            return
        
        try:
            start_page = self.current_page if resume else 1
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            
            for page in range(start_page, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.task_progress['completed_pages'] = page - 1
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if self.auto_save.get() and page % 3 == 0:  # 3í˜ì´ì§€ë§ˆë‹¤ ì €ì¥
                    self.save_checkpoint()
                
                success = False
                for retry in range(self.max_retries):
                    try:
                        page_url = self.generate_page_url(url, page)
                        self.driver.get(page_url)
                        WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        
                        # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ë§
                        if self.site_type.get() == "ë„¤ì´ë²„ ì‡¼í•‘":
                            self.crawl_naver_shopping()
                        elif self.site_type.get() == "ì¸ìŠ¤íƒ€ê·¸ë¨":
                            self.crawl_instagram()
                        elif self.site_type.get() == "ë¶€ë™ì‚°":
                            self.crawl_real_estate()
                        else:
                            self.crawl_general_selenium()
                        
                        success = True
                        break
                        
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"Selenium í˜ì´ì§€ {page} ì¬ì‹œë„ {r}/{self.max_retries}: {err[:30]}"))
                        time.sleep(2)
                
                if not success:
                    self.task_progress['failed_pages'].append(page)
                
                if page < max_pages and self.is_crawling:
                    time.sleep(float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1)
            
            self.root.after(0, self.finalize_crawling_checkpoint, scheduled)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"Selenium ì²´í¬í¬ì¸íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        finally:
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None
    
    def crawl_with_playwright_checkpoint(self, url, scheduled=False, resume=False):
        """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì´ í¬í•¨ëœ Playwright í¬ë¡¤ë§"""
        if not self.setup_playwright():
            return
        
        try:
            start_page = self.current_page if resume else 1
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            
            for page in range(start_page, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.task_progress['completed_pages'] = page - 1
                
                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if self.auto_save.get() and page % 3 == 0:  # 3í˜ì´ì§€ë§ˆë‹¤ ì €ì¥
                    self.save_checkpoint()
                
                success = False
                for retry in range(self.max_retries):
                    try:
                        page_url = self.generate_page_url(url, page)
                        self.page.goto(page_url, wait_until='domcontentloaded', timeout=30000)
                        self.page.wait_for_load_state('networkidle', timeout=10000)
                        self.page.wait_for_timeout(2000)
                        
                        domain = urlparse(page_url).netloc.lower()
                        if 'shopping.naver.com' in domain:
                            self.crawl_naver_shopping_playwright()
                        elif 'instagram.com' in domain:
                            self.crawl_instagram_playwright()
                        elif any(keyword in domain for keyword in ['zigbang', 'dabang', 'ë¶€ë™ì‚°']):
                            self.crawl_real_estate_playwright()
                        else:
                            self.crawl_general_playwright()
                        
                        success = True
                        break
                        
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"Playwright í˜ì´ì§€ {page} ì¬ì‹œë„ {r}/{self.max_retries}: {err[:30]}"))
                        time.sleep(2)
                
                if not success:
                    self.task_progress['failed_pages'].append(page)
                
                if page < max_pages and self.is_crawling:
                    time.sleep(float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1)
            
            self.root.after(0, self.finalize_crawling_checkpoint, scheduled)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"Playwright ì²´í¬í¬ì¸íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        finally:
            # Playwright ì •ë¦¬
            if self.page:
                try:
                    self.page.close()
                except:
                    pass
                self.page = None
            
            if self.browser:
                try:
                    self.browser.close()
                except:
                    pass
                self.browser = None
            
            if self.playwright:
                try:
                    self.playwright.stop()
                except:
                    pass
                self.playwright = None
    
    def finalize_crawling_checkpoint(self, scheduled=False):
        """ì²´í¬í¬ì¸íŠ¸ ê¸°ëŠ¥ì´ í¬í•¨ëœ í¬ë¡¤ë§ ì™„ë£Œ ì²˜ë¦¬"""
        self.progress.stop()
        if not scheduled:
            self.crawl_button.config(state='normal')
            self.stop_button.config(state='disabled')
            self.export_button.config(state='normal')
        
        self.is_crawling = False
        
        total_items = len(self.crawled_data)
        failed_pages = len(self.task_progress['failed_pages'])
        
        status_msg = f"í¬ë¡¤ë§ ì™„ë£Œ - ì´ {total_items}ê°œ ë°ì´í„° ìˆ˜ì§‘"
        if failed_pages > 0:
            status_msg += f" (ì‹¤íŒ¨: {failed_pages}í˜ì´ì§€)"
        
        self.status_var.set(status_msg)
        
        # ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ (ì‘ì—… ì™„ë£Œì‹œ) - ë©”ì‹œì§€ ì—†ì´ ìë™ ì‚­ì œ
        if self.auto_save.get():
            self.clear_checkpoint(show_message=False)

def main():
    root = tk.Tk()
    app = WebCrawlerApp(root)
    root.mainloop()

if __name__ == "__main__":
    main() 