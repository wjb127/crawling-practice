import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import requests
from bs4 import BeautifulSoup
import threading
from urllib.parse import urljoin, urlparse
import pandas as pd
from datetime import datetime
import os
import time
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service

class WebCrawlerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("ì›¹ í¬ë¡¤ëŸ¬")
        self.root.geometry("800x600")
        
        # ë©”ì¸ í”„ë ˆì„
        main_frame = ttk.Frame(root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # URL ì…ë ¥ ì„¹ì…˜
        url_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ì„¤ì •", padding="5")
        url_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        ttk.Label(url_frame, text="URL:").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        self.url_var = tk.StringVar(value="https://example.com")
        self.url_entry = ttk.Entry(url_frame, textvariable=self.url_var, width=60)
        self.url_entry.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(0, 5))
        
        self.crawl_button = ttk.Button(url_frame, text="í¬ë¡¤ë§ ì‹œì‘", command=self.start_crawling)
        self.crawl_button.grid(row=0, column=2, padx=(5, 0))
        
        self.stop_button = ttk.Button(url_frame, text="ì¤‘ì§€", command=self.stop_crawling, state='disabled')
        self.stop_button.grid(row=0, column=3, padx=(5, 0))
        
        # í¬ë¡¤ë§ ì˜µì…˜
        options_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ì˜µì…˜", padding="5")
        options_frame.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(5, 0))
        
        # ê¸°ë³¸ ì¶”ì¶œ ì˜µì…˜
        extract_frame = ttk.Frame(options_frame)
        extract_frame.grid(row=0, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(0, 5))
        
        self.extract_links = tk.BooleanVar(value=True)
        ttk.Checkbutton(extract_frame, text="ë§í¬ ì¶”ì¶œ", variable=self.extract_links).grid(row=0, column=0, sticky=tk.W)
        
        self.extract_images = tk.BooleanVar(value=True)
        ttk.Checkbutton(extract_frame, text="ì´ë¯¸ì§€ ì¶”ì¶œ", variable=self.extract_images).grid(row=0, column=1, sticky=tk.W, padx=(10, 0))
        
        self.extract_text = tk.BooleanVar(value=True)
        ttk.Checkbutton(extract_frame, text="í…ìŠ¤íŠ¸ ì¶”ì¶œ", variable=self.extract_text).grid(row=0, column=2, sticky=tk.W, padx=(10, 0))
        
        # ê³ ê¸‰ ì˜µì…˜
        advanced_frame = ttk.Frame(options_frame)
        advanced_frame.grid(row=1, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(5, 0))
        
        # ë¸Œë¼ìš°ì € ëª¨ë“œ
        ttk.Label(advanced_frame, text="ë¸Œë¼ìš°ì € ëª¨ë“œ:").grid(row=0, column=0, sticky=tk.W)
        self.browser_mode = tk.StringVar(value="requests")
        browser_combo = ttk.Combobox(advanced_frame, textvariable=self.browser_mode, values=["requests", "selenium"], width=10)
        browser_combo.grid(row=0, column=1, sticky=tk.W, padx=(5, 10))
        
        # í˜ì´ì§€ ìˆ˜
        ttk.Label(advanced_frame, text="í˜ì´ì§€ ìˆ˜:").grid(row=0, column=2, sticky=tk.W)
        self.max_pages = tk.StringVar(value="1")
        ttk.Entry(advanced_frame, textvariable=self.max_pages, width=5).grid(row=0, column=3, sticky=tk.W, padx=(5, 10))
        
        # í¬ë¡¤ë§ ê°„ê²©
        ttk.Label(advanced_frame, text="ê°„ê²©(ì´ˆ):").grid(row=0, column=4, sticky=tk.W)
        self.crawl_delay = tk.StringVar(value="1")
        ttk.Entry(advanced_frame, textvariable=self.crawl_delay, width=5).grid(row=0, column=5, sticky=tk.W, padx=(5, 10))
        
        # ì‚¬ì´íŠ¸ë³„ ì„¤ì •
        site_frame = ttk.Frame(options_frame)
        site_frame.grid(row=2, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(5, 0))
        
        ttk.Label(site_frame, text="ì‚¬ì´íŠ¸ ì¢…ë¥˜:").grid(row=0, column=0, sticky=tk.W)
        self.site_type = tk.StringVar(value="ì¼ë°˜")
        site_combo = ttk.Combobox(site_frame, textvariable=self.site_type, 
                                values=["ì¼ë°˜", "ë„¤ì´ë²„ ì‡¼í•‘", "ì¸ìŠ¤íƒ€ê·¸ë¨", "ë¶€ë™ì‚°"], width=12)
        site_combo.grid(row=0, column=1, sticky=tk.W, padx=(5, 10))
        
        # ì¶”ì¶œí•  ë°ì´í„° ì„ íƒ
        data_frame = ttk.Frame(options_frame)
        data_frame.grid(row=3, column=0, columnspan=4, sticky=(tk.W, tk.E), pady=(5, 0))
        
        ttk.Label(data_frame, text="ì¶”ì¶œ ë°ì´í„°:").grid(row=0, column=0, sticky=tk.W)
        self.extract_title = tk.BooleanVar(value=True)
        ttk.Checkbutton(data_frame, text="ì œëª©", variable=self.extract_title).grid(row=0, column=1, sticky=tk.W, padx=(5, 0))
        
        self.extract_price = tk.BooleanVar(value=True)
        ttk.Checkbutton(data_frame, text="ê°€ê²©", variable=self.extract_price).grid(row=0, column=2, sticky=tk.W, padx=(5, 0))
        
        self.extract_date = tk.BooleanVar(value=True)
        ttk.Checkbutton(data_frame, text="ë‚ ì§œ", variable=self.extract_date).grid(row=0, column=3, sticky=tk.W, padx=(5, 0))
        
        self.extract_description = tk.BooleanVar(value=False)
        ttk.Checkbutton(data_frame, text="ì„¤ëª…", variable=self.extract_description).grid(row=0, column=4, sticky=tk.W, padx=(5, 0))
        
        # ì§„í–‰ìƒí™© í‘œì‹œ
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # ì—‘ì…€ ì €ì¥ ë²„íŠ¼
        export_frame = ttk.Frame(main_frame)
        export_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        self.export_button = ttk.Button(export_frame, text="ì—‘ì…€ë¡œ ì €ì¥", command=self.export_to_excel, state='disabled')
        self.export_button.pack(side=tk.RIGHT)
        
        ttk.Label(export_frame, text="í¬ë¡¤ë§ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.").pack(side=tk.LEFT)
        
        # ê²°ê³¼ í‘œì‹œ ì˜ì—­
        result_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ê²°ê³¼", padding="5")
        result_frame.grid(row=4, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # íƒ­ ìœ„ì ¯
        self.notebook = ttk.Notebook(result_frame)
        self.notebook.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í˜ì´ì§€ ì •ë³´ íƒ­
        self.info_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.info_frame, text="í˜ì´ì§€ ì •ë³´")
        
        self.info_text = scrolledtext.ScrolledText(self.info_frame, height=8, wrap=tk.WORD)
        self.info_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ë§í¬ íƒ­
        self.links_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.links_frame, text="ë§í¬")
        
        self.links_text = scrolledtext.ScrolledText(self.links_frame, height=8, wrap=tk.WORD)
        self.links_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ì´ë¯¸ì§€ íƒ­
        self.images_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.images_frame, text="ì´ë¯¸ì§€")
        
        self.images_text = scrolledtext.ScrolledText(self.images_frame, height=8, wrap=tk.WORD)
        self.images_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í…ìŠ¤íŠ¸ íƒ­
        self.content_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.content_frame, text="í…ìŠ¤íŠ¸ ë‚´ìš©")
        
        self.content_text = scrolledtext.ScrolledText(self.content_frame, height=8, wrap=tk.WORD)
        self.content_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ê²°ê³¼ í…Œì´ë¸” íƒ­
        self.table_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.table_frame, text="ê²°ê³¼ í…Œì´ë¸”")
        
        # í…Œì´ë¸” ë·° ìƒì„±
        self.create_table_view()
        
        # ì‚¬ì´íŠ¸ ì¶”ì²œ íƒ­
        self.recommend_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.recommend_frame, text="ì‚¬ì´íŠ¸ ì¶”ì²œ")
        
        self.recommend_text = scrolledtext.ScrolledText(self.recommend_frame, height=8, wrap=tk.WORD)
        self.recommend_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ê¸°ìˆ ìŠ¤íƒ ì„¤ëª… íƒ­
        self.tech_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.tech_frame, text="ê¸°ìˆ ìŠ¤íƒ ê°€ì´ë“œ")
        
        self.tech_text = scrolledtext.ScrolledText(self.tech_frame, height=8, wrap=tk.WORD)
        self.tech_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ìƒíƒœ í‘œì‹œì¤„
        self.status_var = tk.StringVar(value="ì¤€ë¹„")
        status_bar = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN)
        status_bar.grid(row=5, column=0, columnspan=2, sticky=(tk.W, tk.E))
        
        # ê·¸ë¦¬ë“œ ê°€ì¤‘ì¹˜ ì„¤ì •
        root.columnconfigure(0, weight=1)
        root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(4, weight=1)
        result_frame.columnconfigure(0, weight=1)
        result_frame.rowconfigure(0, weight=1)
        
        # ê° íƒ­ì˜ ê·¸ë¦¬ë“œ ì„¤ì •
        for frame in [self.info_frame, self.links_frame, self.images_frame, self.content_frame, 
                      self.table_frame, self.recommend_frame, self.tech_frame]:
            frame.columnconfigure(0, weight=1)
            frame.rowconfigure(0, weight=1)
        
        url_frame.columnconfigure(1, weight=1)
        
        # í¬ë¡¤ë§ ìƒíƒœ ë° ë°ì´í„° ì €ì¥ìš© ë³€ìˆ˜
        self.is_crawling = False
        self.crawled_data = []
        self.driver = None
        self.current_page = 1
        self.retry_count = 0
        self.max_retries = 3
        
        # íƒ­ ë‚´ìš© ì´ˆê¸°í™”
        self.load_static_content()
    
    def create_table_view(self):
        """ê²°ê³¼ í…Œì´ë¸” ë·°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # í…Œì´ë¸” í”„ë ˆì„
        table_container = ttk.Frame(self.table_frame)
        table_container.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í…Œì´ë¸” (Treeview) ìƒì„±
        columns = ('Type', 'Title', 'URL', 'Description')
        self.tree = ttk.Treeview(table_container, columns=columns, show='headings', height=15)
        
        # ì»¬ëŸ¼ í—¤ë” ì„¤ì •
        self.tree.heading('Type', text='íƒ€ì…')
        self.tree.heading('Title', text='ì œëª©/í…ìŠ¤íŠ¸')
        self.tree.heading('URL', text='URL')
        self.tree.heading('Description', text='ì„¤ëª…')
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree.column('Type', width=80, minwidth=60)
        self.tree.column('Title', width=200, minwidth=150)
        self.tree.column('URL', width=300, minwidth=200)
        self.tree.column('Description', width=150, minwidth=100)
        
        # ìŠ¤í¬ë¡¤ë°” ì¶”ê°€
        v_scrollbar = ttk.Scrollbar(table_container, orient=tk.VERTICAL, command=self.tree.yview)
        h_scrollbar = ttk.Scrollbar(table_container, orient=tk.HORIZONTAL, command=self.tree.xview)
        self.tree.configure(yscrollcommand=v_scrollbar.set, xscrollcommand=h_scrollbar.set)
        
        # ìœ„ì ¯ ë°°ì¹˜
        self.tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        v_scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        h_scrollbar.grid(row=1, column=0, sticky=(tk.W, tk.E))
        
        # ê·¸ë¦¬ë“œ ê°€ì¤‘ì¹˜ ì„¤ì •
        table_container.columnconfigure(0, weight=1)
        table_container.rowconfigure(0, weight=1)
    
    def stop_crawling(self):
        """í¬ë¡¤ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.is_crawling = False
        self.status_var.set("í¬ë¡¤ë§ ì¤‘ì§€ë¨")
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None
    
    def start_crawling(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        url = self.url_var.get().strip()
        if not url:
            messagebox.showerror("ì˜¤ë¥˜", "URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            self.url_var.set(url)
        
        # UI ìƒíƒœ ë³€ê²½
        self.crawl_button.config(state='disabled')
        self.stop_button.config(state='normal')
        self.export_button.config(state='disabled')
        self.progress.start()
        self.status_var.set("í¬ë¡¤ë§ ì¤‘...")
        self.is_crawling = True
        
        # ê²°ê³¼ ì˜ì—­ ì´ˆê¸°í™”
        self.clear_results()
        self.crawled_data = []
        self.current_page = 1
        self.retry_count = 0
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        if self.browser_mode.get() == "selenium":
            thread = threading.Thread(target=self.crawl_with_selenium, args=(url,))
        else:
            thread = threading.Thread(target=self.crawl_website, args=(url,))
        thread.daemon = True
        thread.start()
    
    def clear_results(self):
        """ê²°ê³¼ ì˜ì—­ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.info_text.delete(1.0, tk.END)
        self.links_text.delete(1.0, tk.END)
        self.images_text.delete(1.0, tk.END)
        self.content_text.delete(1.0, tk.END)
        
        # í…Œì´ë¸” ì´ˆê¸°í™”
        for item in self.tree.get_children():
            self.tree.delete(item)
    
    def load_static_content(self):
        """ì •ì ì¸ íƒ­ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        # ì‚¬ì´íŠ¸ ì¶”ì²œ ë‚´ìš©
        recommend_content = """
ğŸŒ í¬ë¡¤ë§í•˜ê¸° ì‰¬ìš´ ì¶”ì²œ ì‚¬ì´íŠ¸ë“¤

ğŸ“° ë‰´ìŠ¤ ì‚¬ì´íŠ¸
â€¢ https://news.ycombinator.com - í•´ì»¤ë‰´ìŠ¤ (ê°„ë‹¨í•œ HTML êµ¬ì¡°)
â€¢ https://quotes.toscrape.com - í¬ë¡¤ë§ ì—°ìŠµìš© ì‚¬ì´íŠ¸
â€¢ https://books.toscrape.com - ë„ì„œ ì •ë³´ ì—°ìŠµ ì‚¬ì´íŠ¸

ğŸ›ï¸ ê³µê³µ ë°ì´í„°
â€¢ https://httpbin.org - HTTP í…ŒìŠ¤íŠ¸ìš© ì‚¬ì´íŠ¸
â€¢ https://jsonplaceholder.typicode.com - JSON API í…ŒìŠ¤íŠ¸
â€¢ https://reqres.in - API í…ŒìŠ¤íŠ¸ìš©

ğŸ“Š ì—°ìŠµìš© ì‚¬ì´íŠ¸
â€¢ https://scrape.center - í¬ë¡¤ë§ ì—°ìŠµ ì „ìš©
â€¢ https://webscraper.io/test-sites - ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
â€¢ https://example.com - ê°€ì¥ ê¸°ë³¸ì ì¸ í…ŒìŠ¤íŠ¸

ğŸ›’ ì „ììƒê±°ë˜ (ì£¼ì˜ í•„ìš”)
â€¢ ì¼ë¶€ ì „ììƒê±°ë˜ ì‚¬ì´íŠ¸ë“¤ (robots.txt ì¤€ìˆ˜ í•„ìˆ˜)
â€¢ ì˜¤í”ˆ ë§ˆì¼“ ìƒí’ˆ ì •ë³´ (ì´ìš©ì•½ê´€ í™•ì¸ í•„ìš”)

âš ï¸ ì£¼ì˜ì‚¬í•­
â€¢ robots.txt íŒŒì¼ì„ í•­ìƒ í™•ì¸í•˜ì„¸ìš”
â€¢ ê³¼ë„í•œ ìš”ì²­ì€ í”¼í•˜ì„¸ìš” (ì„œë²„ ë¶€í•˜ ë°©ì§€)
â€¢ ì €ì‘ê¶Œì´ ìˆëŠ” ì½˜í…ì¸ ëŠ” ì£¼ì˜í•˜ì„¸ìš”
â€¢ ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„°ëŠ” ìˆ˜ì§‘í•˜ì§€ ë§ˆì„¸ìš”
â€¢ ì›¹ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ì„ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”

ğŸ¯ ì´ˆë³´ì ì¶”ì²œ ìˆœì„œ
1. quotes.toscrape.com (ê¸°ë³¸ ì—°ìŠµ)
2. books.toscrape.com (í˜ì´ì§€ë„¤ì´ì…˜ ì—°ìŠµ)
3. news.ycombinator.com (ì‹¤ì œ ì‚¬ì´íŠ¸ ì—°ìŠµ)
4. ì›í•˜ëŠ” ì‚¬ì´íŠ¸ (ë‹¨ê³„ì  ì ‘ê·¼)
"""
        
        # ê¸°ìˆ ìŠ¤íƒ ê°€ì´ë“œ ë‚´ìš©
        tech_content = """
ğŸŒ ì‚¬ì´íŠ¸ ê¸°ìˆ ìŠ¤íƒë³„ í¬ë¡¤ë§ ë‚œì´ë„ ê°€ì´ë“œ

ğŸ“Š í¬ë¡¤ë§ ë‚œì´ë„ ë¶„ë¥˜

ğŸŸ¢ ì‰¬ì›€ (ì •ì  ì‚¬ì´íŠ¸)
â€¢ ì¼ë°˜ HTML/CSS ì‚¬ì´íŠ¸
â€¢ ì„œë²„ ì‚¬ì´ë“œ ë Œë”ë§ (SSR)
â€¢ WordPress, Jekyll ë“± ì •ì  ì‚¬ì´íŠ¸ ìƒì„±ê¸°

í¬ë¡¤ë§ ë°©ë²•:
â†’ requests + BeautifulSoupë¡œ ì¶©ë¶„
â†’ í˜ì´ì§€ ì†ŒìŠ¤ì— ëª¨ë“  ë°ì´í„°ê°€ í¬í•¨ë¨
â†’ ì˜ˆ: ë¸”ë¡œê·¸, ë‰´ìŠ¤ ì‚¬ì´íŠ¸, ê¸°ì—… í™ˆí˜ì´ì§€

ğŸŸ¡ ë³´í†µ (í•˜ì´ë¸Œë¦¬ë“œ)
â€¢ jQuery ê¸°ë°˜ ì‚¬ì´íŠ¸
â€¢ ì¼ë¶€ ë™ì  ë¡œë”©ì´ ìˆëŠ” ì‚¬ì´íŠ¸
â€¢ AJAXë¡œ ì¼ë¶€ ì½˜í…ì¸  ë¡œë“œ

í¬ë¡¤ë§ ë°©ë²•:
â†’ requests + BeautifulSoup (ê¸°ë³¸ ì½˜í…ì¸ )
â†’ API ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„ í›„ ì§ì ‘ í˜¸ì¶œ
â†’ í•„ìš”ì‹œ Seleniumìœ¼ë¡œ ë™ì  ë¶€ë¶„ë§Œ ì²˜ë¦¬

ğŸŸ  ì–´ë ¤ì›€ (SPA - Single Page Application)
â€¢ React, Vue.js, Angular ê¸°ë°˜
â€¢ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ë Œë”ë§ (CSR)
â€¢ ëŒ€ë¶€ë¶„ì˜ ì½˜í…ì¸ ê°€ JavaScriptë¡œ ìƒì„±

í¬ë¡¤ë§ ë°©ë²•:
â†’ Selenium ë˜ëŠ” Playwright í•„ìˆ˜
â†’ í˜ì´ì§€ ë¡œë”© ì™„ë£Œê¹Œì§€ ëŒ€ê¸° í•„ìš”
â†’ ì˜ˆ: ëª¨ë˜ ì „ììƒê±°ë˜, ì†Œì…œë¯¸ë””ì–´

ğŸ”´ ë§¤ìš° ì–´ë ¤ì›€ (ê³ ë„í™”ëœ SPA)
â€¢ Next.js, Nuxt.js (SSR + CSR í˜¼í•©)
â€¢ ë³µì¡í•œ ìƒíƒœ ê´€ë¦¬ (Redux, Vuex)
â€¢ ì§€ì—° ë¡œë”©, ë¬´í•œ ìŠ¤í¬ë¡¤
â€¢ ê°•ë ¥í•œ ë´‡ ì°¨ë‹¨ ì‹œìŠ¤í…œ

í¬ë¡¤ë§ ë°©ë²•:
â†’ Playwright + ê³ ê¸‰ ê¸°ë²•
â†’ í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € + í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
â†’ API ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§
â†’ ì˜ˆ: Netflix, Instagram, LinkedIn

ğŸ› ï¸ ê¸°ìˆ ìŠ¤íƒë³„ ìƒì„¸ ë¶„ì„

ğŸ“„ ì •ì  HTML ì‚¬ì´íŠ¸
íŠ¹ì§•: ì„œë²„ì—ì„œ ì™„ì „í•œ HTML ë°˜í™˜
ë„êµ¬: requests + BeautifulSoup
ì˜ˆì‹œ: ì •ë¶€ê¸°ê´€, í•™êµ, ì „í†µì  ê¸°ì—… ì‚¬ì´íŠ¸

ğŸ¯ WordPress/Drupal ì‚¬ì´íŠ¸
íŠ¹ì§•: PHP ê¸°ë°˜, ëŒ€ë¶€ë¶„ ì„œë²„ ë Œë”ë§
ë„êµ¬: requests + BeautifulSoup
ì£¼ì˜: í”ŒëŸ¬ê·¸ì¸ì— ë”°ë¼ ë™ì  ìš”ì†Œ ìˆì„ ìˆ˜ ìˆìŒ

âš¡ jQuery ê¸°ë°˜ ì‚¬ì´íŠ¸
íŠ¹ì§•: í˜ì´ì§€ ë¡œë“œ í›„ ì¼ë¶€ ì½˜í…ì¸  ì¶”ê°€ ë¡œë“œ
ë„êµ¬: requests (ê¸°ë³¸) + Selenium (ë™ì  ë¶€ë¶„)
ì „ëµ: Network íƒ­ì—ì„œ AJAX ìš”ì²­ ë¶„ì„

ğŸ”¥ React/Vue/Angular SPA
íŠ¹ì§•: ë¹ˆ HTML + JavaScriptë¡œ ëª¨ë“  ì½˜í…ì¸  ìƒì„±
ë„êµ¬: Selenium/Playwright í•„ìˆ˜
ì „ëµ: 
- componentDidMount ëŒ€ê¸°
- API í˜¸ì¶œ ì§ì ‘ ë¶„ì„ (ë” íš¨ìœ¨ì )
- SSR ë²„ì „ ì°¾ê¸° (SEOìš© í˜ì´ì§€)

ğŸª ì „ììƒê±°ë˜ í”Œë«í¼ë³„
â€¢ Shopify: Liquid í…œí”Œë¦¿, ëŒ€ë¶€ë¶„ ì„œë²„ ë Œë”ë§
â€¢ WooCommerce: WordPress ê¸°ë°˜, ë¹„êµì  ì‰¬ì›€
â€¢ Magento: ë³µì¡í•œ êµ¬ì¡°, ì¼ë¶€ AJAX
â€¢ ì»¤ìŠ¤í…€ React: ë§¤ìš° ì–´ë ¤ì›€

ğŸ“± ëª¨ë°”ì¼ ì•± ê¸°ë°˜ ì›¹
íŠ¹ì§•: ì•±ê³¼ ë™ì¼í•œ API ì‚¬ìš©, ê³ ë„í™”ëœ ë³´ì•ˆ
ë„êµ¬: API ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ + requests
ì „ëµ: ëª¨ë°”ì¼ ì•± íŒ¨í‚· ë¶„ì„

ğŸ›¡ï¸ ë´‡ ì°¨ë‹¨ ê¸°ìˆ ë³„ ëŒ€ì‘
â€¢ Cloudflare: User-Agent, í—¤ë” ì¡°ì‘
â€¢ reCAPTCHA: ìˆ˜ë™ í•´ê²° ë˜ëŠ” ìš°íšŒ
â€¢ Rate Limiting: ìš”ì²­ ê°„ê²© ì¡°ì ˆ
â€¢ JavaScript Challenge: í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € í•„ìˆ˜

ğŸ¯ ì‹¤ì „ íŒë³„ ë°©ë²•

1ï¸âƒ£ í˜ì´ì§€ ì†ŒìŠ¤ ë³´ê¸° (Ctrl+U)
â†’ ë‚´ìš©ì´ ë³´ì´ë©´: ì •ì  (ì‰¬ì›€)
â†’ ê±°ì˜ ë¹„ì–´ìˆìœ¼ë©´: SPA (ì–´ë ¤ì›€)

2ï¸âƒ£ ê°œë°œì ë„êµ¬ Network íƒ­
â†’ HTML í•˜ë‚˜ë§Œ: ì •ì 
â†’ ë§ì€ XHR/Fetch: ë™ì 

3ï¸âƒ£ JavaScript ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
â†’ ë‚´ìš©ì´ ê·¸ëŒ€ë¡œ: ì •ì 
â†’ ë‚´ìš©ì´ ì‚¬ë¼ì§: ë™ì 

4ï¸âƒ£ URL íŒ¨í„´ í™•ì¸
â†’ /page/1, /category/tech: ì „í†µì 
â†’ /#/page/1, /app: SPA ê°€ëŠ¥ì„±

ğŸ’¡ íš¨ìœ¨ì ì¸ ì ‘ê·¼ ì „ëµ

1. ì •ì  ë¶„ì„ ìš°ì„  (requests + BeautifulSoup)
2. ì•ˆ ë˜ë©´ API ë¶„ì„ (Network íƒ­)
3. ë§ˆì§€ë§‰ì— Selenium/Playwright ì‚¬ìš©
4. ê°€ëŠ¥í•˜ë©´ ëª¨ë°”ì¼ ë²„ì „ì´ë‚˜ RSS í™œìš©

âš ï¸ ì£¼ì˜ì‚¬í•­
â€¢ robots.txtì™€ ì´ìš©ì•½ê´€ í™•ì¸ í•„ìˆ˜
â€¢ ê³¼ë„í•œ ìš”ì²­ ê¸ˆì§€ (ì„œë²„ ë¶€í•˜)
â€¢ ê°œì¸ì •ë³´ ì²˜ë¦¬ ì‹œ ë²•ì  ê²€í†  í•„ìš”
â€¢ ì €ì‘ê¶Œ ì½˜í…ì¸  ì£¼ì˜
"""
        
        self.recommend_text.insert(tk.END, recommend_content)
        self.tech_text.insert(tk.END, tech_content)
        
        # í¸ì§‘ ë¶ˆê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        self.recommend_text.config(state='disabled')
        self.tech_text.config(state='disabled')
    
    def crawl_website(self, url):
        """requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            delay = float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            for page in range(1, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p}/{max_pages} í¬ë¡¤ë§ ì¤‘..."))
                
                # í˜ì´ì§€ë³„ URL ìƒì„±
                page_url = self.generate_page_url(url, page)
                
                # ì¬ì‹œë„ ë¡œì§
                success = False
                for retry in range(self.max_retries):
                    try:
                        response = requests.get(page_url, headers=headers, timeout=10)
                        response.raise_for_status()
                        
                        # í•œê¸€ ì¸ì½”ë”© ì²˜ë¦¬ ê°œì„ 
                        if response.encoding == 'ISO-8859-1':
                            # ì˜ëª»ëœ ì¸ì½”ë”©ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
                            response.encoding = response.apparent_encoding
                        elif not response.encoding:
                            # ì¸ì½”ë”©ì´ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš°
                            response.encoding = 'utf-8'
                        
                        # í•œê¸€ ì‚¬ì´íŠ¸ì˜ ê²½ìš° ì¶”ê°€ ì²´í¬
                        if 'charset=' in response.headers.get('content-type', '').lower():
                            charset = response.headers.get('content-type', '').lower()
                            if 'euc-kr' in charset:
                                response.encoding = 'euc-kr'
                            elif 'utf-8' in charset:
                                response.encoding = 'utf-8'
                        else:
                            # apparent_encodingìœ¼ë¡œ ìë™ ê°ì§€
                            response.encoding = response.apparent_encoding or 'utf-8'
                        
                        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # ê²°ê³¼ ì—…ë°ì´íŠ¸ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
                        self.root.after(0, self.update_results, page_url, response, soup)
                        
                        success = True
                        break
                        
                    except requests.exceptions.RequestException as e:
                        self.root.after(0, lambda r=retry+1: self.status_var.set(f"ìš”ì²­ ì‹¤íŒ¨, ì¬ì‹œë„ {r}/{self.max_retries}"))
                        time.sleep(2)
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"ì˜¤ë¥˜ ë°œìƒ, ì¬ì‹œë„ {r}/{self.max_retries}: {err[:50]}"))
                        time.sleep(2)
                
                if not success:
                    self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p} í¬ë¡¤ë§ ì‹¤íŒ¨"))
                
                # í¬ë¡¤ë§ ê°„ê²©
                if page < max_pages and self.is_crawling:
                    time.sleep(delay)
            
            self.root.after(0, self.finalize_crawling)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
    
    def update_results(self, url, response, soup):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ UIì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            # í˜ì´ì§€ ì •ë³´ - í•œê¸€ ì²˜ë¦¬ ê°œì„ 
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"
            
            # ì œëª© í…ìŠ¤íŠ¸ ì •ë¦¬
            if title_text:
                import unicodedata
                title_text = unicodedata.normalize('NFC', title_text)
                title_text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', title_text)
                title_text = re.sub(r'\s+', ' ', title_text).strip()
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', 'ì„¤ëª… ì—†ìŒ') if meta_desc else "ì„¤ëª… ì—†ìŒ"
            
            # ì„¤ëª… í…ìŠ¤íŠ¸ ì •ë¦¬
            if description and description != 'ì„¤ëª… ì—†ìŒ':
                description = unicodedata.normalize('NFC', description)
                description = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', description)
                description = re.sub(r'\s+', ' ', description).strip()
            
            info = f"ì œëª©: {title_text}\n"
            info += f"URL: {url}\n"
            info += f"ìƒíƒœ ì½”ë“œ: {response.status_code}\n"
            info += f"ì½˜í…ì¸  íƒ€ì…: {response.headers.get('content-type', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
            info += f"ì½˜í…ì¸  í¬ê¸°: {len(response.content)} bytes\n"
            info += f"ì„¤ëª…: {description}\n"
            
            self.info_text.insert(tk.END, info)
            
            # ë§í¬ ì¶”ì¶œ
            if self.extract_links.get():
                links = soup.find_all('a', href=True)
                self.links_text.insert(tk.END, f"ì´ {len(links)}ê°œì˜ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n\n")
                
                for i, link in enumerate(links[:50], 1):  # ìµœëŒ€ 50ê°œë§Œ í‘œì‹œ
                    href = link['href']
                    text = link.get_text(strip=True)
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ ì •ë¦¬
                    if text:
                        text = unicodedata.normalize('NFC', text)
                        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                        text = re.sub(r'\s+', ' ', text).strip()
                        # ë„ˆë¬´ ê¸´ í…ìŠ¤íŠ¸ëŠ” ì œí•œ
                        if len(text) > 100:
                            text = text[:100] + "..."
                    else:
                        text = "(í…ìŠ¤íŠ¸ ì—†ìŒ)"
                    
                    full_url = urljoin(url, href)
                    self.links_text.insert(tk.END, f"{i}. {text}\n   URL: {full_url}\n\n")
                
                if len(links) > 50:
                    self.links_text.insert(tk.END, f"... ê·¸ ì™¸ {len(links) - 50}ê°œ ë§í¬ê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            if self.extract_images.get():
                images = soup.find_all('img', src=True)
                self.images_text.insert(tk.END, f"ì´ {len(images)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n\n")
                
                for i, img in enumerate(images[:30], 1):  # ìµœëŒ€ 30ê°œë§Œ í‘œì‹œ
                    src = img['src']
                    alt = img.get('alt', 'ëŒ€ì²´ í…ìŠ¤íŠ¸ ì—†ìŒ')
                    full_url = urljoin(url, src)
                    self.images_text.insert(tk.END, f"{i}. {alt}\n   URL: {full_url}\n\n")
                
                if len(images) > 30:
                    self.images_text.insert(tk.END, f"... ê·¸ ì™¸ {len(images) - 30}ê°œ ì´ë¯¸ì§€ê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            if self.extract_text.get():
                # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                for script in soup(["script", "style"]):
                    script.decompose()
                
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                # í•œê¸€ í…ìŠ¤íŠ¸ ì •ë¦¬
                import unicodedata
                # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (í•œê¸€ í˜¸í™˜ì„± ë¬¸ì ì²˜ë¦¬)
                text = unicodedata.normalize('NFC', text)
                # ì œì–´ ë¬¸ì ì œê±°
                text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                text = re.sub(r'\s+', ' ', text).strip()
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                if len(text) > 5000:
                    text = text[:5000] + "\n\n... (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
                
                self.content_text.insert(tk.END, text)
            
            # í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€
            self.populate_table(url, soup)
            
            self.status_var.set(f"í¬ë¡¤ë§ ì™„ë£Œ - {title_text}")
            
        except Exception as e:
            self.show_error(f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
    
    def show_error(self, error_message):
        """ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        self.progress.stop()
        self.crawl_button.config(state='normal')
        self.stop_button.config(state='disabled')
        self.is_crawling = False
        self.status_var.set("ì˜¤ë¥˜ ë°œìƒ")
        messagebox.showerror("í¬ë¡¤ë§ ì˜¤ë¥˜", error_message)
    
    def populate_table(self, url, soup):
        """í…Œì´ë¸”ì— í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            # í˜ì´ì§€ ê¸°ë³¸ ì •ë³´
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"
            
            # í˜ì´ì§€ ì •ë³´ ì¶”ê°€
            page_data = {
                'type': 'í˜ì´ì§€',
                'title': title_text,
                'url': url,
                'description': 'ì›¹í˜ì´ì§€ ê¸°ë³¸ ì •ë³´'
            }
            self.tree.insert('', 'end', values=(page_data['type'], page_data['title'], 
                                              page_data['url'], page_data['description']))
            self.crawled_data.append(page_data)
            
            # ë§í¬ ì •ë³´ ì¶”ê°€
            if self.extract_links.get():
                links = soup.find_all('a', href=True)
                for i, link in enumerate(links[:50]):  # ìµœëŒ€ 50ê°œ
                    href = link['href']
                    text = link.get_text(strip=True)
                    
                    # ë§í¬ í…ìŠ¤íŠ¸ ì •ë¦¬
                    if text:
                        import unicodedata
                        text = unicodedata.normalize('NFC', text)
                        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
                        text = re.sub(r'\s+', ' ', text).strip()
                        # ê¸¸ì´ ì œí•œ
                        if len(text) > 100:
                            text = text[:100] + "..."
                    else:
                        text = '(í…ìŠ¤íŠ¸ ì—†ìŒ)'
                    
                    full_url = urljoin(url, href)
                    
                    link_data = {
                        'type': 'ë§í¬',
                        'title': text,
                        'url': full_url,
                        'description': f'ë§í¬ #{i+1}'
                    }
                    self.tree.insert('', 'end', values=(link_data['type'], link_data['title'], 
                                                      link_data['url'], link_data['description']))
                    self.crawled_data.append(link_data)
            
            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            if self.extract_images.get():
                images = soup.find_all('img', src=True)
                for i, img in enumerate(images[:30]):  # ìµœëŒ€ 30ê°œ
                    src = img['src']
                    alt = img.get('alt', 'ëŒ€ì²´ í…ìŠ¤íŠ¸ ì—†ìŒ')[:100]
                    full_url = urljoin(url, src)
                    
                    img_data = {
                        'type': 'ì´ë¯¸ì§€',
                        'title': alt,
                        'url': full_url,
                        'description': f'ì´ë¯¸ì§€ #{i+1}'
                    }
                    self.tree.insert('', 'end', values=(img_data['type'], img_data['title'], 
                                                      img_data['url'], img_data['description']))
                    self.crawled_data.append(img_data)
                    
        except Exception as e:
            print(f"í…Œì´ë¸” ì±„ìš°ê¸° ì˜¤ë¥˜: {str(e)}")
    
    def export_to_excel(self):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.crawled_data:
            messagebox.showwarning("ê²½ê³ ", "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # íŒŒì¼ ì €ì¥ ëŒ€í™”ìƒì - í•œê¸€ íŒŒì¼ëª… ì§€ì›
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            default_filename = f"í¬ë¡¤ë§ê²°ê³¼_{timestamp}.xlsx"  # í•œê¸€ íŒŒì¼ëª… ì‚¬ìš©
            
            file_path = filedialog.asksaveasfilename(
                defaultextension=".xlsx",
                filetypes=[("Excel íŒŒì¼", "*.xlsx"), ("ëª¨ë“  íŒŒì¼", "*.*")],
                initialvalue=default_filename,
                title="í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥"
            )
            
            if not file_path:
                return
            
            # í•œê¸€ ê²½ë¡œ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì •ê·œí™”
            file_path = os.path.normpath(file_path)
            
            # DataFrame ìƒì„± - í•œê¸€ ë°ì´í„° ì²˜ë¦¬ ê°œì„ 
            processed_data = []
            for item in self.crawled_data:
                processed_item = {}
                for key, value in item.items():
                    # í…ìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬ ë° ì¸ì½”ë”© ì²˜ë¦¬
                    if isinstance(value, str):
                        # íŠ¹ìˆ˜ë¬¸ì ë° ì œì–´ë¬¸ì ì œê±°
                        value = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', value)
                        # ì—°ì†ëœ ê³µë°± ì •ë¦¬
                        value = re.sub(r'\s+', ' ', value).strip()
                        # Excel ì…€ ê¸¸ì´ ì œí•œ (32767ì)
                        if len(value) > 32000:
                            value = value[:32000] + "..."
                    processed_item[key] = value
                processed_data.append(processed_item)
            
            df = pd.DataFrame(processed_data)
            df.columns = ['íƒ€ì…', 'ì œëª©/í…ìŠ¤íŠ¸', 'URL', 'ì„¤ëª…']
            
            # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ - í•œê¸€ ì§€ì› ê°•í™”
            with pd.ExcelWriter(
                file_path, 
                engine='openpyxl',
                options={'encoding': 'utf-8'}
            ) as writer:
                # ì‹œíŠ¸ëª…ì„ ì˜ë¬¸ìœ¼ë¡œ ë³€ê²½ (ì¼ë¶€ Excel ë²„ì „ í˜¸í™˜ì„±)
                sheet_name = 'CrawlingResults'
                df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ì„œì‹ ì„¤ì •
                worksheet = writer.sheets[sheet_name]
                
                # í—¤ë” ìŠ¤íƒ€ì¼ ì„¤ì •
                from openpyxl.styles import Font, PatternFill, Alignment
                header_font = Font(bold=True, color="FFFFFF")
                header_fill = PatternFill(start_color="366092", end_color="366092", fill_type="solid")
                
                # í—¤ë” í–‰ ìŠ¤íƒ€ì¼ ì ìš©
                for cell in worksheet[1]:
                    cell.font = header_font
                    cell.fill = header_fill
                    cell.alignment = Alignment(horizontal="center", vertical="center")
                
                # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì • - í•œê¸€ í…ìŠ¤íŠ¸ ê³ ë ¤
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if cell.value:
                                # í•œê¸€ í…ìŠ¤íŠ¸ì˜ ì‹¤ì œ í‘œì‹œ ê¸¸ì´ ê³„ì‚°
                                text = str(cell.value)
                                # í•œê¸€ì€ 2ë°°, ì˜ë¬¸ì€ 1ë°°ë¡œ ê³„ì‚°
                                display_length = sum(2 if ord(char) > 127 else 1 for char in text)
                                if display_length > max_length:
                                    max_length = display_length
                        except:
                            pass
                    
                    # ìµœì†Œ 8, ìµœëŒ€ 60ìœ¼ë¡œ ì œí•œ
                    adjusted_width = min(max(max_length * 0.8, 8), 60)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
                
                # í–‰ ë†’ì´ ìë™ ì¡°ì •
                for row in worksheet.iter_rows():
                    max_lines = 1
                    for cell in row:
                        if cell.value and isinstance(cell.value, str):
                            lines = cell.value.count('\n') + 1
                            max_lines = max(max_lines, lines)
                    if max_lines > 1:
                        worksheet.row_dimensions[row[0].row].height = max_lines * 15
            
            # ì €ì¥ ì™„ë£Œ ë©”ì‹œì§€ - íŒŒì¼ í¬ê¸° ì •ë³´ ì¶”ê°€
            file_size = os.path.getsize(file_path)
            size_str = f"{file_size:,} bytes"
            if file_size > 1024:
                size_str = f"{file_size/1024:.1f} KB"
            if file_size > 1024*1024:
                size_str = f"{file_size/(1024*1024):.1f} MB"
            
            messagebox.showinfo(
                "ì €ì¥ ì™„ë£Œ", 
                f"í¬ë¡¤ë§ ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                f"íŒŒì¼: {os.path.basename(file_path)}\n"
                f"ìœ„ì¹˜: {os.path.dirname(file_path)}\n"
                f"í¬ê¸°: {size_str}\n"
                f"ë°ì´í„° ìˆ˜: {len(self.crawled_data)}ê°œ"
            )
            
        except UnicodeEncodeError as e:
            messagebox.showerror("ì¸ì½”ë”© ì˜¤ë¥˜", 
                f"íŒŒì¼ ì €ì¥ ì¤‘ ì¸ì½”ë”© ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
                f"ì¼ë¶€ íŠ¹ìˆ˜ë¬¸ìê°€ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                f"ì˜¤ë¥˜ ìƒì„¸: {str(e)}")
        except PermissionError:
            messagebox.showerror("ê¶Œí•œ ì˜¤ë¥˜", 
                f"íŒŒì¼ì„ ì €ì¥í•  ê¶Œí•œì´ ì—†ê±°ë‚˜ íŒŒì¼ì´ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤.\n"
                f"ë‹¤ë¥¸ ìœ„ì¹˜ì— ì €ì¥í•˜ê±°ë‚˜ íŒŒì¼ì„ ë‹«ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        except Exception as e:
            messagebox.showerror("ì €ì¥ ì˜¤ë¥˜", 
                f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n\n"
                f"ì˜¤ë¥˜ ë‚´ìš©: {str(e)}\n\n"
                f"í•´ê²° ë°©ë²•:\n"
                f"1. ë‹¤ë¥¸ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥í•´ë³´ì„¸ìš”\n"
                f"2. ë‹¤ë¥¸ í´ë”ì— ì €ì¥í•´ë³´ì„¸ìš”\n"
                f"3. í”„ë¡œê·¸ë¨ì„ ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”")
    
    def setup_selenium_driver(self):
        """Selenium WebDriverë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        try:
            chrome_options = Options()
            chrome_options.add_argument("--headless")  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--window-size=1920,1080")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-logging"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            
            # User-Agent ì„¤ì •
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            return True
        except Exception as e:
            self.root.after(0, self.show_error, f"Selenium ì„¤ì • ì˜¤ë¥˜: {str(e)}")
            return False
    
    def crawl_with_selenium(self, url):
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not self.setup_selenium_driver():
            return
        
        try:
            max_pages = int(self.max_pages.get()) if self.max_pages.get().isdigit() else 1
            delay = float(self.crawl_delay.get()) if self.crawl_delay.get().replace('.', '').isdigit() else 1
            
            for page in range(1, max_pages + 1):
                if not self.is_crawling:
                    break
                
                self.current_page = page
                self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p}/{max_pages} í¬ë¡¤ë§ ì¤‘..."))
                
                # í˜ì´ì§€ë³„ URL ìƒì„±
                page_url = self.generate_page_url(url, page)
                
                # ì¬ì‹œë„ ë¡œì§
                success = False
                for retry in range(self.max_retries):
                    try:
                        self.driver.get(page_url)
                        WebDriverWait(self.driver, 10).until(
                            EC.presence_of_element_located((By.TAG_NAME, "body"))
                        )
                        
                        # ì‚¬ì´íŠ¸ë³„ íŠ¹í™” í¬ë¡¤ë§
                        if self.site_type.get() == "ë„¤ì´ë²„ ì‡¼í•‘":
                            self.crawl_naver_shopping()
                        elif self.site_type.get() == "ì¸ìŠ¤íƒ€ê·¸ë¨":
                            self.crawl_instagram()
                        elif self.site_type.get() == "ë¶€ë™ì‚°":
                            self.crawl_real_estate()
                        else:
                            self.crawl_general_selenium()
                        
                        success = True
                        break
                        
                    except TimeoutException:
                        self.root.after(0, lambda r=retry+1: self.status_var.set(f"í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„ {r}/{self.max_retries}"))
                        time.sleep(2)
                    except Exception as e:
                        self.root.after(0, lambda r=retry+1, err=str(e): self.status_var.set(f"ì˜¤ë¥˜ ë°œìƒ, ì¬ì‹œë„ {r}/{self.max_retries}: {err[:50]}"))
                        time.sleep(2)
                
                if not success:
                    self.root.after(0, lambda p=page: self.status_var.set(f"í˜ì´ì§€ {p} í¬ë¡¤ë§ ì‹¤íŒ¨"))
                
                # í¬ë¡¤ë§ ê°„ê²©
                if page < max_pages and self.is_crawling:
                    time.sleep(delay)
            
            self.root.after(0, self.finalize_crawling)
            
        except Exception as e:
            self.root.after(0, self.show_error, f"Selenium í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
        finally:
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
                self.driver = None
    
    def generate_page_url(self, base_url, page):
        """í˜ì´ì§€ ë²ˆí˜¸ì— ë”°ë¥¸ URLì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if page == 1:
            return base_url
        
        # ì¼ë°˜ì ì¸ í˜ì´ì§€ë„¤ì´ì…˜ íŒ¨í„´ë“¤
        if "?" in base_url:
            return f"{base_url}&page={page}"
        else:
            return f"{base_url}?page={page}"
    
    def crawl_naver_shopping(self):
        """ë„¤ì´ë²„ ì‡¼í•‘ í¬ë¡¤ë§"""
        try:
            # ìƒí’ˆ ëª©ë¡ ìš”ì†Œ ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".basicList_list__2YY7H, .product_list, .list_basis"))
            )
            
            # ìƒí’ˆ ìš”ì†Œë“¤ ì°¾ê¸°
            products = self.driver.find_elements(By.CSS_SELECTOR, ".basicList_item__1MBN3, .product_item, .item")
            
            for product in products:
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ë„¤ì´ë²„ì‡¼í•‘'}
                    
                    # ì œëª© ì¶”ì¶œ
                    if self.extract_title.get():
                        title_elem = product.find_element(By.CSS_SELECTOR, ".basicList_title__3P9Q7, .product_title, .item_title")
                        data['title'] = title_elem.text.strip()
                    
                    # ê°€ê²© ì¶”ì¶œ
                    if self.extract_price.get():
                        price_elem = product.find_element(By.CSS_SELECTOR, ".price_num__2WUXn, .product_price, .item_price")
                        data['price'] = price_elem.text.strip()
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = product.find_element(By.CSS_SELECTOR, "a")
                    data['url'] = link_elem.get_attribute('href')
                    
                    # ì´ë¯¸ì§€ ì¶”ì¶œ
                    if self.extract_images.get():
                        img_elem = product.find_element(By.CSS_SELECTOR, "img")
                        data['image_url'] = img_elem.get_attribute('src')
                    
                    data['description'] = f"ë„¤ì´ë²„ì‡¼í•‘ ìƒí’ˆ (í˜ì´ì§€ {self.current_page})"
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except NoSuchElementException:
                    continue
                    
        except TimeoutException:
            self.root.after(0, lambda: self.status_var.set("ë„¤ì´ë²„ ì‡¼í•‘ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"))
    
    def crawl_instagram(self):
        """ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ë¡¤ë§ (ì œí•œì )"""
        try:
            # ì¸ìŠ¤íƒ€ê·¸ë¨ì€ ë¡œê·¸ì¸ì´ í•„ìš”í•˜ë¯€ë¡œ ê¸°ë³¸ì ì¸ ë©”íƒ€ë°ì´í„°ë§Œ ì¶”ì¶œ
            posts = self.driver.find_elements(By.CSS_SELECTOR, "article, ._aagu, .v1Nh3")
            
            for i, post in enumerate(posts[:10]):  # ìµœëŒ€ 10ê°œë§Œ
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ì¸ìŠ¤íƒ€ê·¸ë¨'}
                    
                    # ê¸°ë³¸ ì •ë³´
                    data['title'] = f"Instagram Post {i+1}"
                    data['url'] = self.driver.current_url
                    data['description'] = f"ì¸ìŠ¤íƒ€ê·¸ë¨ ê²Œì‹œë¬¼ (í˜ì´ì§€ {self.current_page})"
                    
                    # ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œë„
                    if self.extract_images.get():
                        try:
                            img_elem = post.find_element(By.CSS_SELECTOR, "img")
                            data['image_url'] = img_elem.get_attribute('src')
                        except:
                            pass
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except:
                    continue
                    
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ì¸ìŠ¤íƒ€ê·¸ë¨ í¬ë¡¤ë§ ì œí•œ: {err[:50]}"))
    
    def crawl_real_estate(self):
        """ë¶€ë™ì‚° ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        try:
            # ë¶€ë™ì‚° ë§¤ë¬¼ ìš”ì†Œë“¤ ì°¾ê¸°
            properties = self.driver.find_elements(By.CSS_SELECTOR, ".item, .list-item, .property-item, .estate-item")
            
            for prop in properties:
                if not self.is_crawling:
                    break
                
                try:
                    data = {'type': 'ë¶€ë™ì‚°'}
                    
                    # ì œëª© (ì£¼ì†Œ/ë§¤ë¬¼ëª…)
                    if self.extract_title.get():
                        title_selectors = [".item-title", ".property-title", ".title", "h3", "h4"]
                        for selector in title_selectors:
                            try:
                                title_elem = prop.find_element(By.CSS_SELECTOR, selector)
                                data['title'] = title_elem.text.strip()
                                break
                            except:
                                continue
                    
                    # ê°€ê²© ì¶”ì¶œ
                    if self.extract_price.get():
                        price_selectors = [".price", ".cost", ".amount", ".fee"]
                        for selector in price_selectors:
                            try:
                                price_elem = prop.find_element(By.CSS_SELECTOR, selector)
                                data['price'] = price_elem.text.strip()
                                break
                            except:
                                continue
                    
                    # ë§í¬ ì¶”ì¶œ
                    try:
                        link_elem = prop.find_element(By.CSS_SELECTOR, "a")
                        data['url'] = link_elem.get_attribute('href')
                    except:
                        data['url'] = self.driver.current_url
                    
                    data['description'] = f"ë¶€ë™ì‚° ë§¤ë¬¼ (í˜ì´ì§€ {self.current_page})"
                    
                    self.crawled_data.append(data)
                    self.root.after(0, self.add_to_table, data)
                    
                except:
                    continue
                    
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ë¶€ë™ì‚° í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))
    
    def crawl_general_selenium(self):
        """ì¼ë°˜ì ì¸ Selenium í¬ë¡¤ë§"""
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ê¸°ì¡´ populate_table ë©”ì„œë“œ í™œìš©
            self.root.after(0, self.populate_table, self.driver.current_url, soup)
            
        except Exception as e:
            self.root.after(0, lambda err=str(e): self.status_var.set(f"ì¼ë°˜ í¬ë¡¤ë§ ì˜¤ë¥˜: {err[:50]}"))
    
    def add_to_table(self, data):
        """í…Œì´ë¸”ì— ê°œë³„ ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            self.tree.insert('', 'end', values=(
                data.get('type', ''),
                data.get('title', '')[:100],
                data.get('url', ''),
                data.get('description', '')
            ))
        except Exception as e:
            print(f"í…Œì´ë¸” ì¶”ê°€ ì˜¤ë¥˜: {str(e)}")
    
    def finalize_crawling(self):
        """í¬ë¡¤ë§ ì™„ë£Œ ì²˜ë¦¬"""
        self.progress.stop()
        self.crawl_button.config(state='normal')
        self.stop_button.config(state='disabled')
        self.export_button.config(state='normal')
        self.is_crawling = False
        self.status_var.set(f"í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(self.crawled_data)}ê°œ ë°ì´í„° ìˆ˜ì§‘")

def main():
    root = tk.Tk()
    app = WebCrawlerApp(root)
    root.mainloop()

if __name__ == "__main__":
    main() 