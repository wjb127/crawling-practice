import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox, filedialog
import requests
from bs4 import BeautifulSoup
import threading
from urllib.parse import urljoin, urlparse
import pandas as pd
from datetime import datetime
import os

class WebCrawlerApp:
    def __init__(self, root):
        self.root = root
        self.root.title("ì›¹ í¬ë¡¤ëŸ¬")
        self.root.geometry("800x600")
        
        # ë©”ì¸ í”„ë ˆì„
        main_frame = ttk.Frame(root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # URL ì…ë ¥ ì„¹ì…˜
        url_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ì„¤ì •", padding="5")
        url_frame.grid(row=0, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        ttk.Label(url_frame, text="URL:").grid(row=0, column=0, sticky=tk.W, padx=(0, 5))
        self.url_var = tk.StringVar(value="https://example.com")
        self.url_entry = ttk.Entry(url_frame, textvariable=self.url_var, width=60)
        self.url_entry.grid(row=0, column=1, sticky=(tk.W, tk.E), padx=(0, 5))
        
        self.crawl_button = ttk.Button(url_frame, text="í¬ë¡¤ë§ ì‹œì‘", command=self.start_crawling)
        self.crawl_button.grid(row=0, column=2, padx=(5, 0))
        
        self.stop_button = ttk.Button(url_frame, text="ì¤‘ì§€", command=self.stop_crawling, state='disabled')
        self.stop_button.grid(row=0, column=3, padx=(5, 0))
        
        # í¬ë¡¤ë§ ì˜µì…˜
        options_frame = ttk.Frame(url_frame)
        options_frame.grid(row=1, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=(5, 0))
        
        self.extract_links = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="ë§í¬ ì¶”ì¶œ", variable=self.extract_links).grid(row=0, column=0, sticky=tk.W)
        
        self.extract_images = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="ì´ë¯¸ì§€ ì¶”ì¶œ", variable=self.extract_images).grid(row=0, column=1, sticky=tk.W, padx=(10, 0))
        
        self.extract_text = tk.BooleanVar(value=True)
        ttk.Checkbutton(options_frame, text="í…ìŠ¤íŠ¸ ì¶”ì¶œ", variable=self.extract_text).grid(row=0, column=2, sticky=tk.W, padx=(10, 0))
        
        # ì§„í–‰ìƒí™© í‘œì‹œ
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress.grid(row=1, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        # ì—‘ì…€ ì €ì¥ ë²„íŠ¼
        export_frame = ttk.Frame(main_frame)
        export_frame.grid(row=2, column=0, columnspan=2, sticky=(tk.W, tk.E), pady=(0, 10))
        
        self.export_button = ttk.Button(export_frame, text="ì—‘ì…€ë¡œ ì €ì¥", command=self.export_to_excel, state='disabled')
        self.export_button.pack(side=tk.RIGHT)
        
        ttk.Label(export_frame, text="í¬ë¡¤ë§ì´ ì™„ë£Œë˜ë©´ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.").pack(side=tk.LEFT)
        
        # ê²°ê³¼ í‘œì‹œ ì˜ì—­
        result_frame = ttk.LabelFrame(main_frame, text="í¬ë¡¤ë§ ê²°ê³¼", padding="5")
        result_frame.grid(row=3, column=0, columnspan=2, sticky=(tk.W, tk.E, tk.N, tk.S), pady=(0, 10))
        
        # íƒ­ ìœ„ì ¯
        self.notebook = ttk.Notebook(result_frame)
        self.notebook.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í˜ì´ì§€ ì •ë³´ íƒ­
        self.info_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.info_frame, text="í˜ì´ì§€ ì •ë³´")
        
        self.info_text = scrolledtext.ScrolledText(self.info_frame, height=8, wrap=tk.WORD)
        self.info_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ë§í¬ íƒ­
        self.links_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.links_frame, text="ë§í¬")
        
        self.links_text = scrolledtext.ScrolledText(self.links_frame, height=8, wrap=tk.WORD)
        self.links_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ì´ë¯¸ì§€ íƒ­
        self.images_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.images_frame, text="ì´ë¯¸ì§€")
        
        self.images_text = scrolledtext.ScrolledText(self.images_frame, height=8, wrap=tk.WORD)
        self.images_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í…ìŠ¤íŠ¸ íƒ­
        self.content_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.content_frame, text="í…ìŠ¤íŠ¸ ë‚´ìš©")
        
        self.content_text = scrolledtext.ScrolledText(self.content_frame, height=8, wrap=tk.WORD)
        self.content_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ê²°ê³¼ í…Œì´ë¸” íƒ­
        self.table_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.table_frame, text="ê²°ê³¼ í…Œì´ë¸”")
        
        # í…Œì´ë¸” ë·° ìƒì„±
        self.create_table_view()
        
        # ì‚¬ì´íŠ¸ ì¶”ì²œ íƒ­
        self.recommend_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.recommend_frame, text="ì‚¬ì´íŠ¸ ì¶”ì²œ")
        
        self.recommend_text = scrolledtext.ScrolledText(self.recommend_frame, height=8, wrap=tk.WORD)
        self.recommend_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ê¸°ìˆ ìŠ¤íƒ ì„¤ëª… íƒ­
        self.tech_frame = ttk.Frame(self.notebook)
        self.notebook.add(self.tech_frame, text="ê¸°ìˆ ìŠ¤íƒ ê°€ì´ë“œ")
        
        self.tech_text = scrolledtext.ScrolledText(self.tech_frame, height=8, wrap=tk.WORD)
        self.tech_text.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # ìƒíƒœ í‘œì‹œì¤„
        self.status_var = tk.StringVar(value="ì¤€ë¹„")
        status_bar = ttk.Label(main_frame, textvariable=self.status_var, relief=tk.SUNKEN)
        status_bar.grid(row=4, column=0, columnspan=2, sticky=(tk.W, tk.E))
        
        # ê·¸ë¦¬ë“œ ê°€ì¤‘ì¹˜ ì„¤ì •
        root.columnconfigure(0, weight=1)
        root.rowconfigure(0, weight=1)
        main_frame.columnconfigure(0, weight=1)
        main_frame.rowconfigure(3, weight=1)
        result_frame.columnconfigure(0, weight=1)
        result_frame.rowconfigure(0, weight=1)
        
        # ê° íƒ­ì˜ ê·¸ë¦¬ë“œ ì„¤ì •
        for frame in [self.info_frame, self.links_frame, self.images_frame, self.content_frame, 
                      self.table_frame, self.recommend_frame, self.tech_frame]:
            frame.columnconfigure(0, weight=1)
            frame.rowconfigure(0, weight=1)
        
        url_frame.columnconfigure(1, weight=1)
        
        # í¬ë¡¤ë§ ìƒíƒœ ë° ë°ì´í„° ì €ì¥ìš© ë³€ìˆ˜
        self.is_crawling = False
        self.crawled_data = []
        
        # íƒ­ ë‚´ìš© ì´ˆê¸°í™”
        self.load_static_content()
    
    def create_table_view(self):
        """ê²°ê³¼ í…Œì´ë¸” ë·°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # í…Œì´ë¸” í”„ë ˆì„
        table_container = ttk.Frame(self.table_frame)
        table_container.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # í…Œì´ë¸” (Treeview) ìƒì„±
        columns = ('Type', 'Title', 'URL', 'Description')
        self.tree = ttk.Treeview(table_container, columns=columns, show='headings', height=15)
        
        # ì»¬ëŸ¼ í—¤ë” ì„¤ì •
        self.tree.heading('Type', text='íƒ€ì…')
        self.tree.heading('Title', text='ì œëª©/í…ìŠ¤íŠ¸')
        self.tree.heading('URL', text='URL')
        self.tree.heading('Description', text='ì„¤ëª…')
        
        # ì»¬ëŸ¼ ë„ˆë¹„ ì„¤ì •
        self.tree.column('Type', width=80, minwidth=60)
        self.tree.column('Title', width=200, minwidth=150)
        self.tree.column('URL', width=300, minwidth=200)
        self.tree.column('Description', width=150, minwidth=100)
        
        # ìŠ¤í¬ë¡¤ë°” ì¶”ê°€
        v_scrollbar = ttk.Scrollbar(table_container, orient=tk.VERTICAL, command=self.tree.yview)
        h_scrollbar = ttk.Scrollbar(table_container, orient=tk.HORIZONTAL, command=self.tree.xview)
        self.tree.configure(yscrollcommand=v_scrollbar.set, xscrollcommand=h_scrollbar.set)
        
        # ìœ„ì ¯ ë°°ì¹˜
        self.tree.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        v_scrollbar.grid(row=0, column=1, sticky=(tk.N, tk.S))
        h_scrollbar.grid(row=1, column=0, sticky=(tk.W, tk.E))
        
        # ê·¸ë¦¬ë“œ ê°€ì¤‘ì¹˜ ì„¤ì •
        table_container.columnconfigure(0, weight=1)
        table_container.rowconfigure(0, weight=1)
    
    def stop_crawling(self):
        """í¬ë¡¤ë§ì„ ì¤‘ì§€í•©ë‹ˆë‹¤."""
        self.is_crawling = False
        self.status_var.set("í¬ë¡¤ë§ ì¤‘ì§€ë¨")
    
    def start_crawling(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤."""
        url = self.url_var.get().strip()
        if not url:
            messagebox.showerror("ì˜¤ë¥˜", "URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
            self.url_var.set(url)
        
        # UI ìƒíƒœ ë³€ê²½
        self.crawl_button.config(state='disabled')
        self.stop_button.config(state='normal')
        self.export_button.config(state='disabled')
        self.progress.start()
        self.status_var.set("í¬ë¡¤ë§ ì¤‘...")
        self.is_crawling = True
        
        # ê²°ê³¼ ì˜ì—­ ì´ˆê¸°í™”
        self.clear_results()
        self.crawled_data = []
        
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
        thread = threading.Thread(target=self.crawl_website, args=(url,))
        thread.daemon = True
        thread.start()
    
    def clear_results(self):
        """ê²°ê³¼ ì˜ì—­ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        self.info_text.delete(1.0, tk.END)
        self.links_text.delete(1.0, tk.END)
        self.images_text.delete(1.0, tk.END)
        self.content_text.delete(1.0, tk.END)
        
        # í…Œì´ë¸” ì´ˆê¸°í™”
        for item in self.tree.get_children():
            self.tree.delete(item)
    
    def load_static_content(self):
        """ì •ì ì¸ íƒ­ ë‚´ìš©ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        # ì‚¬ì´íŠ¸ ì¶”ì²œ ë‚´ìš©
        recommend_content = """
ğŸŒ í¬ë¡¤ë§í•˜ê¸° ì‰¬ìš´ ì¶”ì²œ ì‚¬ì´íŠ¸ë“¤

ğŸ“° ë‰´ìŠ¤ ì‚¬ì´íŠ¸
â€¢ https://news.ycombinator.com - í•´ì»¤ë‰´ìŠ¤ (ê°„ë‹¨í•œ HTML êµ¬ì¡°)
â€¢ https://quotes.toscrape.com - í¬ë¡¤ë§ ì—°ìŠµìš© ì‚¬ì´íŠ¸
â€¢ https://books.toscrape.com - ë„ì„œ ì •ë³´ ì—°ìŠµ ì‚¬ì´íŠ¸

ğŸ›ï¸ ê³µê³µ ë°ì´í„°
â€¢ https://httpbin.org - HTTP í…ŒìŠ¤íŠ¸ìš© ì‚¬ì´íŠ¸
â€¢ https://jsonplaceholder.typicode.com - JSON API í…ŒìŠ¤íŠ¸
â€¢ https://reqres.in - API í…ŒìŠ¤íŠ¸ìš©

ğŸ“Š ì—°ìŠµìš© ì‚¬ì´íŠ¸
â€¢ https://scrape.center - í¬ë¡¤ë§ ì—°ìŠµ ì „ìš©
â€¢ https://webscraper.io/test-sites - ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
â€¢ https://example.com - ê°€ì¥ ê¸°ë³¸ì ì¸ í…ŒìŠ¤íŠ¸

ğŸ›’ ì „ììƒê±°ë˜ (ì£¼ì˜ í•„ìš”)
â€¢ ì¼ë¶€ ì „ììƒê±°ë˜ ì‚¬ì´íŠ¸ë“¤ (robots.txt ì¤€ìˆ˜ í•„ìˆ˜)
â€¢ ì˜¤í”ˆ ë§ˆì¼“ ìƒí’ˆ ì •ë³´ (ì´ìš©ì•½ê´€ í™•ì¸ í•„ìš”)

âš ï¸ ì£¼ì˜ì‚¬í•­
â€¢ robots.txt íŒŒì¼ì„ í•­ìƒ í™•ì¸í•˜ì„¸ìš”
â€¢ ê³¼ë„í•œ ìš”ì²­ì€ í”¼í•˜ì„¸ìš” (ì„œë²„ ë¶€í•˜ ë°©ì§€)
â€¢ ì €ì‘ê¶Œì´ ìˆëŠ” ì½˜í…ì¸ ëŠ” ì£¼ì˜í•˜ì„¸ìš”
â€¢ ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„°ëŠ” ìˆ˜ì§‘í•˜ì§€ ë§ˆì„¸ìš”
â€¢ ì›¹ì‚¬ì´íŠ¸ì˜ ì´ìš©ì•½ê´€ì„ ë°˜ë“œì‹œ í™•ì¸í•˜ì„¸ìš”

ğŸ¯ ì´ˆë³´ì ì¶”ì²œ ìˆœì„œ
1. quotes.toscrape.com (ê¸°ë³¸ ì—°ìŠµ)
2. books.toscrape.com (í˜ì´ì§€ë„¤ì´ì…˜ ì—°ìŠµ)
3. news.ycombinator.com (ì‹¤ì œ ì‚¬ì´íŠ¸ ì—°ìŠµ)
4. ì›í•˜ëŠ” ì‚¬ì´íŠ¸ (ë‹¨ê³„ì  ì ‘ê·¼)
"""
        
        # ê¸°ìˆ ìŠ¤íƒ ê°€ì´ë“œ ë‚´ìš©
        tech_content = """
ğŸŒ ì‚¬ì´íŠ¸ ê¸°ìˆ ìŠ¤íƒë³„ í¬ë¡¤ë§ ë‚œì´ë„ ê°€ì´ë“œ

ğŸ“Š í¬ë¡¤ë§ ë‚œì´ë„ ë¶„ë¥˜

ğŸŸ¢ ì‰¬ì›€ (ì •ì  ì‚¬ì´íŠ¸)
â€¢ ì¼ë°˜ HTML/CSS ì‚¬ì´íŠ¸
â€¢ ì„œë²„ ì‚¬ì´ë“œ ë Œë”ë§ (SSR)
â€¢ WordPress, Jekyll ë“± ì •ì  ì‚¬ì´íŠ¸ ìƒì„±ê¸°

í¬ë¡¤ë§ ë°©ë²•:
â†’ requests + BeautifulSoupë¡œ ì¶©ë¶„
â†’ í˜ì´ì§€ ì†ŒìŠ¤ì— ëª¨ë“  ë°ì´í„°ê°€ í¬í•¨ë¨
â†’ ì˜ˆ: ë¸”ë¡œê·¸, ë‰´ìŠ¤ ì‚¬ì´íŠ¸, ê¸°ì—… í™ˆí˜ì´ì§€

ğŸŸ¡ ë³´í†µ (í•˜ì´ë¸Œë¦¬ë“œ)
â€¢ jQuery ê¸°ë°˜ ì‚¬ì´íŠ¸
â€¢ ì¼ë¶€ ë™ì  ë¡œë”©ì´ ìˆëŠ” ì‚¬ì´íŠ¸
â€¢ AJAXë¡œ ì¼ë¶€ ì½˜í…ì¸  ë¡œë“œ

í¬ë¡¤ë§ ë°©ë²•:
â†’ requests + BeautifulSoup (ê¸°ë³¸ ì½˜í…ì¸ )
â†’ API ì—”ë“œí¬ì¸íŠ¸ ë¶„ì„ í›„ ì§ì ‘ í˜¸ì¶œ
â†’ í•„ìš”ì‹œ Seleniumìœ¼ë¡œ ë™ì  ë¶€ë¶„ë§Œ ì²˜ë¦¬

ğŸŸ  ì–´ë ¤ì›€ (SPA - Single Page Application)
â€¢ React, Vue.js, Angular ê¸°ë°˜
â€¢ í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ë Œë”ë§ (CSR)
â€¢ ëŒ€ë¶€ë¶„ì˜ ì½˜í…ì¸ ê°€ JavaScriptë¡œ ìƒì„±

í¬ë¡¤ë§ ë°©ë²•:
â†’ Selenium ë˜ëŠ” Playwright í•„ìˆ˜
â†’ í˜ì´ì§€ ë¡œë”© ì™„ë£Œê¹Œì§€ ëŒ€ê¸° í•„ìš”
â†’ ì˜ˆ: ëª¨ë˜ ì „ììƒê±°ë˜, ì†Œì…œë¯¸ë””ì–´

ğŸ”´ ë§¤ìš° ì–´ë ¤ì›€ (ê³ ë„í™”ëœ SPA)
â€¢ Next.js, Nuxt.js (SSR + CSR í˜¼í•©)
â€¢ ë³µì¡í•œ ìƒíƒœ ê´€ë¦¬ (Redux, Vuex)
â€¢ ì§€ì—° ë¡œë”©, ë¬´í•œ ìŠ¤í¬ë¡¤
â€¢ ê°•ë ¥í•œ ë´‡ ì°¨ë‹¨ ì‹œìŠ¤í…œ

í¬ë¡¤ë§ ë°©ë²•:
â†’ Playwright + ê³ ê¸‰ ê¸°ë²•
â†’ í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € + í”„ë¡ì‹œ ë¡œí…Œì´ì…˜
â†’ API ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§
â†’ ì˜ˆ: Netflix, Instagram, LinkedIn

ğŸ› ï¸ ê¸°ìˆ ìŠ¤íƒë³„ ìƒì„¸ ë¶„ì„

ğŸ“„ ì •ì  HTML ì‚¬ì´íŠ¸
íŠ¹ì§•: ì„œë²„ì—ì„œ ì™„ì „í•œ HTML ë°˜í™˜
ë„êµ¬: requests + BeautifulSoup
ì˜ˆì‹œ: ì •ë¶€ê¸°ê´€, í•™êµ, ì „í†µì  ê¸°ì—… ì‚¬ì´íŠ¸

ğŸ¯ WordPress/Drupal ì‚¬ì´íŠ¸
íŠ¹ì§•: PHP ê¸°ë°˜, ëŒ€ë¶€ë¶„ ì„œë²„ ë Œë”ë§
ë„êµ¬: requests + BeautifulSoup
ì£¼ì˜: í”ŒëŸ¬ê·¸ì¸ì— ë”°ë¼ ë™ì  ìš”ì†Œ ìˆì„ ìˆ˜ ìˆìŒ

âš¡ jQuery ê¸°ë°˜ ì‚¬ì´íŠ¸
íŠ¹ì§•: í˜ì´ì§€ ë¡œë“œ í›„ ì¼ë¶€ ì½˜í…ì¸  ì¶”ê°€ ë¡œë“œ
ë„êµ¬: requests (ê¸°ë³¸) + Selenium (ë™ì  ë¶€ë¶„)
ì „ëµ: Network íƒ­ì—ì„œ AJAX ìš”ì²­ ë¶„ì„

ğŸ”¥ React/Vue/Angular SPA
íŠ¹ì§•: ë¹ˆ HTML + JavaScriptë¡œ ëª¨ë“  ì½˜í…ì¸  ìƒì„±
ë„êµ¬: Selenium/Playwright í•„ìˆ˜
ì „ëµ: 
- componentDidMount ëŒ€ê¸°
- API í˜¸ì¶œ ì§ì ‘ ë¶„ì„ (ë” íš¨ìœ¨ì )
- SSR ë²„ì „ ì°¾ê¸° (SEOìš© í˜ì´ì§€)

ğŸª ì „ììƒê±°ë˜ í”Œë«í¼ë³„
â€¢ Shopify: Liquid í…œí”Œë¦¿, ëŒ€ë¶€ë¶„ ì„œë²„ ë Œë”ë§
â€¢ WooCommerce: WordPress ê¸°ë°˜, ë¹„êµì  ì‰¬ì›€
â€¢ Magento: ë³µì¡í•œ êµ¬ì¡°, ì¼ë¶€ AJAX
â€¢ ì»¤ìŠ¤í…€ React: ë§¤ìš° ì–´ë ¤ì›€

ğŸ“± ëª¨ë°”ì¼ ì•± ê¸°ë°˜ ì›¹
íŠ¹ì§•: ì•±ê³¼ ë™ì¼í•œ API ì‚¬ìš©, ê³ ë„í™”ëœ ë³´ì•ˆ
ë„êµ¬: API ë¦¬ë²„ìŠ¤ ì—”ì§€ë‹ˆì–´ë§ + requests
ì „ëµ: ëª¨ë°”ì¼ ì•± íŒ¨í‚· ë¶„ì„

ğŸ›¡ï¸ ë´‡ ì°¨ë‹¨ ê¸°ìˆ ë³„ ëŒ€ì‘
â€¢ Cloudflare: User-Agent, í—¤ë” ì¡°ì‘
â€¢ reCAPTCHA: ìˆ˜ë™ í•´ê²° ë˜ëŠ” ìš°íšŒ
â€¢ Rate Limiting: ìš”ì²­ ê°„ê²© ì¡°ì ˆ
â€¢ JavaScript Challenge: í—¤ë“œë¦¬ìŠ¤ ë¸Œë¼ìš°ì € í•„ìˆ˜

ğŸ¯ ì‹¤ì „ íŒë³„ ë°©ë²•

1ï¸âƒ£ í˜ì´ì§€ ì†ŒìŠ¤ ë³´ê¸° (Ctrl+U)
â†’ ë‚´ìš©ì´ ë³´ì´ë©´: ì •ì  (ì‰¬ì›€)
â†’ ê±°ì˜ ë¹„ì–´ìˆìœ¼ë©´: SPA (ì–´ë ¤ì›€)

2ï¸âƒ£ ê°œë°œì ë„êµ¬ Network íƒ­
â†’ HTML í•˜ë‚˜ë§Œ: ì •ì 
â†’ ë§ì€ XHR/Fetch: ë™ì 

3ï¸âƒ£ JavaScript ë¹„í™œì„±í™” í…ŒìŠ¤íŠ¸
â†’ ë‚´ìš©ì´ ê·¸ëŒ€ë¡œ: ì •ì 
â†’ ë‚´ìš©ì´ ì‚¬ë¼ì§: ë™ì 

4ï¸âƒ£ URL íŒ¨í„´ í™•ì¸
â†’ /page/1, /category/tech: ì „í†µì 
â†’ /#/page/1, /app: SPA ê°€ëŠ¥ì„±

ğŸ’¡ íš¨ìœ¨ì ì¸ ì ‘ê·¼ ì „ëµ

1. ì •ì  ë¶„ì„ ìš°ì„  (requests + BeautifulSoup)
2. ì•ˆ ë˜ë©´ API ë¶„ì„ (Network íƒ­)
3. ë§ˆì§€ë§‰ì— Selenium/Playwright ì‚¬ìš©
4. ê°€ëŠ¥í•˜ë©´ ëª¨ë°”ì¼ ë²„ì „ì´ë‚˜ RSS í™œìš©

âš ï¸ ì£¼ì˜ì‚¬í•­
â€¢ robots.txtì™€ ì´ìš©ì•½ê´€ í™•ì¸ í•„ìˆ˜
â€¢ ê³¼ë„í•œ ìš”ì²­ ê¸ˆì§€ (ì„œë²„ ë¶€í•˜)
â€¢ ê°œì¸ì •ë³´ ì²˜ë¦¬ ì‹œ ë²•ì  ê²€í†  í•„ìš”
â€¢ ì €ì‘ê¶Œ ì½˜í…ì¸  ì£¼ì˜
"""
        
        self.recommend_text.insert(tk.END, recommend_content)
        self.tech_text.insert(tk.END, tech_content)
        
        # í¸ì§‘ ë¶ˆê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        self.recommend_text.config(state='disabled')
        self.tech_text.config(state='disabled')
    
    def crawl_website(self, url):
        """ì‹¤ì œ í¬ë¡¤ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            # í¬ë¡¤ë§ ì¤‘ì§€ í™•ì¸
            if not self.is_crawling:
                return
                
            # ìš”ì²­ í—¤ë” ì„¤ì •
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }
            
            # ì›¹ í˜ì´ì§€ ìš”ì²­
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸ (ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
            self.root.after(0, self.update_results, url, response, soup)
            
        except requests.exceptions.RequestException as e:
            self.root.after(0, self.show_error, f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            self.root.after(0, self.show_error, f"í¬ë¡¤ë§ ì˜¤ë¥˜: {str(e)}")
    
    def update_results(self, url, response, soup):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ UIì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
        try:
            # í˜ì´ì§€ ì •ë³´
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"
            
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            description = meta_desc.get('content', 'ì„¤ëª… ì—†ìŒ') if meta_desc else "ì„¤ëª… ì—†ìŒ"
            
            info = f"ì œëª©: {title_text}\n"
            info += f"URL: {url}\n"
            info += f"ìƒíƒœ ì½”ë“œ: {response.status_code}\n"
            info += f"ì½˜í…ì¸  íƒ€ì…: {response.headers.get('content-type', 'ì•Œ ìˆ˜ ì—†ìŒ')}\n"
            info += f"ì½˜í…ì¸  í¬ê¸°: {len(response.content)} bytes\n"
            info += f"ì„¤ëª…: {description}\n"
            
            self.info_text.insert(tk.END, info)
            
            # ë§í¬ ì¶”ì¶œ
            if self.extract_links.get():
                links = soup.find_all('a', href=True)
                self.links_text.insert(tk.END, f"ì´ {len(links)}ê°œì˜ ë§í¬ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n\n")
                
                for i, link in enumerate(links[:50], 1):  # ìµœëŒ€ 50ê°œë§Œ í‘œì‹œ
                    href = link['href']
                    text = link.get_text(strip=True)
                    full_url = urljoin(url, href)
                    self.links_text.insert(tk.END, f"{i}. {text}\n   URL: {full_url}\n\n")
                
                if len(links) > 50:
                    self.links_text.insert(tk.END, f"... ê·¸ ì™¸ {len(links) - 50}ê°œ ë§í¬ê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ
            if self.extract_images.get():
                images = soup.find_all('img', src=True)
                self.images_text.insert(tk.END, f"ì´ {len(images)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤:\n\n")
                
                for i, img in enumerate(images[:30], 1):  # ìµœëŒ€ 30ê°œë§Œ í‘œì‹œ
                    src = img['src']
                    alt = img.get('alt', 'ëŒ€ì²´ í…ìŠ¤íŠ¸ ì—†ìŒ')
                    full_url = urljoin(url, src)
                    self.images_text.insert(tk.END, f"{i}. {alt}\n   URL: {full_url}\n\n")
                
                if len(images) > 30:
                    self.images_text.insert(tk.END, f"... ê·¸ ì™¸ {len(images) - 30}ê°œ ì´ë¯¸ì§€ê°€ ë” ìˆìŠµë‹ˆë‹¤.")
            
            # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
            if self.extract_text.get():
                # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ íƒœê·¸ ì œê±°
                for script in soup(["script", "style"]):
                    script.decompose()
                
                text = soup.get_text()
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                if len(text) > 5000:
                    text = text[:5000] + "\n\n... (í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤)"
                
                self.content_text.insert(tk.END, text)
            
            # í…Œì´ë¸”ì— ë°ì´í„° ì¶”ê°€
            self.populate_table(url, soup)
            
            self.status_var.set(f"í¬ë¡¤ë§ ì™„ë£Œ - {title_text}")
            
        except Exception as e:
            self.show_error(f"ê²°ê³¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
        finally:
            # UI ë‹¤ì‹œ í™œì„±í™”
            self.progress.stop()
            self.crawl_button.config(state='normal')
            self.stop_button.config(state='disabled')
            self.export_button.config(state='normal')
            self.is_crawling = False
    
    def show_error(self, error_message):
        """ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
        self.progress.stop()
        self.crawl_button.config(state='normal')
        self.stop_button.config(state='disabled')
        self.is_crawling = False
        self.status_var.set("ì˜¤ë¥˜ ë°œìƒ")
        messagebox.showerror("í¬ë¡¤ë§ ì˜¤ë¥˜", error_message)
    
    def populate_table(self, url, soup):
        """í…Œì´ë¸”ì— í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
        try:
            # í˜ì´ì§€ ê¸°ë³¸ ì •ë³´
            title = soup.find('title')
            title_text = title.get_text(strip=True) if title else "ì œëª© ì—†ìŒ"
            
            # í˜ì´ì§€ ì •ë³´ ì¶”ê°€
            page_data = {
                'type': 'í˜ì´ì§€',
                'title': title_text,
                'url': url,
                'description': 'ì›¹í˜ì´ì§€ ê¸°ë³¸ ì •ë³´'
            }
            self.tree.insert('', 'end', values=(page_data['type'], page_data['title'], 
                                              page_data['url'], page_data['description']))
            self.crawled_data.append(page_data)
            
            # ë§í¬ ì •ë³´ ì¶”ê°€
            if self.extract_links.get():
                links = soup.find_all('a', href=True)
                for i, link in enumerate(links[:50]):  # ìµœëŒ€ 50ê°œ
                    href = link['href']
                    text = link.get_text(strip=True)[:100]  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                    full_url = urljoin(url, href)
                    
                    link_data = {
                        'type': 'ë§í¬',
                        'title': text or '(í…ìŠ¤íŠ¸ ì—†ìŒ)',
                        'url': full_url,
                        'description': f'ë§í¬ #{i+1}'
                    }
                    self.tree.insert('', 'end', values=(link_data['type'], link_data['title'], 
                                                      link_data['url'], link_data['description']))
                    self.crawled_data.append(link_data)
            
            # ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            if self.extract_images.get():
                images = soup.find_all('img', src=True)
                for i, img in enumerate(images[:30]):  # ìµœëŒ€ 30ê°œ
                    src = img['src']
                    alt = img.get('alt', 'ëŒ€ì²´ í…ìŠ¤íŠ¸ ì—†ìŒ')[:100]
                    full_url = urljoin(url, src)
                    
                    img_data = {
                        'type': 'ì´ë¯¸ì§€',
                        'title': alt,
                        'url': full_url,
                        'description': f'ì´ë¯¸ì§€ #{i+1}'
                    }
                    self.tree.insert('', 'end', values=(img_data['type'], img_data['title'], 
                                                      img_data['url'], img_data['description']))
                    self.crawled_data.append(img_data)
                    
        except Exception as e:
            print(f"í…Œì´ë¸” ì±„ìš°ê¸° ì˜¤ë¥˜: {str(e)}")
    
    def export_to_excel(self):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.crawled_data:
            messagebox.showwarning("ê²½ê³ ", "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # íŒŒì¼ ì €ì¥ ëŒ€í™”ìƒì
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            default_filename = f"crawling_result_{timestamp}.xlsx"
            
            file_path = filedialog.asksaveasfilename(
                defaultextension=".xlsx",
                filetypes=[("Excel files", "*.xlsx"), ("All files", "*.*")],
                initialvalue=default_filename
            )
            
            if not file_path:
                return
            
            # DataFrame ìƒì„±
            df = pd.DataFrame(self.crawled_data)
            df.columns = ['íƒ€ì…', 'ì œëª©/í…ìŠ¤íŠ¸', 'URL', 'ì„¤ëª…']
            
            # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
            with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='í¬ë¡¤ë§ ê²°ê³¼', index=False)
                
                # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸° ë° ì„œì‹ ì„¤ì •
                worksheet = writer.sheets['í¬ë¡¤ë§ ê²°ê³¼']
                
                # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 50)  # ìµœëŒ€ 50ìë¡œ ì œí•œ
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            messagebox.showinfo("ì™„ë£Œ", f"ê²°ê³¼ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\níŒŒì¼: {file_path}")
            
        except Exception as e:
            messagebox.showerror("ì˜¤ë¥˜", f"íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n{str(e)}")

def main():
    root = tk.Tk()
    app = WebCrawlerApp(root)
    root.mainloop()

if __name__ == "__main__":
    main() 